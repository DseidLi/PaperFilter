{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACL 2024 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Proofread: Fixes All Errors with One Tap - Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai',\n",
       "  'Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions - Max Dallabetta, Conrad Dobberstein, Adrian Johannes Breiding, Alan Akbik',\n",
       "  'AutoRE: Document-Level Relation Extraction with Large Language Models - Lilong Xue, Dan Zhang, Yuxiao Dong, Jie Tang',\n",
       "  'XNLP: An Interactive Demonstration System for Universal Structured NLP - Hao Fei, Meishan Zhang, Min Zhang, Tat-Seng Chua',\n",
       "  'PyFoma: a Python finite-state compiler module - Mans Hulden, Michael Ginn, Miikka Silfverberg, Michael Hammond',\n",
       "  'BioLunar: Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery - Oskar Wysocki, Magdalena Wysocka, Danilo Carvalho, Alex Teodor Bogatu, Danilo Miranda Gusicuma, Maxime Delmas, Harriet Unsworth, Andre Freitas',\n",
       "  'CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph - Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao',\n",
       "  'OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety - Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui Zhang, Xinmeng Ji, Tingting Cui, Liutao, Jinwang Song, Hongying ZAN, Sun Li, Deyi Xiong',\n",
       "  'OpenWebAgent: An Open Toolkit to Enable Web Agents on Large Language Models - Iat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao Dong, Jie Tang',\n",
       "  'CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM - Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie GU',\n",
       "  'LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models - Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Elena Voita',\n",
       "  'Variationist: Exploring Multifaceted Variation and Bias in Written Language Data - Alan Ramponi, Camilla Casula, Stefano Menini',\n",
       "  'VeraCT Scan: Retrieval-Augmented Fake News Detection with Justifiable Reasoning - Cheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Juntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu, Shizhe Diao, Tong Zhang',\n",
       "  'LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models - Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan, Zheyan Luo',\n",
       "  'BotEval: Facilitating Interactive Human Evaluation - Hyundong Justin Cho, Thamme Gowda, Yuyang Huang, Zixun Lu, Tianli Tong, Jonathan May',\n",
       "  'Wordflow: Social Prompt Engineering for Large Language Models - Zijie J. Wang, Aishwarya Chakravarthy, David Munechika, Duen Horng Chau',\n",
       "  'LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models - Abhishek Arora, Melissa Dell',\n",
       "  'JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning - Anique Tahir, Lu Cheng, huan liu',\n",
       "  'GenGO: ACL Paper Explorer with Semantic Features - Sotaro Takeshita, Simone Paolo Ponzetto, Kai Eckert',\n",
       "  'OpenVNA: A Framework for Analyzing the Behavior of Multimodal Language Understanding System under Noisy Scenarios - Ziqi Yuan, Baozheng Zhang, Hua Xu, Zhiyun Liang, Kai Gao',\n",
       "  'IMGTB: A Framework for Machine-Generated Text Detection Benchmarking - Michal Spiegel, Dominik Macko',\n",
       "  'NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing - Tim Schopf, Florian Matthes',\n",
       "  'string2string: A Modern Python Library for String-to-String Algorithms - Mirac Suzgun, Stuart Shieber, Dan Jurafsky',\n",
       "  'PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud - Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, Jun Huang',\n",
       "  'DocPilot: Copilot for Automating PDF Edit Workflows in Documents - Puneet Mathur, Alexa Siu, Varun Manjunatha, Tong Sun',\n",
       "  'LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems - Xiao Yu, Yunan Lu, Zhou Yu',\n",
       "  'ITAKE: Interactive Unstructured Text Annotation and Knowledge Extraction System with LLMs and ModelOps - Jiahe Song, Hongxin Ding, Zhiyuan Wang, Yongxin Xu, Yasha Wang, Junfeng Zhao',\n",
       "  'UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs - Chaoqun He, Renjie Luo, Shengding Hu, Ranchi Zhao, Jie Zhou, Hanghao Wu, jiajie zhang, Xu Han, Zhiyuan Liu, Maosong Sun',\n",
       "  'EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models - Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Runnan Fang, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen',\n",
       "  'SeaLLMs - Large Language Models for Southeast Asia - Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, CHAOQUN LIU, Hang Zhang, Lidong Bing',\n",
       "  'LLMBox: A Comprehensive Library for Language Language Models - Tianyi Tang, Hu Yiwen, Bingqian Li, Wenyang Luo, ZiJing Qin, Haoxiang Sun, jiapeng wang, Shiyi Xu, Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie Chen, Ranchi Zhao, Luran Ding, Yuhao Wang, zican Dong, Xia Chunxuan, Junyi Li, Kun Zhou, Xin Zhao, Ji-Rong Wen',\n",
       "  'LinguaLinked: Distributed Large Language Model Inference on Mobile Devices - Junchen Zhao, Yurun song, Simeng Liu, Ian Harris, Sangeetha Abdu Jyothi',\n",
       "  'Towards the TopMost: A Topic Modeling System Toolkit - Xiaobao Wu, Pan Fengjun, Anh Tuan Luu',\n",
       "  'ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice - Yutong Hu, Kangcheng Luo, Yansong Feng',\n",
       "  'EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models - Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen',\n",
       "  'DrugWatch: A Comprehensive Multi-Source Data Visualisation Platform for Drug Safety Information - Artem Bobrov, Domantas Saltenis, ZHAOYUE SUN, Gabriele Pergola, Yulan He',\n",
       "  'LEGENT: An Open Platform for Embodied Agent - Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, Maosong Sun',\n",
       "  'EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot - Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu, Erik Cambria'],\n",
       " 38)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demopapers = \"\"\"Proofread: Fixes All Errors with One Tap - Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai\n",
    "Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions - Max Dallabetta, Conrad Dobberstein, Adrian Johannes Breiding, Alan Akbik\n",
    "AutoRE: Document-Level Relation Extraction with Large Language Models - Lilong Xue, Dan Zhang, Yuxiao Dong, Jie Tang\n",
    "XNLP: An Interactive Demonstration System for Universal Structured NLP - Hao Fei, Meishan Zhang, Min Zhang, Tat-Seng Chua\n",
    "PyFoma: a Python finite-state compiler module - Mans Hulden, Michael Ginn, Miikka Silfverberg, Michael Hammond\n",
    "BioLunar: Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery - Oskar Wysocki, Magdalena Wysocka, Danilo Carvalho, Alex Teodor Bogatu, Danilo Miranda Gusicuma, Maxime Delmas, Harriet Unsworth, Andre Freitas\n",
    "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph - Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao\n",
    "OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety - Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui Zhang, Xinmeng Ji, Tingting Cui, Liutao, Jinwang Song, Hongying ZAN, Sun Li, Deyi Xiong\n",
    "OpenWebAgent: An Open Toolkit to Enable Web Agents on Large Language Models - Iat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao Dong, Jie Tang\n",
    "CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM - Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie GU\n",
    "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models - Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Elena Voita\n",
    "Variationist: Exploring Multifaceted Variation and Bias in Written Language Data - Alan Ramponi, Camilla Casula, Stefano Menini\n",
    "VeraCT Scan: Retrieval-Augmented Fake News Detection with Justifiable Reasoning - Cheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Juntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu, Shizhe Diao, Tong Zhang\n",
    "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models - Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan, Zheyan Luo\n",
    "BotEval: Facilitating Interactive Human Evaluation - Hyundong Justin Cho, Thamme Gowda, Yuyang Huang, Zixun Lu, Tianli Tong, Jonathan May\n",
    "Wordflow: Social Prompt Engineering for Large Language Models - Zijie J. Wang, Aishwarya Chakravarthy, David Munechika, Duen Horng Chau\n",
    "LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models - Abhishek Arora, Melissa Dell\n",
    "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning - Anique Tahir, Lu Cheng, huan liu\n",
    "GenGO: ACL Paper Explorer with Semantic Features - Sotaro Takeshita, Simone Paolo Ponzetto, Kai Eckert\n",
    "OpenVNA: A Framework for Analyzing the Behavior of Multimodal Language Understanding System under Noisy Scenarios - Ziqi Yuan, Baozheng Zhang, Hua Xu, Zhiyun Liang, Kai Gao\n",
    "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking - Michal Spiegel, Dominik Macko\n",
    "NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing - Tim Schopf, Florian Matthes\n",
    "string2string: A Modern Python Library for String-to-String Algorithms - Mirac Suzgun, Stuart Shieber, Dan Jurafsky\n",
    "PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud - Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, Jun Huang\n",
    "DocPilot: Copilot for Automating PDF Edit Workflows in Documents - Puneet Mathur, Alexa Siu, Varun Manjunatha, Tong Sun\n",
    "LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems - Xiao Yu, Yunan Lu, Zhou Yu\n",
    "ITAKE: Interactive Unstructured Text Annotation and Knowledge Extraction System with LLMs and ModelOps - Jiahe Song, Hongxin Ding, Zhiyuan Wang, Yongxin Xu, Yasha Wang, Junfeng Zhao\n",
    "UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs - Chaoqun He, Renjie Luo, Shengding Hu, Ranchi Zhao, Jie Zhou, Hanghao Wu, jiajie zhang, Xu Han, Zhiyuan Liu, Maosong Sun\n",
    "EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models - Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Runnan Fang, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen\n",
    "SeaLLMs - Large Language Models for Southeast Asia - Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, CHAOQUN LIU, Hang Zhang, Lidong Bing\n",
    "LLMBox: A Comprehensive Library for Language Language Models - Tianyi Tang, Hu Yiwen, Bingqian Li, Wenyang Luo, ZiJing Qin, Haoxiang Sun, jiapeng wang, Shiyi Xu, Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie Chen, Ranchi Zhao, Luran Ding, Yuhao Wang, zican Dong, Xia Chunxuan, Junyi Li, Kun Zhou, Xin Zhao, Ji-Rong Wen\n",
    "LinguaLinked: Distributed Large Language Model Inference on Mobile Devices - Junchen Zhao, Yurun song, Simeng Liu, Ian Harris, Sangeetha Abdu Jyothi\n",
    "Towards the TopMost: A Topic Modeling System Toolkit - Xiaobao Wu, Pan Fengjun, Anh Tuan Luu\n",
    "ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice - Yutong Hu, Kangcheng Luo, Yansong Feng\n",
    "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models - Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen\n",
    "DrugWatch: A Comprehensive Multi-Source Data Visualisation Platform for Drug Safety Information - Artem Bobrov, Domantas Saltenis, ZHAOYUE SUN, Gabriele Pergola, Yulan He\n",
    "LEGENT: An Open Platform for Embodied Agent - Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, Maosong Sun\n",
    "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot - Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu, Erik Cambria\"\"\"\n",
    "# split demo_papers with each line as a list element\n",
    "demo_papers = demopapers.split(\"\\n\")\n",
    "demo_papers,len(demo_papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACL 2024 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models',\n",
       "  'Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances',\n",
       "  'MAGE: Machine-generated Text Detection in the Wild',\n",
       "  'PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models',\n",
       "  'GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators',\n",
       "  'Exploring Chain-of-Thought for Multi-modal Metaphor Detection',\n",
       "  'BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation',\n",
       "  'A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation',\n",
       "  'Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation',\n",
       "  'CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers',\n",
       "  'Evaluating Dynamic Topic Models',\n",
       "  'How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition',\n",
       "  'Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification',\n",
       "  'Inference to the Best Explanation in Large Language Models',\n",
       "  'A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus',\n",
       "  'DeVAn: Dense Video Annotation for Video-Language Models',\n",
       "  'MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering',\n",
       "  'SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs',\n",
       "  'SciMON: Scientific Inspiration Machines Optimized for Novelty',\n",
       "  'Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction',\n",
       "  'Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models',\n",
       "  'Retrieval-Augmented Multilingual Knowledge Editing',\n",
       "  'Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge',\n",
       "  'Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models',\n",
       "  'Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News',\n",
       "  'Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences',\n",
       "  'TTM-RE: Memory-Augmented Document-Level Relation Extraction',\n",
       "  'Answer is All You Need: Instruction-following Text Embedding via Answering the Question',\n",
       "  'Explore Spurious Correlations at the Concept Level in Language Models for Text Classification',\n",
       "  'Every Answer Matters: Evaluating Commonsense with Probabilistic Measures',\n",
       "  'GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis',\n",
       "  'How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs',\n",
       "  'Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy',\n",
       "  'An Information-Theoretic Approach to Analyze NLP Classification Tasks',\n",
       "  'Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders',\n",
       "  'Wav2Gloss: Generating Interlinear Glossed Text from Speech',\n",
       "  'Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification',\n",
       "  'SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation',\n",
       "  'OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following',\n",
       "  'Multimodal Instruction Tuning with Conditional Mixture of LoRA',\n",
       "  'DocLens: Multi-aspect Fine-grained Medical Text Evaluation',\n",
       "  'FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability',\n",
       "  'Hyper-CL: Conditioning Sentence Representations with Hypernetworks',\n",
       "  'Analysis of Multi-Source Language Training in Cross-Lingual Transfer',\n",
       "  'ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions',\n",
       "  'The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants',\n",
       "  'Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving',\n",
       "  'Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach',\n",
       "  'IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction',\n",
       "  'Token-wise Influential Training Data Retrieval for Large Language Models',\n",
       "  'Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection',\n",
       "  'VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks',\n",
       "  'FineSurE: Fine-grained Summarization Evaluation using LLMs',\n",
       "  'Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback',\n",
       "  'Prompt Refinement with Image Pivot for Text-to-Image Generation',\n",
       "  'The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models',\n",
       "  'Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation',\n",
       "  'AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation',\n",
       "  'Reflect-RL: Two-Player Online RL Fine-Tuning for LMs',\n",
       "  'Can ChatGPT’s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge',\n",
       "  'Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning',\n",
       "  'An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation',\n",
       "  'RORA: Robust Free-Text Rationale Evaluation',\n",
       "  'Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents',\n",
       "  'Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models',\n",
       "  'L-Eval: Instituting Standardized Evaluation for Long Context Language Models',\n",
       "  'DIALECTBENCH: An NLP Benchmark for Dialects, Varieties, and Closely-Related Languages',\n",
       "  'InstructProtein: Aligning Human and Protein Language via Knowledge Instruction',\n",
       "  'Causal-Guided Active Learning for Debiasing Large Language Models',\n",
       "  'ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models',\n",
       "  'Linguistically Conditioned Semantic Textual Similarity',\n",
       "  'Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future',\n",
       "  'TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models',\n",
       "  'BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering',\n",
       "  'ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base',\n",
       "  'TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation',\n",
       "  'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models',\n",
       "  'Grounding Language Model with Chunking-Free In-Context Retrieval',\n",
       "  'Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation',\n",
       "  'Active Prompting with Chain-of-Thought for Large Language Models',\n",
       "  'EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs',\n",
       "  'Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search',\n",
       "  'A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications',\n",
       "  'Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal',\n",
       "  'Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency',\n",
       "  'Citation-Enhanced Generation for LLM-based Chatbots',\n",
       "  'Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection',\n",
       "  'Feature-Adaptive and Data-Scalable In-Context Learning',\n",
       "  'Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games',\n",
       "  'WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models',\n",
       "  'Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models',\n",
       "  'A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation',\n",
       "  'PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents',\n",
       "  'Probing Language Models for Pre-training Data Detection',\n",
       "  'Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding',\n",
       "  'IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation',\n",
       "  'Language Model Adaption for Reinforcement Learning with Natural Language Action Space',\n",
       "  'Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues',\n",
       "  'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression',\n",
       "  'Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model',\n",
       "  'HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy',\n",
       "  'Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition',\n",
       "  'An Effective Pronunciation Assessment Approach Leveraging Hierarchical Transformers and Pre-training Strategies',\n",
       "  'Detection-Correction Structure via General Language Model for Grammatical Error Correction',\n",
       "  'Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer',\n",
       "  'Selene: Pioneering Automated Proof in Software Verification',\n",
       "  'Dissecting Human and LLM Preferences',\n",
       "  'UniCoder: Scaling Code Large Language Model via Universal Code',\n",
       "  'AoE: Angle-optimized Embeddings for Semantic Textual Similarity',\n",
       "  'InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews',\n",
       "  'Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better',\n",
       "  'AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators',\n",
       "  'Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering',\n",
       "  'LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin',\n",
       "  'Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation',\n",
       "  'M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions',\n",
       "  'AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension',\n",
       "  'Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies',\n",
       "  'ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models',\n",
       "  'DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction',\n",
       "  'SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations',\n",
       "  'Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation',\n",
       "  'REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation',\n",
       "  'Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks',\n",
       "  'MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation',\n",
       "  'Open-Set Semi-Supervised Text Classification via Adversarial Disagreement Maximization',\n",
       "  'ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages',\n",
       "  'A synthetic data approach for domain generalization of NLI models',\n",
       "  'Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild',\n",
       "  'LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models',\n",
       "  'VariErr NLI: Separating Annotation Error from Human Label Variation',\n",
       "  'Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient',\n",
       "  'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation',\n",
       "  'ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval',\n",
       "  'Exploring the Potential of Large Language Models in Computational Argumentation',\n",
       "  'TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks',\n",
       "  'CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning',\n",
       "  'MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter',\n",
       "  'Surgical Feature-Space Decomposition of LLMs: Why, When and How?',\n",
       "  'Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance',\n",
       "  'Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering',\n",
       "  'Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression',\n",
       "  'Emergent Word Order Universals from Cognitively-Motivated Language Models',\n",
       "  'VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models',\n",
       "  'Making Long-Context Language Models Better Multi-Hop Reasoners',\n",
       "  'TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models',\n",
       "  'Extreme Miscalibration and the Illusion of Adversarial Robustness',\n",
       "  'HyCoRec: Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation',\n",
       "  'Co-training for Low Resource Scientific Natural Language Inference',\n",
       "  'RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models',\n",
       "  'Time is Encoded in the Weights of Finetuned Language Models',\n",
       "  'Long-Context Language Modeling with Parallel Context Encoding',\n",
       "  'SirLLM: Streaming Infinite Retentive LLM',\n",
       "  'IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models',\n",
       "  'Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale',\n",
       "  'MELA: Multilingual Evaluation of Linguistic Acceptability',\n",
       "  'Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View',\n",
       "  'CopyNE: Better Contextual ASR by Copying Named Entities',\n",
       "  'Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval',\n",
       "  'Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation',\n",
       "  'ItD: Large Language Models Can Teach Themselves Induction through Deduction',\n",
       "  'MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs',\n",
       "  'MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin',\n",
       "  'Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent',\n",
       "  'On Context Utilization in Summarization with Large Language Models',\n",
       "  'INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning',\n",
       "  'Enhancing In-Context Learning via Implicit Demonstration Augmentation',\n",
       "  'PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA',\n",
       "  'Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics',\n",
       "  'Improving Event Definition Following For Zero-Shot Event Detection',\n",
       "  'Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements',\n",
       "  'Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding',\n",
       "  'Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks',\n",
       "  'CausalGym: Benchmarking causal interpretability methods on linguistic tasks',\n",
       "  'Training Language Models to Generate Text with Citations via Fine-grained Rewards',\n",
       "  'Hypergraph based Understanding for Document Semantic Entity Recognition',\n",
       "  'GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers',\n",
       "  'Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models',\n",
       "  'AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning',\n",
       "  'ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks',\n",
       "  'Virtual Compiler Is All You Need For Assembly Code Search',\n",
       "  'MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning',\n",
       "  'Can LLMs Learn from Previous Mistakes? Investigating LLMs’ Errors to Boost for Reasoning',\n",
       "  'An Iterative Associative Memory Model for Empathetic Response Generation',\n",
       "  'Detoxifying Large Language Models via Knowledge Editing',\n",
       "  'LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding',\n",
       "  'Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models',\n",
       "  'UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages',\n",
       "  'VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval',\n",
       "  'Black-Box Prompt Optimization: Aligning Large Language Models without Model Training',\n",
       "  'Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark',\n",
       "  'Unified Hallucination Detection for Multimodal Large Language Models',\n",
       "  'Empowering Character-level Text Infilling by Eliminating Sub-Tokens',\n",
       "  'Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models',\n",
       "  'GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?',\n",
       "  'Attribute First, then Generate: Locally-attributable Grounded Text Generation',\n",
       "  'T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text',\n",
       "  'OceanGPT: A Large Language Model for Ocean Science Tasks',\n",
       "  'Beyond Memorization: The Challenge of Random Memory Access in Language Models',\n",
       "  'BIPED: Pedagogically Informed Tutoring System for ESL Education',\n",
       "  'Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction',\n",
       "  'Collaboration or Corporate Capture? Quantifying NLP’s Reliance on Industry Artifacts and Contributions',\n",
       "  'Prompt Expansion for Adaptive Text-to-Image Generation',\n",
       "  'Progressively Modality Freezing for Multi-Modal Entity Alignment',\n",
       "  'Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval',\n",
       "  'Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts',\n",
       "  'Metaphor Understanding Challenge Dataset for LLMs',\n",
       "  'A Multi-Task Embedder For Retrieval Augmented LLMs',\n",
       "  'Language Models Don’t Learn the Physical Manifestation of Language',\n",
       "  'Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration',\n",
       "  'What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection',\n",
       "  'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives',\n",
       "  'Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty',\n",
       "  'Mission: Impossible Language Models',\n",
       "  'Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources',\n",
       "  'Semisupervised Neural Proto-Language Reconstruction',\n",
       "  'When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP',\n",
       "  'SBAAM! Eliminating Transcript Dependency in Automatic Subtitling',\n",
       "  'Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?',\n",
       "  'StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection',\n",
       "  'ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling',\n",
       "  'Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference',\n",
       "  'FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model',\n",
       "  'MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations',\n",
       "  'MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning',\n",
       "  'DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows',\n",
       "  'Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective',\n",
       "  'Identifying while Learning for Document Event Causality Identification',\n",
       "  'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems',\n",
       "  'Insert or Attach: Taxonomy Completion via Box Embedding',\n",
       "  'Semiparametric Token-Sequence Co-Supervision',\n",
       "  'Instruction Fusion: Advancing Prompt Evolution through Hybridization',\n",
       "  'TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation',\n",
       "  'Exploring Memorization in Fine-tuned Language Models',\n",
       "  'Towards Real-world Scenario: Imbalanced New Intent Discovery',\n",
       "  'M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection',\n",
       "  'Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue',\n",
       "  'SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training',\n",
       "  'Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?',\n",
       "  'Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning',\n",
       "  'CaMML: Context-Aware Multimodal Learner for Large Models',\n",
       "  'MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation',\n",
       "  'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes',\n",
       "  'Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models',\n",
       "  'Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?',\n",
       "  'Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors',\n",
       "  'D2LLM: Decomposed and Distilled Large Language Models for Semantic Search',\n",
       "  'In\\xad-context Mixing (ICM): Code\\xad-mixed Prompts for Multilingual LLMs',\n",
       "  'Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models',\n",
       "  'Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries',\n",
       "  'Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding',\n",
       "  'Intuitive or Dependent? Investigating LLMs’ Behavior Style to Conflicting Prompts',\n",
       "  'CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending',\n",
       "  'Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization',\n",
       "  'InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification',\n",
       "  'Disinformation Capabilities of Large Language Models',\n",
       "  'Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models',\n",
       "  'CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following',\n",
       "  'DAPR: A Benchmark on Document-Aware Passage Retrieval',\n",
       "  'How to Handle Different Types of Out-of-Distribution Scenarios in Computational Argumentation? A Comprehensive and Fine-Grained Field Study',\n",
       "  'Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors',\n",
       "  'SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph',\n",
       "  'Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages',\n",
       "  'Uncertainty-Guided Modal Rebalance for Hateful Memes Detection',\n",
       "  'Must NLP be Extractive?',\n",
       "  'Spiral of Silence: How is Large Language Model Killing Information Retrieval?—A Case Study on Open Domain Question Answering',\n",
       "  'Missci: Reconstructing Fallacies in Misrepresented Science',\n",
       "  'Uncovering the Full Potential of Visual Grounding Methods in VQA',\n",
       "  'Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs',\n",
       "  'Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative AI Evaluation',\n",
       "  'LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback',\n",
       "  'Graph Language Models',\n",
       "  'Analyzing Semantic Change through Lexical Replacements',\n",
       "  'Exploiting Intrinsic Multilateral Logical Rules for Weakly Supervised Natural Language Video Localization',\n",
       "  'Latxa: An Open Language Model and Evaluation Suite for Basque',\n",
       "  'Interpretability of Language Models via Task Spaces',\n",
       "  'Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types',\n",
       "  'Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators',\n",
       "  'StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback',\n",
       "  'One-Shot Learning as Instruction Data Prospector for Large Language Models',\n",
       "  'Navigating the OverKill in Large Language Models',\n",
       "  'Why are Sensitive Functions Hard for Transformers?',\n",
       "  'A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains',\n",
       "  'Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision',\n",
       "  'NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents',\n",
       "  'FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models',\n",
       "  'Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction',\n",
       "  'Learning to Edit: Aligning LLMs with Knowledge Editing',\n",
       "  'DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning',\n",
       "  'IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators',\n",
       "  'When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality',\n",
       "  'SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models',\n",
       "  'Planning Like Human: A Dual-process Framework for Dialogue Planning',\n",
       "  'Spectral Filters, Dark Signals, and Attention Sinks',\n",
       "  'DiffuCOMET: Contextual Commonsense Knowledge Diffusion',\n",
       "  'Systematic Task Exploration with LLMs: A Study in Citation Text Generation',\n",
       "  'The Echoes of Multilinguality: Tracing Cultural Value Shifts during Language Model Fine-tuning',\n",
       "  'Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition',\n",
       "  'MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling',\n",
       "  'Temporal Knowledge Question Answering via Abstract Reasoning Induction',\n",
       "  'MultiLegalPile: A 689GB Multilingual Legal Corpus',\n",
       "  'Who Wrote this Code? Watermarking for Code Generation',\n",
       "  'MapCoder: Multi-Agent Code Generation for Competitive Problem Solving',\n",
       "  'RelayAttention for Efficient Large Language Model Serving with Long System Prompts',\n",
       "  'Boosting Language Models Reasoning with Chain-of-Knowledge Prompting',\n",
       "  'Open Grounded Planning: Challenges and Benchmark Construction',\n",
       "  'WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations',\n",
       "  'LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures',\n",
       "  'QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction',\n",
       "  'PITA: Prompting Task Interaction for Argumentation Mining',\n",
       "  'Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models',\n",
       "  'Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations',\n",
       "  'Estimating Agreement by Chance for Sequence Annotation',\n",
       "  'What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages',\n",
       "  'Are Emergent Abilities in Large Language Models just In-Context Learning?',\n",
       "  'WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning',\n",
       "  'Eliciting Better Multilingual Structured Reasoning from LLMs through Code',\n",
       "  'OLIVE: Object Level In-Context Visual Embeddings',\n",
       "  'Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness',\n",
       "  'Marathon: A Race Through the Realm of Long Context with Large Language Models',\n",
       "  'Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph',\n",
       "  'PCAD: Towards ASR-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling',\n",
       "  'Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment',\n",
       "  'UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation',\n",
       "  'PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers',\n",
       "  'Triple-Encoders: Representations That Fire Together, Wire Together',\n",
       "  'Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning',\n",
       "  'Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization',\n",
       "  'Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous Constituency Parsing',\n",
       "  'Your Transformer is Secretly Linear',\n",
       "  'Noise Correction on Subjective Datasets',\n",
       "  'Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers',\n",
       "  'Instruction-tuned Language Models are Better Knowledge Learners',\n",
       "  'What Do Language Models Hear? Probing for Auditory Representations in Language Models',\n",
       "  'Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs',\n",
       "  'Jailbreak Open-Sourced Large Language Models via Enforced Decoding',\n",
       "  'NICE: To Optimize In-Context Examples or Not?',\n",
       "  'CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation',\n",
       "  'Digital Socrates: Evaluating LLMs through Explanation Critiques',\n",
       "  'SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding',\n",
       "  'ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs',\n",
       "  'Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?',\n",
       "  'ChatDev: Communicative Agents for Software Development',\n",
       "  'Experiential Co-Learning of Software-Developing Agents',\n",
       "  'Learning Geometry-Aware Representations for New Intent Discovery',\n",
       "  'Speaker Verification in Agent-generated Conversations',\n",
       "  'Benchmarking Data Science Agents',\n",
       "  'Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models',\n",
       "  'Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models',\n",
       "  'A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques',\n",
       "  'Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation',\n",
       "  'PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking',\n",
       "  'RepCodec: A Speech Representation Codec for Speech Tokenization',\n",
       "  'Disentangled Learning with Synthetic Parallel Data for Text Style Transfer',\n",
       "  'GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick',\n",
       "  'PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety',\n",
       "  'Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection',\n",
       "  'Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions',\n",
       "  'Stealthy Attack on Large Language Model based Recommendation',\n",
       "  'Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning',\n",
       "  'Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models',\n",
       "  'SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning',\n",
       "  'Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning',\n",
       "  'SparseFlow: Accelerating Transformers by Sparsifying Information Flows',\n",
       "  'ProtT3: Protein-to-Text Generation for Text-based Protein Understanding',\n",
       "  'KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models',\n",
       "  'EmoBench: Evaluating the Emotional Intelligence of Large Language Models',\n",
       "  'Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation',\n",
       "  'Are AI-Generated Text Detectors Robust to Adversarial Perturbations?',\n",
       "  'FinTextQA: A Dataset for Long-form Financial Question Answering',\n",
       "  'On Measuring Faithfulness or Self-consistency of Natural Language Explanations',\n",
       "  '$\\\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens',\n",
       "  'Learning or Self-aligning? Rethinking Instruction Fine-tuning',\n",
       "  'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?',\n",
       "  'Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct LLM in Knowledge-based VQA',\n",
       "  'TasTe: Teaching Large Language Models to Translate through Self-Reflection',\n",
       "  'Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models',\n",
       "  'Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models',\n",
       "  'UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion',\n",
       "  'The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities',\n",
       "  'Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models',\n",
       "  'AI ‘News’ Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian',\n",
       "  'Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?',\n",
       "  'Unveiling Linguistic Regions in Large Language Models',\n",
       "  'Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment',\n",
       "  'FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection',\n",
       "  'Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations',\n",
       "  'An Open Multilingual System for Scoring Readability of Wikipedia',\n",
       "  'Unlearning Traces the Influential Training Data of Language Models',\n",
       "  'Exploring Alignment in Shared Cross-lingual Spaces',\n",
       "  'Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models',\n",
       "  'Self-Evolving GPT: A Lifelong Autonomous Experiential Learner',\n",
       "  'WRP: Weight Recover Prune for Structured Sparsity',\n",
       "  'Error-preserving Automatic Speech Recognition of Young English Learners’ Language',\n",
       "  'DiFiNet: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition',\n",
       "  'Legal Case Retrieval: A Survey of the State of the Art',\n",
       "  'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models',\n",
       "  'Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation',\n",
       "  'LLaMA Pro: Progressive LLaMA with Block Expansion',\n",
       "  'Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning',\n",
       "  'A Causal Approach for Counterfactual Reasoning in Narratives',\n",
       "  'SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation',\n",
       "  'The Hidden Space of Transformer Language Adapters',\n",
       "  'A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts',\n",
       "  'Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations',\n",
       "  'RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions',\n",
       "  'GroundingGPT: Language Enhanced Multi-modal Grounding Model',\n",
       "  'Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches',\n",
       "  'Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages',\n",
       "  'SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget',\n",
       "  'PixT3: Pixel-based Table-To-Text Generation',\n",
       "  'Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers',\n",
       "  'TAMS: Translation-Assisted Morphological Segmentation',\n",
       "  'Disambiguate Words like Composing Them: A Morphology-Informed Approach to Enhance Chinese Word Sense Disambiguation',\n",
       "  'XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval',\n",
       "  'ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models',\n",
       "  'A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia',\n",
       "  'Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA',\n",
       "  'WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models',\n",
       "  'Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms',\n",
       "  'Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations',\n",
       "  'Robust Frame-Semantic Models with Lexical Unit Trees and Negative Samples',\n",
       "  'Do Llamas Work in English? On the Latent Language of Multilingual Transformers',\n",
       "  'Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation',\n",
       "  'Lightweight reranking for language model generations',\n",
       "  'ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews',\n",
       "  'The Unreasonable Effectiveness of Easy Training Data for Hard Tasks',\n",
       "  'PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning',\n",
       "  'MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning',\n",
       "  'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs',\n",
       "  'Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning',\n",
       "  'Where Do People Tell Stories Online? Story Detection Across Online Communities',\n",
       "  'Large Language Models Are No Longer Shallow Parsers',\n",
       "  'Dialogue Summarization with Mixture of Experts based on Large Language Models',\n",
       "  'ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences',\n",
       "  'An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs',\n",
       "  'Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling',\n",
       "  'Intrinsic Task-based Evaluation for Referring Expression Generation',\n",
       "  'From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models',\n",
       "  'End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction',\n",
       "  'Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?',\n",
       "  'Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder',\n",
       "  'G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation',\n",
       "  'CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers',\n",
       "  'Prompt Optimization via Adversarial In-Context Learning',\n",
       "  'StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion',\n",
       "  'Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering',\n",
       "  'Multimodal Contextualized Semantic Parsing from Speech',\n",
       "  'LaMP: When Large Language Models Meet Personalization',\n",
       "  'AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters',\n",
       "  'MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues',\n",
       "  'EFSA: Towards Event-Level Financial Sentiment Analysis',\n",
       "  'Media Framing: A typology and Survey of Computational Approaches Across Disciplines',\n",
       "  'What Evidence Do Language Models Find Convincing?',\n",
       "  'Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models',\n",
       "  'LangBridge: Multilingual Reasoning Without Multilingual Supervision',\n",
       "  'Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs',\n",
       "  'SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving',\n",
       "  'Unlocking the Power of Large Language Models for Entity Alignment',\n",
       "  'SPZ: A Semantic Perturbation-based Data Augmentation Method with Zonal-Mixing for Alzheimer’s Disease Detection',\n",
       "  'Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents',\n",
       "  'ReFT: Reasoning with Reinforced Fine-Tuning',\n",
       "  'Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment',\n",
       "  'FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation',\n",
       "  'HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition',\n",
       "  'Conundrums in Cross-Prompt Automated Essay Scoring: Making Sense of the State of the Art',\n",
       "  'Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution',\n",
       "  'Label Augmentation for Zero-Shot Hierarchical Text Classification',\n",
       "  'STICKERCONV: Generating Multimodal Empathetic Responses from Scratch',\n",
       "  'EIT: Enhanced Interactive Transformer',\n",
       "  'MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs',\n",
       "  'EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models',\n",
       "  'Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition',\n",
       "  'Text Embedding Inversion Security for Multilingual Language Models',\n",
       "  'Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment',\n",
       "  'Calibrating Large Language Models Using Their Generations Only',\n",
       "  'PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator',\n",
       "  'Synthesizing Text-to-SQL Data from Weak and Strong LLMs',\n",
       "  'Iterative Forward Tuning Boosts In-Context Learning in Language Models',\n",
       "  'STRUCTSUM Generation for Faster Text Comprehension',\n",
       "  'Analysing The Impact of Sequence Composition on Language Model Pre-Training',\n",
       "  'NACL: A General and Effective KV Cache Eviction Framework for LLM at Inference Time',\n",
       "  'SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network',\n",
       "  'Context-aware Difference Distilling for Multi-change Captioning',\n",
       "  'Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion',\n",
       "  'Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation',\n",
       "  'LLMEmbed: Rethinking Lightweight LLM’s Genuine Function in Text Classification',\n",
       "  'LEMON: Reviving Stronger and Smaller LMs from Larger LMs with Linear Parameter Fusion',\n",
       "  'Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation',\n",
       "  'To be Continuous, or to be Discrete, Those are Bits of Questions',\n",
       "  'Moûsai: Efficient Text-to-Music Diffusion Models',\n",
       "  'PokeMQA: Programmable knowledge editing for Multi-hop Question Answering',\n",
       "  'MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention',\n",
       "  'Efficient OCR for Building a Diverse Digital History',\n",
       "  'Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space',\n",
       "  'ANAH: Analytical Annotation of Hallucinations in Large Language Models',\n",
       "  'Aligning Large Language Models for Controllable Recommendations',\n",
       "  'Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods',\n",
       "  'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement',\n",
       "  'Full Parameter Fine-tuning for Large Language Models with Limited Resources',\n",
       "  'M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought',\n",
       "  'Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models',\n",
       "  'Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation',\n",
       "  'Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL',\n",
       "  'A Modular Approach for Multimodal Summarization of TV Shows',\n",
       "  'Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities',\n",
       "  'BizBench: A Quantitative Reasoning Benchmark for Business and Finance',\n",
       "  'Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization',\n",
       "  'Deciphering Hate: Identifying Hateful Memes and Their Targets',\n",
       "  'Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings',\n",
       "  'Label-Efficient Model Selection for Text Generation',\n",
       "  'Machine Unlearning of Pre-trained Large Language Models',\n",
       "  'Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals',\n",
       "  'FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence',\n",
       "  'BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction',\n",
       "  'Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack',\n",
       "  'Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn’t',\n",
       "  'Speech language models lack important brain-relevant semantics',\n",
       "  'DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding',\n",
       "  'Bypassing LLM Watermarks with Color-Aware Substitutions',\n",
       "  'Parallel Structures in Pre-training Data Yield In-Context Learning',\n",
       "  'OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models',\n",
       "  'Towards Privacy-Aware Sign Language Translation at Scale',\n",
       "  'Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards',\n",
       "  'Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters',\n",
       "  'Steering Llama 2 via Contrastive Activation Addition',\n",
       "  'RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations',\n",
       "  'Large Language Models as Zero-shot Dialogue State Tracker through Function Calling',\n",
       "  'Faithful Chart Summarization with ChaTS-Pi',\n",
       "  'Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation',\n",
       "  'MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking',\n",
       "  'KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction',\n",
       "  'ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis',\n",
       "  'EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities',\n",
       "  'On the Multi-turn Instruction Following for Conversational Web Agents',\n",
       "  'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents',\n",
       "  'MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China',\n",
       "  'Decoder-only Streaming Transformer for Simultaneous Translation',\n",
       "  'Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization',\n",
       "  'I am a Strange Dataset: Metalinguistic Tests for Language Models',\n",
       "  'SafetyBench: Evaluating the Safety of Large Language Models',\n",
       "  'Deciphering Oracle Bone Language with Diffusion Models',\n",
       "  'TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space',\n",
       "  'ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training',\n",
       "  'StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning',\n",
       "  'Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models',\n",
       "  'Why Don’t Prompt-Based Fairness Metrics Correlate?',\n",
       "  'NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data',\n",
       "  'M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset',\n",
       "  'Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination',\n",
       "  'Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning',\n",
       "  'Revisiting Demonstration Selection Strategies in In-Context Learning',\n",
       "  'Multimodal Table Understanding',\n",
       "  'Ex\\textsuperscript{3}: Automatic Novel Writing by Extracting, Excelsior and Expanding',\n",
       "  'Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning',\n",
       "  'WatME: Towards Lossless Watermarking Through Lexical Redundancy',\n",
       "  'Text-like Encoding of Collaborative Information in Large Language Models for Recommendation',\n",
       "  'MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception',\n",
       "  'Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning',\n",
       "  'Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation',\n",
       "  'M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models',\n",
       "  'Reward-based Input Construction for Cross-document Relation Extraction',\n",
       "  'Hyperspherical Multi-Prototype with Optimal Transport for Event Argument Extraction',\n",
       "  'Understanding Retrieval Robustness for Retrieval-augmented Image Captioning',\n",
       "  'Semi-Supervised Spoken Language Glossification',\n",
       "  'SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents',\n",
       "  'InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers',\n",
       "  'F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods',\n",
       "  'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning',\n",
       "  'Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback',\n",
       "  'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations',\n",
       "  'Large Language Models are not Fair Evaluators',\n",
       "  'Improving Large Language Models in Event Relation Logical Prediction',\n",
       "  'Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline',\n",
       "  'Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation',\n",
       "  'T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step',\n",
       "  'Are LLM-based Evaluators Confusing NLG Quality Criteria?',\n",
       "  'Synergistic Interplay between Search and Large Language Models for Information Retrieval',\n",
       "  'Linear Transformers with Learnable Kernel Functions are Better In-Context Models',\n",
       "  'Temperature-scaling surprisal estimates improve fit to human reading times – but does it do so for the “right reasons”?',\n",
       "  'Beyond Recognising Entailment: Formalising Natural Language Inference from an Argumentative Perspective',\n",
       "  'RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization',\n",
       "  'AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling',\n",
       "  'CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models',\n",
       "  'Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation',\n",
       "  'Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines',\n",
       "  'Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models',\n",
       "  'Robust Singing Voice Transcription Serves Synthesis',\n",
       "  'VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model',\n",
       "  'Self-Modifying State Modeling for Simultaneous Machine Translation',\n",
       "  'MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation',\n",
       "  'BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents',\n",
       "  'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy',\n",
       "  'LePaRD: A Large-Scale Dataset of Judicial Citations to Precedent',\n",
       "  'To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering',\n",
       "  'MERA: A Comprehensive LLM Evaluation in Russian',\n",
       "  'SC2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer',\n",
       "  'Causal Estimation of Memorisation Profiles',\n",
       "  'CHECKWHY: Causal Fact Verification via Argument Structure',\n",
       "  'Dodo: Dynamic Contextual Compression for Decoder-only LMs',\n",
       "  'POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation',\n",
       "  'NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism',\n",
       "  'MAPO: Advancing Multilingual Reasoning through Multilingual-Alignment-as-Preference Optimization',\n",
       "  'Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training',\n",
       "  'Predicting Text Preference Via Structured Comparative Reasoning',\n",
       "  'CoELM: Construction-Enhanced Language Modeling',\n",
       "  'Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model',\n",
       "  'Uni-Dubbing: Zero-Shot Speech Synthesis from Visual Articulation',\n",
       "  'On the Impact of Calibration Data in Post-training Quantization and Pruning',\n",
       "  'SymKGQA: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution',\n",
       "  'Meta-Task Prompting Elicits Embeddings from Large Language Models',\n",
       "  'A Sentiment Consolidation Framework for Meta-Review Generation',\n",
       "  'Revisiting Structured Sentiment Analysis as Latent Dependency Graph Parsing',\n",
       "  'OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification',\n",
       "  'Do Large Language Models Latently Perform Multi-Hop Reasoning?',\n",
       "  'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning',\n",
       "  'Harnessing Toulmin’s theory for zero-shot argument explication',\n",
       "  'BinaryAlign: Word Alignment as Binary Sequence Labeling',\n",
       "  'Quantifying the Persona Effect in LLM Simulations',\n",
       "  'On Efficient and Statistical Quality Estimation for Data Annotation',\n",
       "  'EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection',\n",
       "  'Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?',\n",
       "  'Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments',\n",
       "  'SyllabusQA: A Course Logistics Question Answering Dataset',\n",
       "  'American Sign Language Handshapes Reflect Pressures for Communicative Efficiency',\n",
       "  'MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models',\n",
       "  'AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts',\n",
       "  'Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks',\n",
       "  'Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning',\n",
       "  'Bridging the Preference Gap between Retrievers and LLMs',\n",
       "  'Large Language Models Can Learn Temporal Reasoning',\n",
       "  'Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research',\n",
       "  'Learning Relational Decomposition of Queries for Question Answering from Tables',\n",
       "  'Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People',\n",
       "  'Pareto Optimal Learning for Estimating Large Language Model Errors',\n",
       "  'Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models',\n",
       "  'Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM',\n",
       "  'Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models',\n",
       "  'LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error',\n",
       "  'HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts',\n",
       "  'Aligning Large Language Models with Human Preferences through Representation Engineering',\n",
       "  'CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models',\n",
       "  'ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation',\n",
       "  'PolCLIP: A Unified Image-Text Word Sense Disambiguation Model via Generating Multimodal Complementary Representations',\n",
       "  'Prompted Aspect Key Point Analysis for Quantitative Review Summarization',\n",
       "  'Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment',\n",
       "  'CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models',\n",
       "  'Multimodal Reasoning with Multimodal Knowledge Graph',\n",
       "  'Confidence is not Timeless: Modeling Temporal Validity for Rule-based Temporal Knowledge Graph Forecasting',\n",
       "  'CARE: A Clue-guided Assistant for CSRs to Read User Manuals',\n",
       "  'Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes',\n",
       "  'PAGED: A Benchmark for Procedural Graphs Extraction from Documents',\n",
       "  'Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors',\n",
       "  'RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models',\n",
       "  'The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models',\n",
       "  'Revisiting Knowledge Distillation for Autoregressive Language Models',\n",
       "  'OLMo: Accelerating the Science of Language Models',\n",
       "  'Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation',\n",
       "  'Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners',\n",
       "  'Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages',\n",
       "  'Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!',\n",
       "  'PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails',\n",
       "  'Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLMs-Powered Assistance',\n",
       "  'CLOMO: Counterfactual Logical Modification with Large Language Models',\n",
       "  'Exploring Hybrid Question Answering via Program-based Prompting',\n",
       "  'IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages',\n",
       "  'Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion',\n",
       "  'Uncertainty Aware Learning for Language Model Alignment',\n",
       "  'Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models',\n",
       "  'Fundamental Capabilities of Large Language Models and their Applications in Domain Scenarios: A Survey',\n",
       "  'IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages',\n",
       "  'Measuring Political Bias in Large Language Models: What Is Said and How It Is Said',\n",
       "  'Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use',\n",
       "  'Layer-Condensed KV Cache for Efficient Inference of Large Language Models',\n",
       "  'Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models',\n",
       "  'Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages',\n",
       "  'Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations',\n",
       "  'Browse and Concentrate: Comprehending Multimodal Content via Prior-LLM Context Fusion',\n",
       "  'Model Composition for Multimodal Large Language Models',\n",
       "  'Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding',\n",
       "  'Soul-Mix: Enhancing Multimodal Machine Translation with Manifold Mixup',\n",
       "  'Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models',\n",
       "  'MIST: Mutual Information Maximization for Short Text Clustering',\n",
       "  'Self-chats from Large Language Models Make Small Emotional Support Chatbot Better',\n",
       "  'Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment',\n",
       "  'Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs',\n",
       "  'An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing',\n",
       "  'Learning to Plan and Generate Text with Citations',\n",
       "  'Exploring Precision and Recall to assess the quality and diversity of LLMs',\n",
       "  'Aligning Large Language Models by On-Policy Self-Judgment',\n",
       "  'IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning',\n",
       "  'JumpCoder: Go Beyond Autoregressive Coder via Online Modification',\n",
       "  'Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning',\n",
       "  'Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks',\n",
       "  'Split and Rephrase with Large Language Models',\n",
       "  'ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition',\n",
       "  'AlignBench: Benchmarking Chinese Alignment of Large Language Models',\n",
       "  'SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models',\n",
       "  'DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution',\n",
       "  'Cross-Lingual Knowledge Editing in Large Language Models',\n",
       "  'Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model',\n",
       "  'Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques',\n",
       "  'Learning Task Decomposition to Assist Humans in Competitive Programming',\n",
       "  'An Entropy-based Text Watermarking Detection Method',\n",
       "  'Enhancing Explainable Rating Prediction through Annotated Macro Concepts',\n",
       "  'How to Engage your Readers? Generating Guiding Questions to Promote Active Reading',\n",
       "  'Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective',\n",
       "  'Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation',\n",
       "  'More frequent verbs are associated with more diverse valency frames: Efficient principles at the lexicon-grammar interface',\n",
       "  'BatchEval: Towards Human-like Text Evaluation',\n",
       "  'Quantifying Generalizations: Exploring the Divide Between Human and LLMs’ Sensitivity to Quantification',\n",
       "  'Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds',\n",
       "  'CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation',\n",
       "  'Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond',\n",
       "  'Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction',\n",
       "  'ToMBench: Benchmarking Theory of Mind in Large Language Models',\n",
       "  'Learning to Generate Answers with Citations via Factual Consistency Models',\n",
       "  'Improving Text Embeddings with Large Language Models',\n",
       "  'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning',\n",
       "  'UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset',\n",
       "  'Document-level Claim Extraction and Decontextualisation for Fact-Checking',\n",
       "  'PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning',\n",
       "  'LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction',\n",
       "  'Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models',\n",
       "  'COKE: A Cognitive Knowledge Graph for Machine Theory of Mind',\n",
       "  'mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models',\n",
       "  'GunStance: Stance Detection for Gun Control and Gun Regulation',\n",
       "  'Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation',\n",
       "  'Don’t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection',\n",
       "  'Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation',\n",
       "  'Generating and Evaluating Plausible Explanations for Knowledge Graph Completion',\n",
       "  'One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation',\n",
       "  'MultiPICo: Multilingual Perspectivist Irony Corpus',\n",
       "  'LANDeRMT: Dectecting and Routing Language-Aware Neurons for Selectively Finetuning LLMs to Machine Translation',\n",
       "  'A Joint Coreference-Aware Approach to Document-Level Target Sentiment Analysis',\n",
       "  'VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models',\n",
       "  'AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints',\n",
       "  'Multipath parsing in the brain',\n",
       "  'Search-Adaptor: Embedding Customization for Information Retrieval',\n",
       "  'Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs',\n",
       "  'VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation',\n",
       "  'Tree Transformer’s Disambiguation Ability of Prepositional Phrase Attachment and Garden Path Effects',\n",
       "  'Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs',\n",
       "  'Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing',\n",
       "  'ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation',\n",
       "  'AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents',\n",
       "  'Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking',\n",
       "  'What Do Language Models Learn in Context? The Structured Task Hypothesis.',\n",
       "  'Agent Lumos: Unified and Modular Training for Open-Source Language Agents',\n",
       "  'Investigating Cultural Alignment of Large Language Models',\n",
       "  'More Victories, Less Cooperation: Assessing Cicero’s Diplomacy Play',\n",
       "  'VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild',\n",
       "  'RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors',\n",
       "  'Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles',\n",
       "  'On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning',\n",
       "  'Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends',\n",
       "  'MMToM-QA: Multimodal Theory of Mind Question Answering',\n",
       "  'LLM in a flash: Efficient Large Language Model Inference with Limited Memory',\n",
       "  'Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models',\n",
       "  'To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation',\n",
       "  'DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents',\n",
       "  'LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding',\n",
       "  'Unintended Impacts of LLM Alignment on Global Representation',\n",
       "  'Classist Tools: Social Class Correlates with Performance in NLP',\n",
       "  'ActionIE: Action Extraction from Scientific Literature with Programming Languages',\n",
       "  'A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech',\n",
       "  'Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs',\n",
       "  'COSMIC: Mutual Information for Task-Agnostic Summarization Evaluation',\n",
       "  'ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer',\n",
       "  'EUROPA: A Legal Multilingual Keyphrase Generation Dataset',\n",
       "  'GLIMPSE: Pragmatically Informative Multi-Document Summarization for Scholarly Reviews',\n",
       "  'MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy',\n",
       "  'Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks',\n",
       "  'Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks',\n",
       "  'Cheetah: Natural Language Generation for 517 African Languages',\n",
       "  'TaPERA: Enhancing Faithfulness and Interpretability in Long-Form Table QA by Content Planning and Execution-based Reasoning',\n",
       "  'KnowledgeFMath: A Knowledge-Intensive Math Reasoning Dataset in Finance Domains',\n",
       "  'API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs',\n",
       "  'LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks',\n",
       "  'Harder Task Needs More Experts: Dynamic Routing in MoE Models',\n",
       "  'XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception',\n",
       "  'SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents',\n",
       "  '${\\\\mathcal X}$FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts',\n",
       "  'Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning',\n",
       "  'Learning to Decode Collaboratively with Multiple Language Models',\n",
       "  'DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models',\n",
       "  'Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?',\n",
       "  'CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation',\n",
       "  'LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments',\n",
       "  'Small But Funny: A Feedback-Driven Approach to Humor Distillation',\n",
       "  'Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models',\n",
       "  'From Sights to Insights: Towards Summarization of Multimodal Clinical Documents',\n",
       "  'When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models',\n",
       "  'Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation',\n",
       "  'Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in Arabic',\n",
       "  'Document-Level Machine Translation with Large-Scale Public Parallel Corpora',\n",
       "  'Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!',\n",
       "  'NounAtlas: Filling the Gap in Nominal Semantic Role Labeling',\n",
       "  'Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length',\n",
       "  'Context versus Prior Knowledge in Language Models',\n",
       "  'Word Matters: What Influences Domain Adaptation in Summarization?',\n",
       "  'Visualization Recommendation with Prompt-based Reprogramming of Large Language Models',\n",
       "  'HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs',\n",
       "  'Toward In-Context Teaching: Adapting Examples to Students’ Misconceptions',\n",
       "  'Bridging Word-Pair and Token-Level Metaphor Detection with Explainable Domain Mining',\n",
       "  'Faithful Logical Reasoning via Symbolic Chain-of-Thought',\n",
       "  'S$^2$GSL: Incorporating Segment to Syntactic Enhanced Graph Structure Learning for Aspect-based Sentiment Analysis',\n",
       "  'Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends',\n",
       "  'ESCoT: Towards Interpretable Emotional Support Dialogue Systems',\n",
       "  'PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering',\n",
       "  'WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection',\n",
       "  'Advancing Parameter Efficiency in Fine-tuning via Representation Editing',\n",
       "  'Context Consistency between Training and Inference in Simultaneous Machine Translation',\n",
       "  'Using Natural Language Explanations to Improve Robustness of In-context Learning',\n",
       "  'The Earth is Flat because…: Investigating LLMs’ Belief towards Misinformation via Persuasive Conversation',\n",
       "  'Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers',\n",
       "  'LooGLE: Can Long-Context Language Models Understand Long Contexts?',\n",
       "  'ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models',\n",
       "  'Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation',\n",
       "  'Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels',\n",
       "  'MULFE: A Multi-Level Benchmark for Free Text Model Editing',\n",
       "  'MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech',\n",
       "  'Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation',\n",
       "  'HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position',\n",
       "  'Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training',\n",
       "  'CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges',\n",
       "  'When is Tree Search Useful for LLM Planning? It Depends on the Discriminator',\n",
       "  'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models',\n",
       "  'ECBD: Evidence-Centered Benchmark Design for NLP',\n",
       "  'Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding',\n",
       "  'Reducing Privacy Risks in Online Self-Disclosures with Language Models',\n",
       "  'Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models',\n",
       "  'REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset',\n",
       "  'When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards',\n",
       "  'LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts',\n",
       "  'LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition',\n",
       "  'Evaluating Very Long-Term Conversational Memory of LLM Agents',\n",
       "  'Prototypical Reward Network for Data-Efficient Model Alignment',\n",
       "  'NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms',\n",
       "  'Impacts of Misspelled Queries on Translation and Product Search',\n",
       "  'Having Beer after Prayer? Measuring Cultural Bias in Large Language Models',\n",
       "  'Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs',\n",
       "  'The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition',\n",
       "  'Transparent and Scrutable Recommendations Using Natural Language User Profiles',\n",
       "  'Fora: A corpus and framework for the study of facilitated dialogue',\n",
       "  'Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning',\n",
       "  'What is the Best Way for ChatGPT to Translate Poetry?',\n",
       "  'Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling',\n",
       "  'DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention',\n",
       "  'Representation Learning with Conditional Information Flow Maximization',\n",
       "  'GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction',\n",
       "  'Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models',\n",
       "  'Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic',\n",
       "  'Tracking the Newsworthiness of Public Documents',\n",
       "  'EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems',\n",
       "  'Explicating the Implicit: Argument Detection Beyond Sentence Boundaries',\n",
       "  'Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models',\n",
       "  'Word Embeddings Are Steers for Language Models',\n",
       "  'Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation',\n",
       "  'Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor',\n",
       "  'LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP',\n",
       "  'Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning',\n",
       "  'Confabulation: The Surprising Value of Large Language Model Hallucinations',\n",
       "  'IAPT: Instance-Aware Prompt Tuning for Large Language Models',\n",
       "  'Can Language Models Serve as Text-Based World Simulators?',\n",
       "  'FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models',\n",
       "  'Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance',\n",
       "  'Resisting the Lure of the Skyline: Grounding Practices in Active Learning for Morphological Inflection',\n",
       "  'Speculative Contrastive Decoding',\n",
       "  'RDRec: Rationale Distillation for LLM-based Recommendation',\n",
       "  'Isotropy, Clusters, and Classifiers',\n",
       "  'Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks',\n",
       "  'Cleaner Pretraining Corpus Curation with Neural Web Scraping',\n",
       "  'Simpson’s Paradox and the Accuracy-Fluency Tradeoff in Translation',\n",
       "  'UltraSparseBERT: 99% Conditionally Sparse Language Modelling',\n",
       "  'SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark',\n",
       "  'On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models',\n",
       "  'IEPile: Unearthing Large Scale Schema-Conditioned Information Extraction Corpus',\n",
       "  'Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model',\n",
       "  'Code-Switching Can be Better Aligners: Advancing Cross-Lingual SLU through Representation-Level and Prediction-Level Alignment',\n",
       "  'AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models',\n",
       "  'DDPrompt: Differential Diversity Prompting in Large Language Models',\n",
       "  'Monotonic Representation of Numeric Attributes in Language Models',\n",
       "  'Two Issues with Chinese Spelling Correction and A Refinement Solution',\n",
       "  'Linear-time Minimum Bayes Risk Decoding with Reference Aggregation',\n",
       "  'DynaSemble: Dynamic Ensembling of Textual and Structure-Based Models for Knowledge Graph Completion',\n",
       "  'Fine-Tuning Pre-Trained Language Models with Gaze Supervision',\n",
       "  'Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency Parsing of Speech',\n",
       "  'Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access',\n",
       "  'On the Semantic Latent Space of Diffusion-Based Text-To-Speech Models',\n",
       "  'Learnable Privacy Neurons Localization in Language Models',\n",
       "  'Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs',\n",
       "  'Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing',\n",
       "  'Soft Self-Consistency Improves Language Models Agents',\n",
       "  'RecGPT: Generative Pre-training for Text-based Recommendation',\n",
       "  'MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations',\n",
       "  'What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects',\n",
       "  'What Does Parameter-free Probing Really Uncover?',\n",
       "  'ATLAS: Improving Lay Summarisation with Attribute-based Control',\n",
       "  'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models',\n",
       "  'Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark',\n",
       "  'Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval',\n",
       "  'That’s Optional: A Contemporary Exploration of “that” Omission in English Subordinate Clauses',\n",
       "  'Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?',\n",
       "  'Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster',\n",
       "  'Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models',\n",
       "  'Estimating the Level of Dialectness Predicts Inter-annotator Agreement in Multi-dialect Arabic Datasets',\n",
       "  'Born Differently Makes a Difference: Counterfactual Study of Bias in Biography Generation from a Data-to-Text Perspective',\n",
       "  'Greed is All You Need: An Evaluation of Tokenizer Inference Methods',\n",
       "  'Sign Language Translation with Sentence Embedding Supervision',\n",
       "  'STREAM: Simplified Topic Retrieval, Exploration, and Analysis Module',\n",
       "  'DocFinQA: A Long-Context Financial Reasoning Dataset',\n",
       "  'MaskLID: Code-Switching Language Identification through Iterative Masking',\n",
       "  'An Empirical Analysis on Large Language Models in Debate Evaluation',\n",
       "  'Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains',\n",
       "  'IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages',\n",
       "  'AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization',\n",
       "  'Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research',\n",
       "  'The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models',\n",
       "  'Don’t Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models',\n",
       "  'Naming, Describing, and Quantifying Visual Objects in Humans and LLMs',\n",
       "  'Are LLMs classical or nonmonotonic reasoners? Lessons from generics',\n",
       "  'ConstitutionalExperts: Training a Mixture of Principle-based Prompts',\n",
       "  'Time Sensitive Knowledge Editing through Efficient Finetuning',\n",
       "  'PRewrite: Prompt Rewriting with Reinforcement Learning',\n",
       "  'SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes',\n",
       "  'Paraphrasing in Affirmative Terms Improves Negation Understanding',\n",
       "  'Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios',\n",
       "  'Consistency Training by Synthetic Question Generation for Conversational Question Answering',\n",
       "  'How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?',\n",
       "  'Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages',\n",
       "  'Cross-Modal Projection in Multimodal LLMs Doesn’t Really Project Visual Attributes to Textual Space',\n",
       "  'Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition',\n",
       "  'Aligning Large Language Models via Fine-grained Supervision',\n",
       "  'Annotating FrameNet via Structure-Conditioned Language Generation',\n",
       "  'DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms',\n",
       "  'Towards Artwork Explanation in Large-scale Vision Language Models',\n",
       "  'On the Hallucination in Simultaneous Machine Translation',\n",
       "  'Self-Augmented In-Context Learning for Unsupervised Word Translation',\n",
       "  'RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records'],\n",
       " 940)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainpapers = \"\"\"Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models\n",
    "Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Qing Li, Yong Jiang, Zhihao Jia\n",
    "Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances\n",
    "Hanlei Zhang, Hua Xu, Fei Long, Xin Wang, Kai Gao\n",
    "MAGE: Machine-generated Text Detection in the Wild\n",
    "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang\n",
    "PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models\n",
    "Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi YAO, Yuan Yao, Yangqiu Song\n",
    "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators\n",
    "Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, EngSiong Chng\n",
    "Exploring Chain-of-Thought for Multi-modal Metaphor Detection\n",
    "Yanzhi Xu, Yueying Hua, Shichen Li, Zhongqing Wang\n",
    "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation\n",
    "DaYou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi Xu\n",
    "A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation\n",
    "Kai Chen, Ye Wang, Yitong Li, Aiping Li, Han Yu, Xin Song\n",
    "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation\n",
    "Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou\n",
    "CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers\n",
    "Yong Hu, Fandong Meng, Jie Zhou\n",
    "Evaluating Dynamic Topic Models\n",
    "Charu Karakkaparambil James, Mayank Nagda, Nooshin Haji Ghassemi, Marius Kloft, Sophie Fellenz\n",
    "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition\n",
    "Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou\n",
    "Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification\n",
    "Shanshan Xu, Santosh T.Y.S.S, Oana Ichim, Barbara Plank, Matthias Grabmair\n",
    "Inference to the Best Explanation in Large Language Models\n",
    "Dhairya Dalal, Marco Valentino, Andre Freitas, Paul Buitelaar\n",
    "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus\n",
    "Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu\n",
    "DeVAn: Dense Video Annotation for Video-Language Models\n",
    "Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fang, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang\n",
    "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering\n",
    "Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang\n",
    "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs\n",
    "Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu\n",
    "SciMON: Scientific Inspiration Machines Optimized for Novelty\n",
    "Qingyun Wang, Doug Downey, Heng Ji, Tom Hope\n",
    "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction\n",
    "Yiren Jian, Tingkai Liu, Yunzhe Tao, Chunhui Zhang, Soroush Vosoughi, Hongxia Yang\n",
    "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models\n",
    "Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, Ali Emami\n",
    "Retrieval-Augmented Multilingual Knowledge Editing\n",
    "Weixuan Wang, Barry Haddow, Alexandra Birch\n",
    "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge\n",
    "Brendan Park, Madeline Janecek, Naser Ezzati-Jivan, Yifeng Li, Ali Emami\n",
    "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models\n",
    "Abhishek Kumar, Sarfaroz Yunusov, Ali Emami\n",
    "Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News\n",
    "Alexandria Leto, Elliot E. Pickens, Coen D. Needell, David Rothschild, Maria Leonor Pacheco\n",
    "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences\n",
    "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong Huang\n",
    "TTM-RE: Memory-Augmented Document-Level Relation Extraction\n",
    "Chufan Gao, Xuan Wang, Jimeng Sun\n",
    "Answer is All You Need: Instruction-following Text Embedding via Answering the Question\n",
    "Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang\n",
    "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification\n",
    "Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang\n",
    "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures\n",
    "Qi Cheng, Michael Boratko, Pranay Kumar Yelugam, Tim O’Gorman, Nalini Singh, Andrew McCallum, Xiang Lorraine Li\n",
    "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis\n",
    "Yueqi XIE, Minghong Fang, Renjie Pi, Neil Zhenqiang Gong\n",
    "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs\n",
    "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi\n",
    "Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy\n",
    "Gyeongeun Lee, Christina Wong, Meghan Guo, Natalie Parde\n",
    "An Information-Theoretic Approach to Analyze NLP Classification Tasks\n",
    "Luran Wang, Mark Gales, Vatsal Raina\n",
    "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders\n",
    "Yuwei Zhang, Siffi Singh, Sailik Sengupta, Igor Shalyminov, Hang Su, Hwanjun Song, Saab Mansour\n",
    "Wav2Gloss: Generating Interlinear Glossed Text from Speech\n",
    "Taiqi He, Kwanghee Choi, Lindia Tjuatja, Nathaniel Romney Robinson, Jiatong Shi, Shinji Watanabe, Graham Neubig, David R Mortensen, Lori Levin\n",
    "Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification\n",
    "Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick Brandt, Javier Osorio, Vito D’Orazio\n",
    "SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation\n",
    "Ziyao Xu, Houfeng Wang\n",
    "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following\n",
    "Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Côté, Bang Liu\n",
    "Multimodal Instruction Tuning with Conditional Mixture of LoRA\n",
    "Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wenpeng Yin, Lifu Huang\n",
    "DocLens: Multi-aspect Fine-grained Medical Text Evaluation\n",
    "Yiqing Xie, Sheng Zhang, Hao Cheng, Pengfei Liu, Zelalem Gero, Cliff Wong, Tristan Naumann, Hoifung Poon, Carolyn Rose\n",
    "FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability\n",
    "Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, Caiming Xiong\n",
    "Hyper-CL: Conditioning Sentence Representations with Hypernetworks\n",
    "Young Hyun Yoo, Jii Cha, Changhyeon Kim, Taeuk Kim\n",
    "Analysis of Multi-Source Language Training in Cross-Lingual Transfer\n",
    "Seonghoon Lim, Taejun Yun, Jinhyeon Kim, Jihun Choi, Taeuk Kim\n",
    "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions\n",
    "Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramaneswaran S, S Sakshi, Dinesh Manocha\n",
    "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants\n",
    "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa\n",
    "Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving\n",
    "Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang\n",
    "Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach\n",
    "Saehyung Lee, Sangwon Yu, Junsung Park, Jihun Yi, Sungroh Yoon\n",
    "IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction\n",
    "Inna Wanyin Lin, Ashish Sharma, Christopher Michael Rytting, Adam S Miner, Jina Suh, Tim Althoff\n",
    "Token-wise Influential Training Data Retrieval for Large Language Models\n",
    "Huawei Lin, Jikai Long, Zhaozhuo Xu, Weijie Zhao\n",
    "Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection\n",
    "Maxwell Weinzierl, Sanda Harabagiu\n",
    "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks\n",
    "Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried\n",
    "FineSurE: Fine-grained Summarization Evaluation using LLMs\n",
    "Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour\n",
    "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback\n",
    "Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi\n",
    "Prompt Refinement with Image Pivot for Text-to-Image Generation\n",
    "Jingtao Zhan, Qingyao Ai, Yiqun LIU, Yingwei Pan, Ting Yao, Jiaxin Mao, Shaoping Ma, Tao Mei\n",
    "The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models\n",
    "Adithya Bhaskar, Dan Friedman, Danqi Chen\n",
    "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation\n",
    "Masato Mita, Soichiro Murakami, Akihiko Kato, Peinan Zhang\n",
    "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation\n",
    "Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Wong, Simon See\n",
    "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs\n",
    "Runlong Zhou, Simon Shaolei Du, Beibin Li\n",
    "Can ChatGPT’s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge\n",
    "Cheng Yang, Puli Chen, Qingbao Huang\n",
    "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning\n",
    "Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, Qian Liu\n",
    "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation\n",
    "kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, Bing Qin\n",
    "RORA: Robust Free-Text Rationale Evaluation\n",
    "Zhengping Jiang, Yining Lu, Hanjie Chen, Daniel Khashabi, Benjamin Van Durme, Anqi Liu\n",
    "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents\n",
    "Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun\n",
    "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\n",
    "Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu\n",
    "L-Eval: Instituting Standardized Evaluation for Long Context Language Models\n",
    "Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu\n",
    "DIALECTBENCH: An NLP Benchmark for Dialects, Varieties, and Closely-Related Languages\n",
    "Fahim Faisal, Orevaoghene Ahia, Aarohi Srivastava, Kabir Ahuja, David Chiang, Yulia Tsvetkov, Antonios Anastasopoulos\n",
    "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction\n",
    "Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen\n",
    "Causal-Guided Active Learning for Debiasing Large Language Models\n",
    "Zhouhao Sun, Li Du, Xiao Ding, Yixuan Ma, Yang Zhao, Kaitao Qiu, Ting Liu, Bing Qin\n",
    "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models\n",
    "Aparna Elangovan, Ling Liu, Lei Xu, Sravan Babu Bodapati, Dan Roth\n",
    "Linguistically Conditioned Semantic Textual Similarity\n",
    "Jingxuan Tu, Keer Xu, Liulu Yue, Bingyang Ye, Kyeongmin Rim, James Pustejovsky\n",
    "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future\n",
    "Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu\n",
    "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models\n",
    "Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin\n",
    "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering\n",
    "Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, Bing Qin\n",
    "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base\n",
    "Siyu Yuan, Jiangjie Chen, Changzhi Sun, Jiaqing Liang, Yanghua Xiao, Deqing Yang\n",
    "TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation\n",
    "Yujie Feng, Xu Chu, Yongxin Xu, Guangyuan SHI, Bo LIU, Xiao-Ming Wu\n",
    "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n",
    "Damai Dai, Chengqi Deng, Chenggang Zhao, R.X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang\n",
    "Grounding Language Model with Chunking-Free In-Context Retrieval\n",
    "Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou\n",
    "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation\n",
    "Jiaxin Bai, Yicheng Wang, Tianshi Zheng, Yue Guo, Xin Liu, Yangqiu Song\n",
    "Active Prompting with Chain-of-Thought for Large Language Models\n",
    "Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang\n",
    "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs\n",
    "Xiangyu Zhao, Bo LIU, Qijiong Liu, Guangyuan SHI, Xiao-Ming Wu\n",
    "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search\n",
    "Haochen Li, Xin Zhou, Zhiqi Shen\n",
    "A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications\n",
    "Naomi Baes, Nick Haslam, Ekaterina Vylomova\n",
    "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal\n",
    "Jianheng Huang, Leyang Cui, Ante Wang, chengyiyang, Xinting Liao, Linfeng Song, Junfeng Yao, Jinsong Su\n",
    "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency\n",
    "Baizhou Huang, Shuai Lu, Xiaojun Wan, Nan Duan\n",
    "Citation-Enhanced Generation for LLM-based Chatbots\n",
    "Weitao Li, Junkai Li, Weizhi Ma, Yang Liu\n",
    "Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection\n",
    "Haoyang Wen, Eduard Hovy, Alexander G Hauptmann\n",
    "Feature-Adaptive and Data-Scalable In-Context Learning\n",
    "Jiahao Li, Quan Wang, Licheng Zhang, Guoqing Jin, Zhendong Mao\n",
    "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games\n",
    "Yizhe Zhang, Jiarui Lu, Navdeep Jaitly\n",
    "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models\n",
    "Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li\n",
    "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models\n",
    "Yida Zhao, Chao Lou, Kewei Tu\n",
    "A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation\n",
    "Zhengrui Ma, Qingkai Fang, Shaolei Zhang, Shoutao Guo, Yang Feng, Min zhang\n",
    "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents\n",
    "Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang\n",
    "Probing Language Models for Pre-training Data Detection\n",
    "Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Bing Liu, Haonan Lu, Wenliang Chen\n",
    "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding\n",
    "Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, Tat-Seng Chua\n",
    "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation\n",
    "Senyu Han, Lu Chen, Li-Min Lin, Zhengshan Xu, Kai Yu\n",
    "Language Model Adaption for Reinforcement Learning with Natural Language Action Space\n",
    "Jiangxing Wang, Jiachen Li, Xiao Han, Deheng Ye, Zongqing Lu\n",
    "Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues\n",
    "Hiromasa Sakurai, Yusuke Miyao\n",
    "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression\n",
    "Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu\n",
    "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model\n",
    "Chuhao Jin, Kening Ren, Lingzhen Kong, Xiting Wang, Ruihua Song, huan chen\n",
    "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy\n",
    "Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min Peng, Weiguang Han, Jimin Huang\n",
    "Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition\n",
    "Zirun Guo, Tao Jin, Zhou Zhao\n",
    "An Effective Pronunciation Assessment Approach Leveraging Hierarchical Transformers and Pre-training Strategies\n",
    "Bi-Cheng Yan, Jiun-Ting Li, Yi-Cheng Wang, Hsin Wei Wang, Tien-Hong Lo, Yung-Chang Hsu, Wei-Cheng Chao, Berlin Chen\n",
    "Detection-Correction Structure via General Language Model for Grammatical Error Correction\n",
    "Wei Li, Houfeng Wang\n",
    "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer\n",
    "Yongxin Zhu, Dan Su, Liqiang He, Linli Xu, Dong Yu\n",
    "Selene: Pioneering Automated Proof in Software Verification\n",
    "Lichen Zhang, Shuai Lu, Nan Duan\n",
    "Dissecting Human and LLM Preferences\n",
    "Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, hai zhao, Pengfei Liu\n",
    "UniCoder: Scaling Code Large Language Model via Universal Code\n",
    "Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun Yang, Zhoujun Li\n",
    "AoE: Angle-optimized Embeddings for Semantic Textual Similarity\n",
    "Xianming LI, Jing Li\n",
    "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews\n",
    "Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao\n",
    "Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better\n",
    "Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu Li, Zhaohan Zhang, Yu Lan, Chao Shen\n",
    "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\n",
    "Jingwei Ni, Minjing Shi, Dominik Stammbach, Mrinmaya Sachan, Elliott Ash, Markus Leippold\n",
    "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering\n",
    "Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold\n",
    "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin\n",
    "Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei Shen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhiheng Xi, Xiaoran Fan, Shiliang Pu, Zhu Jiang, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang\n",
    "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation\n",
    "Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, Helen M. Meng\n",
    "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions\n",
    "Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun xu, Wei Shi\n",
    "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension\n",
    "Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou\n",
    "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies\n",
    "Tom Kocmi, Vilém Zouhar, Christian Federmann, Matt Post\n",
    "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models\n",
    "Yuanyi Ren, Haoran Ye, Hanjun Fang, Xin Zhang, Guojie Song\n",
    "DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction\n",
    "Ling Hu, Yuemei Xu\n",
    "SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations\n",
    "Jesus Solano, Mardhiyah Sanni, Oana-Maria Camburu, Pasquale Minervini\n",
    "Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation\n",
    "Wen Wu, Bo Li, Chao Zhang, Chung-Cheng Chiu, Qiujia Li, Junwen Bai, Tara N Sainath, Phil Woodland\n",
    "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation\n",
    "Jinyuan Fang, Zaiqiao Meng, Craig MacDonald\n",
    "Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks\n",
    "Yingji Zhang, Danilo Carvalho, Andre Freitas\n",
    "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation\n",
    "Yan Ma, Yu Qiao, Pengfei Liu\n",
    "Open-Set Semi-Supervised Text Classification via Adversarial Disagreement Maximization\n",
    "Junfan Chen, Richong Zhang, Junchi Chen, Chunming Hu\n",
    "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages\n",
    "Junjie Ye, Sixian Li, Guanyu Li, Huangcaishuang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huang\n",
    "A synthetic data approach for domain generalization of NLI models\n",
    "Mohammad Javad Hosseini, Andrey Petrov, Alex Fabrikant, Annie Louis\n",
    "Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild\n",
    "Ting Wu, Jingyi Liu, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang\n",
    "LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models\n",
    "Jiaqi Zhao, Miao Zhang, Chao Zeng, Ming Wang, Xuebo Liu, Liqiang Nie\n",
    "VariErr NLI: Separating Annotation Error from Human Label Variation\n",
    "Leon Weber-Genzel, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank\n",
    "Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient\n",
    "Mingxin Li, Richong Zhang, Zhijie Nie\n",
    "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation\n",
    "Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan\n",
    "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval\n",
    "Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Hyeongu Yun, Yireun Kim, seung-won hwang\n",
    "Exploring the Potential of Large Language Models in Computational Argumentation\n",
    "Guizhen Chen, Liying Cheng, Anh Tuan Luu, Lidong Bing\n",
    "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks\n",
    "Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina\n",
    "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning\n",
    "Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Cheng Jiayang, Chunkit Chan, Yangqiu Song\n",
    "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter\n",
    "Jitai Hao, Weiwei Sun, Xin Xin, Qi Meng, Zhumin Chen, Pengjie Ren, Zhaochun Ren\n",
    "Surgical Feature-Space Decomposition of LLMs: Why, When and How?\n",
    "Arnav Chavan, Nahush Lele, Deepak Gupta\n",
    "Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance\n",
    "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Junqi Dai, Qinyuan Cheng, Xuanjing Huang, Xipeng Qiu\n",
    "Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering\n",
    "Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang\n",
    "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression\n",
    "Peiyu Liu, Ze-Feng Gao, Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen\n",
    "Emergent Word Order Universals from Cognitively-Motivated Language Models\n",
    "Tatsuki Kuribayashi, Ryo Ueda, Ryo Yoshida, Yohei Oseki, Ted Briscoe, Timothy Baldwin\n",
    "VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models\n",
    "Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee\n",
    "Making Long-Context Language Models Better Multi-Hop Reasoners\n",
    "Yanyang Li, Shuo Liang, Michael Lyu, Liwei Wang\n",
    "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models\n",
    "Yihong Liu, Chunlan Ma, Haotian Ye, Hinrich Schuetze\n",
    "Extreme Miscalibration and the Illusion of Adversarial Robustness\n",
    "Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George Karypis\n",
    "HyCoRec: Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation\n",
    "Yongsen Zheng, Ruilin Xu, Ziliang Chen, Guohua Wang, Mingjie Qian, Jinghui Qin, Liang Lin\n",
    "Co-training for Low Resource Scientific Natural Language Inference\n",
    "Mobashir Sadat, Cornelia Caragea\n",
    "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models\n",
    "Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao\n",
    "Time is Encoded in the Weights of Finetuned Language Models\n",
    "Kai Nylund, Suchin Gururangan, Noah A. Smith\n",
    "Long-Context Language Modeling with Parallel Context Encoding\n",
    "Howard Yen, Tianyu Gao, Danqi Chen\n",
    "SirLLM: Streaming Infinite Retentive LLM\n",
    "Yao Yao, Zuchao Li, hai zhao\n",
    "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models\n",
    "Tao Feng, Lizhen Qu, Zhuang Li, Haolan Zhan, YUNCHENG HUA, Reza Haf\n",
    "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale\n",
    "Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu\n",
    "MELA: Multilingual Evaluation of Linguistic Acceptability\n",
    "Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu\n",
    "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View\n",
    "Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin Deng\n",
    "CopyNE: Better Contextual ASR by Copying Named Entities\n",
    "Shilin Zhou, Zhenghua Li, Yu Hong, Min Zhang, Zhefeng Wang, Baoxing Huai\n",
    "Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval\n",
    "Peter Baile Chen, Yi Zhang, Dan Roth\n",
    "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation\n",
    "Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao\n",
    "ItD: Large Language Models Can Teach Themselves Induction through Deduction\n",
    "Wangtao Sun, Haotian Xu, Xuanqing Yu, Pei Chen, Shizhu He, Jun Zhao, Kang Liu\n",
    "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs\n",
    "Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li\n",
    "MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin\n",
    "Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Ge Yu\n",
    "Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent\n",
    "Heng-Da Xu, Xian-Ling Mao, Puhai Yang, Fanshu Sun, Heyan Huang\n",
    "On Context Utilization in Summarization with Large Language Models\n",
    "Mathieu Ravaut, Aixin Sun, Nancy F. Chen, Shafiq Joty\n",
    "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning\n",
    "Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, Zhicheng Dou\n",
    "Enhancing In-Context Learning via Implicit Demonstration Augmentation\n",
    "Xiaoling Zhou, Wei Ye, Yidong Wang, Chaoya Jiang, Zhemg Lee, Rui Xie, Shikun Zhang\n",
    "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA\n",
    "Sheng Wang, Boyang XUE, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu\n",
    "Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics\n",
    "Chun Hei Lo, Wai Lam, Hong Cheng, Guy Emerson\n",
    "Improving Event Definition Following For Zero-Shot Event Detection\n",
    "Zefan Cai, Po-Nien Kung, Ashima Suvarna, Mingyu Derek Ma, Hritik Bansal, Baobao Chang, P. Jeffrey Brantingham, Wei Wang, Nanyun Peng\n",
    "Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements\n",
    "Xiao Wei, Xu Qi, Hang Yu, Qian Liu, Erik Cambria\n",
    "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding\n",
    "Yiruo Cheng, Kelong Mao, Zhicheng Dou\n",
    "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks\n",
    "Yichen Wang, Shangbin Feng, Abe Bohan Hou, Xiao Pu, Chao Shen, Xiaoming Liu, Yulia Tsvetkov, Tianxing He\n",
    "CausalGym: Benchmarking causal interpretability methods on linguistic tasks\n",
    "Aryaman Arora, Dan Jurafsky, Christopher Potts\n",
    "Training Language Models to Generate Text with Citations via Fine-grained Rewards\n",
    "Chengyu Huang, Zeqiu Wu, Yushi Hu, Wenya Wang\n",
    "Hypergraph based Understanding for Document Semantic Entity Recognition\n",
    "Qiwei Li, Zuchao Li, Ping Wang, Haojun Ai, hai zhao\n",
    "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers\n",
    "Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi\n",
    "Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models\n",
    "Qingkai Min, Qipeng Guo, Xiangkun Hu, Songfang Huang, Zheng Zhang, Yue Zhang\n",
    "AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning\n",
    "Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, chengfei lv, Huajun Chen\n",
    "ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks\n",
    "Santosh T.Y.S.S, Tuan-Quang Vuong, Matthias Grabmair\n",
    "Virtual Compiler Is All You Need For Assembly Code Search\n",
    "Zeyu Gao, Hao Wang, Yuanda Wang, Chao Zhang\n",
    "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning\n",
    "Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei\n",
    "Can LLMs Learn from Previous Mistakes? Investigating LLMs’ Errors to Boost for Reasoning\n",
    "Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang\n",
    "An Iterative Associative Memory Model for Empathetic Response Generation\n",
    "Zhou Yang, Zhaochun Ren, Wang Yufeng, Haizhou Sun, Chao Chen, Xiaofei Zhu, Xiangwen Liao\n",
    "Detoxifying Large Language Models via Knowledge Editing\n",
    "Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen\n",
    "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\n",
    "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li\n",
    "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models\n",
    "Yuyan Chen, Songzhou Yan, Panjun Liu, Yanghua Xiao\n",
    "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages\n",
    "Trinh Pham, Khoi M. Le, Anh Tuan Luu\n",
    "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval\n",
    "Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, yongping xiong\n",
    "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training\n",
    "Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang\n",
    "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark\n",
    "Chanjun Park, Hyeonwoo Kim, Dahyun Kim, SeongHwan Cho, Sanghoon Kim, Sukyung Lee, Yungi Kim, Hwalsuk Lee\n",
    "Unified Hallucination Detection for Multimodal Large Language Models\n",
    "Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, xiaoyan yang, Qiang Li, YUE SHEN, Lei Liang, Jinjie GU, Huajun Chen\n",
    "Empowering Character-level Text Infilling by Eliminating Sub-Tokens\n",
    "Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Hongsheng Li\n",
    "Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models\n",
    "Kun Luo, Zheng Liu, Shitao Xiao, Tong Zhou, Yubo Chen, Jun Zhao, Kang Liu\n",
    "GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?\n",
    "Dayoon Ko, Jinyoung Kim, Hahyeon Choi, Gunhee Kim\n",
    "Attribute First, then Generate: Locally-attributable Grounded Text Generation\n",
    "Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan\n",
    "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text\n",
    "Aoxiong Yin, Haoyuan Li, Kai Shen, Siliang Tang, Yueting Zhuang\n",
    "OceanGPT: A Large Language Model for Ocean Science Tasks\n",
    "Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen\n",
    "Beyond Memorization: The Challenge of Random Memory Access in Language Models\n",
    "Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin\n",
    "BIPED: Pedagogically Informed Tutoring System for ESL Education\n",
    "Soonwoo Kwon, Sojung Kim, Minju Park, Seunghyun Lee, Kyuseok Kim\n",
    "Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction\n",
    "Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu\n",
    "Collaboration or Corporate Capture? Quantifying NLP’s Reliance on Industry Artifacts and Contributions\n",
    "Will Aitken, Mohamed Abdalla, Karen Rudie, Catherine Stinson\n",
    "Prompt Expansion for Adaptive Text-to-Image Generation\n",
    "Siddhartha Datta, Alexander Ku, Deepak Ramachandran, Peter Anderson\n",
    "Progressively Modality Freezing for Multi-Modal Entity Alignment\n",
    "Yani Huang, Xuefeng Zhang, Richong Zhang, Junfan Chen, Jaein Kim\n",
    "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval\n",
    "Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, Defu Lian\n",
    "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts\n",
    "Xuan-Phi Nguyen, Mahani Aljunied, Shafiq Joty, Lidong Bing\n",
    "Metaphor Understanding Challenge Dataset for LLMs\n",
    "Xiaoyu Tong, Rochelle Choenni, Martha Lewis, Ekaterina Shutova\n",
    "A Multi-Task Embedder For Retrieval Augmented LLMs\n",
    "Peitian Zhang, Zheng Liu, Shitao Xiao, Zhicheng Dou, Jian-Yun Nie\n",
    "Language Models Don’t Learn the Physical Manifestation of Language\n",
    "Bruce W Lee, Jaehyuk Lim\n",
    "Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\n",
    "Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov\n",
    "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection\n",
    "Shangbin Feng, Herun Wan, Ningnan Wang, Zhaoxuan Tan, Minnan Luo, Yulia Tsvetkov\n",
    "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives\n",
    "Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu\n",
    "Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty\n",
    "Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Maarten Sap\n",
    "Mission: Impossible Language Models\n",
    "Julie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, Christopher Potts\n",
    "Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources\n",
    "Xiaochen Wang, Junyu Luo, Jiaqi Wang, Yuan Zhong, Xiaokun Zhang, Yaqing Wang, Parminder Bhatia, Cao Xiao, Fenglong Ma\n",
    "Semisupervised Neural Proto-Language Reconstruction\n",
    "Liang Lu, Peirong Xie, David R Mortensen\n",
    "When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP\n",
    "Sara Papi, Marco Gaido, Andrea Pilzer, Matteo Negri\n",
    "SBAAM! Eliminating Transcript Dependency in Automatic Subtitling\n",
    "Marco Gaido, Sara Papi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli\n",
    "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?\n",
    "Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli\n",
    "StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection\n",
    "Sara Papi, Marco Gaido, Matteo Negri, Luisa Bentivogli\n",
    "ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling\n",
    "LingXi Zhang, Yue Yu, Kuan Wang, Chao Zhang\n",
    "Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference\n",
    "Jihwan Bang, Juntae Lee, Kyuhong Shim, Seunghan Yang, Simyung Chang\n",
    "FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model\n",
    "Yebin Lee, Imseong Park, Myungjoo Kang\n",
    "MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations\n",
    "Yuxin Wang, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi\n",
    "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning\n",
    "Zhenlong Dai, Chang Yao, WenKang Han, Yuanying, Zhipeng Gao, Jingyuan Chen\n",
    "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\n",
    "Ajay Patel, Colin Raffel, Chris Callison-Burch\n",
    "Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective\n",
    "Chenze Shao, Fandong Meng, Jiali Zeng, Jie Zhou\n",
    "Identifying while Learning for Document Event Causality Identification\n",
    "Cheng Liu, Wei Xiang, Bang Wang\n",
    "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems\n",
    "Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun\n",
    "Insert or Attach: Taxonomy Completion via Box Embedding\n",
    "Wei Xue, Yongliang Shen, Wenqi Ren, Jietian Guo, Shiliang Pu, Weiming Lu\n",
    "Semiparametric Token-Sequence Co-Supervision\n",
    "Hyunji Lee, Doyoung Kim, Jihoon Jun, Se June Joo, Joel Jang, Kyoung-Woon On, Minjoon Seo\n",
    "Instruction Fusion: Advancing Prompt Evolution through Hybridization\n",
    "Weidong Guo, Jiuding Yang, Kaitong Yang, Xiangyang Li, Zhuwei Rao, Yu Xu, Di Niu\n",
    "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation\n",
    "Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, Jiangjie Chen\n",
    "Exploring Memorization in Fine-tuned Language Models\n",
    "Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin\n",
    "Towards Real-world Scenario: Imbalanced New Intent Discovery\n",
    "Shun Zhang, Yan Chaoran, Jian Yang, Jiaheng Liu, Ying Mo, Jiaqi Bai, Tongliang Li, Zhoujun Li\n",
    "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection\n",
    "Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, OSAMA MOHAMMED AFZAL, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov\n",
    "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue\n",
    "Jian Wang, Chak Tou Leong, Jiashuo WANG, Dongding Lin, Wenjie Li, Xiaoyong Wei\n",
    "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training\n",
    "Nan He, Weichen Xiong, Hanwen Liu, Yi Liao, Lei Ding, Kai Zhang, Guohua Tang, Xiao Han, Yang Wei\n",
    "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?\n",
    "Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun\n",
    "Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning\n",
    "Zeqi Tan, Yongliang Shen, Xiaoxia Cheng, Chang Zong, Wenqi Zhang, Jian Shao, Weiming Lu, Yueting Zhuang\n",
    "CaMML: Context-Aware Multimodal Learner for Large Models\n",
    "Yixin Chen, Shuai Zhang, Boran Han, Tong He, Bo Li\n",
    "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation\n",
    "Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li\n",
    "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes\n",
    "Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang\n",
    "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models\n",
    "Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang\n",
    "Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?\n",
    "Roshan Sharma, Suwon Shon, Mark Lindsey, Hira Dhamyal, Bhiksha Raj\n",
    "Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors\n",
    "Alicja Chaszczewicz, Raj Sanjay Shah, Ryan Louie, Bruce A Arnow, Robert Kraut, Diyi Yang\n",
    "D2LLM: Decomposed and Distilled Large Language Models for Semantic Search\n",
    "Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang\n",
    "In­-context Mixing (ICM): Code­-mixed Prompts for Multilingual LLMs\n",
    "Bhavani Shankar, Preethi Jyothi, Pushpak Bhattacharyya\n",
    "Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models\n",
    "Liang Zhang, Qin Jin, Haoyang Huang, Dongdong Zhang, Furu Wei\n",
    "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries\n",
    "Yu-Hsiang Huang, Yuche Tsai, Hsiang Hsiao, Hong-Yi Lin, Shou-De Lin\n",
    "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding\n",
    "Kuo Liao, Shuang Li, Meng Zhao, Liqun Liu, Mengge Xue, zhenyu hu, Honglin Han, Chengguo Yin\n",
    "Intuitive or Dependent? Investigating LLMs’ Behavior Style to Conflicting Prompts\n",
    "Jiahao Ying, Yixin Cao, Kai Xiong, Long Cui, yidong He, Yongbin Liu\n",
    "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending\n",
    "Shiyi Zhu, Jing Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo Li\n",
    "Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization\n",
    "Salman Elgamal, Ossama Obeid, MHD Tameem Kabbani, Go Inoue, Nizar Habash\n",
    "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification\n",
    "Jan Trienes, Sebastian Antony Joseph, Jörg Schlötterer, Christin Seifert, Kyle Lo, Wei Xu, Byron C Wallace, Junyi Jessy Li\n",
    "Disinformation Capabilities of Large Language Models\n",
    "Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro, Dominik Macko, Maria Bielikova\n",
    "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models\n",
    "Junhao Zheng, Shengjie Qiu, Qianli Ma\n",
    "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following\n",
    "Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou\n",
    "DAPR: A Benchmark on Document-Aware Passage Retrieval\n",
    "Kexin Wang, Nils Reimers, Iryna Gurevych\n",
    "How to Handle Different Types of Out-of-Distribution Scenarios in Computational Argumentation? A Comprehensive and Fine-Grained Field Study\n",
    "Andreas Waldis, Yufang Hou, Iryna Gurevych\n",
    "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors\n",
    "Mengge Xue, zhenyu hu, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Meng Zhao, Chengguo Yin\n",
    "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph\n",
    "Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, Jieping Ye\n",
    "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages\n",
    "Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri, Wawan Cenggoro, Jhonson Lee, Salsabil Maulana Akbar, Emmanuel Dave, Nuurshadieq, Muhammad Ihza Mahendra, Rr Dea Annisayanti Putri, Bryan Wilie, Genta Indra Winata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung\n",
    "Uncertainty-Guided Modal Rebalance for Hateful Memes Detection\n",
    "Chuanpeng Yang, Yaxin Liu, Fuqing Zhu, Jizhong Han, Songlin Hu\n",
    "Must NLP be Extractive?\n",
    "Steven Bird\n",
    "Spiral of Silence: How is Large Language Model Killing Information Retrieval?—A Case Study on Open Domain Question Answering\n",
    "Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, Yingfei Sun\n",
    "Missci: Reconstructing Fallacies in Misrepresented Science\n",
    "Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych\n",
    "Uncovering the Full Potential of Visual Grounding Methods in VQA\n",
    "Daniel Reich, Tanja Schultz\n",
    "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs\n",
    "Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen\n",
    "Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative AI Evaluation\n",
    "Pius von Däniken, Jan Milan Deriu, Don Tuggener, Mark Cieliebak\n",
    "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback\n",
    "Timon Ziegenbein, Gabriella Skitalinskaya, Alireza Bayat Makou, Henning Wachsmuth\n",
    "Graph Language Models\n",
    "Moritz Plenz, Anette Frank\n",
    "Analyzing Semantic Change through Lexical Replacements\n",
    "Francesco Periti, Pierluigi Cassotti, Haim Dubossarsky, Nina Tahmasebi\n",
    "Exploiting Intrinsic Multilateral Logical Rules for Weakly Supervised Natural Language Video Localization\n",
    "Zhe Xu, Kun Wei, Xu Yang, Cheng Deng\n",
    "Latxa: An Open Language Model and Evaluation Suite for Basque\n",
    "Julen Etxaniz, Oscar Sainz, Naiara Perez Miguel, Itziar Aldabe, German Rigau, Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, Aitor Soroa\n",
    "Interpretability of Language Models via Task Spaces\n",
    "Lucas Weber, Jaap Jumelet, Elia Bruni, Dieuwke Hupkes\n",
    "Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types\n",
    "Pierluigi Cassotti, Stefano De Pascale, Nina Tahmasebi\n",
    "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators\n",
    "Matéo Mahaut, Laura Aina, Paula Czarnowska, Momchil Hardalov, Thomas Müller, Lluis Marquez\n",
    "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback\n",
    "Shihan Dou, Yan Liu, Haoxiang Jia, Enyu Zhou, Limao Xiong, Junjie Shan, Huangcaishuang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Tao Gui, Xuanjing Huang\n",
    "One-Shot Learning as Instruction Data Prospector for Large Language Models\n",
    "Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li\n",
    "Navigating the OverKill in Large Language Models\n",
    "Chenyu Shi, Xiao Wang, Qiming Ge, Songyang Gao, Xianjun Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Xun Zhao, Dahua Lin\n",
    "Why are Sensitive Functions Hard for Transformers?\n",
    "Michael Hahn, Mark Rofin\n",
    "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains\n",
    "Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva\n",
    "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision\n",
    "Qian Ruan, Ilia Kuznetsov, Iryna Gurevych\n",
    "NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents\n",
    "Tamara Czinczoll, Christoph Hönes, Maximilian Schall, Gerard de Melo\n",
    "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models\n",
    "Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang\n",
    "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction\n",
    "Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu\n",
    "Learning to Edit: Aligning LLMs with Knowledge Editing\n",
    "Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang\n",
    "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning\n",
    "Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Weiran Xu, Jingang Wang, Mengdi Zhang, Xunliang Cai\n",
    "IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators\n",
    "Indraneil Paul, Goran Glavaš, Iryna Gurevych\n",
    "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality\n",
    "Brielen Madureira, Patrick Kahardipraja, David Schlangen\n",
    "SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models\n",
    "Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych\n",
    "Planning Like Human: A Dual-process Framework for Dialogue Planning\n",
    "Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, Bing Qin\n",
    "Spectral Filters, Dark Signals, and Attention Sinks\n",
    "Nicola Cancedda\n",
    "DiffuCOMET: Contextual Commonsense Knowledge Diffusion\n",
    "Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut\n",
    "Systematic Task Exploration with LLMs: A Study in Citation Text Generation\n",
    "Furkan Şahinuç, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych\n",
    "The Echoes of Multilinguality: Tracing Cultural Value Shifts during Language Model Fine-tuning\n",
    "Rochelle Choenni, Anne Lauscher, Ekaterina Shutova\n",
    "Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition\n",
    "Matteo Bortoletto, Constantin Ruhdorfer, Adnen Abdessaied, Lei Shi, Andreas Bulling\n",
    "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling\n",
    "Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, Luke Zettlemoyer\n",
    "Temporal Knowledge Question Answering via Abstract Reasoning Induction\n",
    "Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang\n",
    "MultiLegalPile: A 689GB Multilingual Legal Corpus\n",
    "Joel Niklaus, Veton Matoshi, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho\n",
    "Who Wrote this Code? Watermarking for Code Generation\n",
    "Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim\n",
    "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving\n",
    "Md. Ashraful Islam, Mohammed Eunus Ali, Md Rizwan Parvez\n",
    "RelayAttention for Efficient Large Language Model Serving with Long System Prompts\n",
    "Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau\n",
    "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
    "Jianing Wang, Qiushi Sun, Xiang Li, Ming Gao\n",
    "Open Grounded Planning: Challenges and Benchmark Construction\n",
    "Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun\n",
    "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations\n",
    "Haolin Deng, Chang Wang, Li Xin, Dezhang Yuan, Junlang Zhan, Tian Hua Zhou, Jin Ma, Jun Gao, Ruifeng Xu\n",
    "LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures\n",
    "Chenghao Xu, Guangtao Lyu, Jiexi Yan, Muli Yang, Cheng Deng\n",
    "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction\n",
    "Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu\n",
    "PITA: Prompting Task Interaction for Argumentation Mining\n",
    "Yang Sun, Muyi Wang, Jianzhu Bao, Bin Liang, Xiaoyan Zhao, Caihua Yang, Min Yang, Ruifeng Xu\n",
    "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models\n",
    "Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu\n",
    "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations\n",
    "Gregor Geigle, Radu Timofte, Goran Glavaš\n",
    "Estimating Agreement by Chance for Sequence Annotation\n",
    "Diya Li, Carolyn Rose, Ao Yuan, Chunxiao Zhou\n",
    "What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages\n",
    "Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell\n",
    "Are Emergent Abilities in Large Language Models just In-Context Learning?\n",
    "Sheng Lu, Irina Bigoulaeva, Rachneet Singh Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych\n",
    "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning\n",
    "Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin\n",
    "Eliciting Better Multilingual Structured Reasoning from LLMs through Code\n",
    "Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, Saab Mansour\n",
    "OLIVE: Object Level In-Context Visual Embeddings\n",
    "Timothy Ossowski, Junjie Hu\n",
    "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\n",
    "Jiuhai Chen, Jonas Mueller\n",
    "Marathon: A Race Through the Realm of Long Context with Large Language Models\n",
    "Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi Yang, Junhao Liu, Longze Chen, Run Luo, Min Yang\n",
    "Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph\n",
    "Xiaochen Kev Gao, Feng Yao, Kewen Zhao, Beilei He, Animesh Kumar, Vish Krishnan, Jingbo Shang\n",
    "PCAD: Towards ASR-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling\n",
    "Xianwei Zhuang, Xuxin Cheng, Liming Liang, Yuxin Xie, Zhichang Wang, Zhiqi Huang, Yuexian Zou\n",
    "Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment\n",
    "Tao Jin, Wang Lin, Ye Wang, Linjun Li, Xize Cheng, Zhou Zhao\n",
    "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation\n",
    "Xun Liang, Shichao Song, Simin Niu, Zhiyu li, Feiyu Xiong, Bo Tang, Yezhaohui Wang, Dawei He, Cheng Peng, Zhonghao Wang, Haiying Deng\n",
    "PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers\n",
    "Weizhe Lin, Jingbiao Mei, Jinghong Chen, Bill Byrne\n",
    "Triple-Encoders: Representations That Fire Together, Wire Together\n",
    "Justus-Jonas Erker, Florian Mai, Nils Reimers, Gerasimos Spanakis, Iryna Gurevych\n",
    "Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning\n",
    "Jingbiao Mei, Jinghong Chen, Weizhe Lin, Bill Byrne, Marcus Tomalin\n",
    "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\n",
    "Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu\n",
    "Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous Constituency Parsing\n",
    "Behzad Shayegh, Yuqiao Wen, Lili Mou\n",
    "Your Transformer is Secretly Linear\n",
    "Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Nikolai Gerasimenko, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov\n",
    "Noise Correction on Subjective Datasets\n",
    "Uthman Jinadu, Yi Ding\n",
    "Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers\n",
    "Lütfi Kerem Senel, Besnik Fetahu, Davis Yoshida, Zhiyu Chen, Giuseppe Castellucci, Nikhita Vedula, Jason Ingyu Choi, Shervin Malmasi\n",
    "Instruction-tuned Language Models are Better Knowledge Learners\n",
    "Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srini Iyer\n",
    "What Do Language Models Hear? Probing for Auditory Representations in Language Models\n",
    "Jerry Ngo, Yoon Kim\n",
    "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs\n",
    "Zae Myung Kim, Kwang Hee Lee, Preston Zhu, Vipul Raheja, Dongyeop Kang\n",
    "Jailbreak Open-Sourced Large Language Models via Enforced Decoding\n",
    "Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu\n",
    "NICE: To Optimize In-Context Examples or Not?\n",
    "Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma\n",
    "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation\n",
    "Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng\n",
    "Digital Socrates: Evaluating LLMs through Explanation Critiques\n",
    "Yuling Gu, Oyvind Tafjord, Peter Clark\n",
    "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding\n",
    "Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran\n",
    "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs\n",
    "Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran\n",
    "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?\n",
    "Guijin Son, SangWon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim\n",
    "ChatDev: Communicative Agents for Software Development\n",
    "Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, dahai li, Zhiyuan Liu, Maosong Sun\n",
    "Experiential Co-Learning of Software-Developing Agents\n",
    "Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, YiFei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, Maosong Sun\n",
    "Learning Geometry-Aware Representations for New Intent Discovery\n",
    "Kai Tang, Junbo Zhao, Xiao Ding, Runze Wu, Lei Feng, Gang Chen, Haobo Wang\n",
    "Speaker Verification in Agent-generated Conversations\n",
    "Yizhe Yang, Palakorn Achananuparp, Heyan Huang, Jing Jiang, Ee-Peng Lim\n",
    "Benchmarking Data Science Agents\n",
    "Yuge Zhang, Qiyang Jiang, XingyuHan, Nan Chen, Yuqing Yang, Kan Ren\n",
    "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models\n",
    "Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen\n",
    "Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models\n",
    "Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang\n",
    "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques\n",
    "Megh Thakkar, Quentin Fournier, Matthew D Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar\n",
    "Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation\n",
    "Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang\n",
    "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking\n",
    "Jian Luo, Xuanang Chen, Ben He, Le Sun\n",
    "RepCodec: A Speech Representation Codec for Speech Tokenization\n",
    "Zhichao Huang, Chutong Meng, Tom Ko\n",
    "Disentangled Learning with Synthetic Parallel Data for Text Style Transfer\n",
    "Jingxuan Han, Quan Wang, Zikang Guo, Benfeng Xu, Licheng Zhang, Zhendong Mao\n",
    "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick\n",
    "Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen, Yanghua Xiao\n",
    "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety\n",
    "Zaibin Zhang, Yongting Zhang, Lijun Li, Jing Shao, Hongzhi Gao, Yu Qiao, Lijun Wang, Huchuan Lu, Feng Zhao\n",
    "Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection\n",
    "Zihan Ma, Minnan Luo, Hao Guo, Zhi Zeng, Yiran Hao, Xiang Zhao\n",
    "Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions\n",
    "Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou\n",
    "Stealthy Attack on Large Language Model based Recommendation\n",
    "Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang\n",
    "Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning\n",
    "Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Lee, Jungseul Ok\n",
    "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models\n",
    "Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao, Ji-Rong Wen, Rui Yan, Yongbin Li\n",
    "SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning\n",
    "Guoxin Chen, kexin Tang, Chao Yang, Fuying Ye, Yu Qiao, Yiming Qian\n",
    "Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning\n",
    "Yeachan Kim, Junho Kim, SangKeun Lee\n",
    "SparseFlow: Accelerating Transformers by Sparsifying Information Flows\n",
    "Yeachan Kim, SangKeun Lee\n",
    "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding\n",
    "Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua\n",
    "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models\n",
    "Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang\n",
    "EmoBench: Evaluating the Emotional Intelligence of Large Language Models\n",
    "Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna Shiergetya Sunaryo, Tatia M.C. Lee, Rada Mihalcea, Minlie Huang\n",
    "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation\n",
    "Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo\n",
    "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?\n",
    "Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang\n",
    "FinTextQA: A Dataset for Long-form Financial Question Answering\n",
    "Jian Chen, Peilin Zhou, Yining Hua, Loh Ying Xin, Kehui chen, Ziyuan Li, Bing Zhu, Junwei Liang\n",
    "On Measuring Faithfulness or Self-consistency of Natural Language Explanations\n",
    "Letitia Parcalabescu, Anette Frank\n",
    "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens\n",
    "Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, Maosong Sun\n",
    "Learning or Self-aligning? Rethinking Instruction Fine-tuning\n",
    "Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng, Wan Guanglu, Xunliang Cai, Le Sun\n",
    "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?\n",
    "Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song\n",
    "Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct LLM in Knowledge-based VQA\n",
    "Qunbo Wang, Ruyi Ji, Tianhao Peng, Wenjun Wu, Zechao Li, Jing Liu\n",
    "TasTe: Teaching Large Language Models to Translate through Self-Reflection\n",
    "Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang\n",
    "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models\n",
    "Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li\n",
    "Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models\n",
    "Tharindu Madusanka, Ian Pratt-Hartmann, Riza Batista-Navarro\n",
    "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion\n",
    "Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao\n",
    "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities\n",
    "David Stap, Eva Hasler, Bill Byrne, Christof Monz, Ke Tran\n",
    "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models\n",
    "Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schuetze, Dirk Hovy\n",
    "AI ‘News’ Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian\n",
    "Giovanni Puccetti, Anna Rogers, Chiara Alzetta, Felice Dell’Orletta, Andrea Esuli\n",
    "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?\n",
    "Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng\n",
    "Unveiling Linguistic Regions in Large Language Models\n",
    "Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang\n",
    "Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment\n",
    "Zhiqing Hong, Rongjie Huang, Xize Cheng, Yongqi Wang, Ruiqi Li, Fuming You, Zhou Zhao, Zhimeng Zhang\n",
    "FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection\n",
    "Yufei Huang, Xu Han, Maosong Sun\n",
    "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations\n",
    "Yisong Miao, Hongfu Liu, Wenqiang Lei, Nancy F. Chen, Min-Yen Kan\n",
    "An Open Multilingual System for Scoring Readability of Wikipedia\n",
    "Mykola Trokhymovych, Indira Sen, Martin Gerlach\n",
    "Unlearning Traces the Influential Training Data of Language Models\n",
    "Masaru Isonuma, Ivan Titov\n",
    "Exploring Alignment in Shared Cross-lingual Spaces\n",
    "Basel Mousi, Nadir Durrani, Fahim Dalvi, Majd Hawasly, Ahmed Abdelali\n",
    "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models\n",
    "Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, Michael Lyu\n",
    "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner\n",
    "Jinglong Gao, Xiao Ding, Yiming Cui, Jianbai Zhao, Hepeng Wang, Ting Liu, Bing Qin\n",
    "WRP: Weight Recover Prune for Structured Sparsity\n",
    "Zhendong Tan, Xingjun Zhang, Zheng Wei\n",
    "Error-preserving Automatic Speech Recognition of Young English Learners’ Language\n",
    "Janick Michot, Manuela Hürlimann, Jan Milan Deriu, Luzia Sauer, Katsiaryna Mlynchyk, Mark Cieliebak\n",
    "DiFiNet: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition\n",
    "Yuxiang Cai, Qiao Liu, Yanglei Gan, Run Lin, Changlin Li, Xueyi Liu, Da Luo, JiayeYang\n",
    "Legal Case Retrieval: A Survey of the State of the Art\n",
    "Yi Feng, Chuanyi Li, Vincent Ng\n",
    "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models\n",
    "Mosh Levy, Alon Jacoby, Yoav Goldberg\n",
    "Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation\n",
    "Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, Zhendong Mao\n",
    "LLaMA Pro: Progressive LLaMA with Block Expansion\n",
    "Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, Ping Luo\n",
    "Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning\n",
    "Feiteng Mu, Wenjie Li\n",
    "A Causal Approach for Counterfactual Reasoning in Narratives\n",
    "Feiteng Mu, Wenjie Li\n",
    "SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation\n",
    "Matthias Lindemann, Alexander Koller, Ivan Titov\n",
    "The Hidden Space of Transformer Language Adapters\n",
    "Jesujoba Oluwadara Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, Mor Geva\n",
    "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts\n",
    "Nafis Irtiza Tripto, Saranya Venkatraman, Dominik Macko, Robert Moro, Ivan Srba, Adaku Uchendu, Thai Le, Dongwon Lee\n",
    "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations\n",
    "Guan-Ting Lin, Cheng-Han Chiang, Hung-yi Lee\n",
    "RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions\n",
    "Prayushi Faldu, Indrajit Bhattacharya, Mausam .\n",
    "GroundingGPT: Language Enhanced Multi-modal Grounding Model\n",
    "Zhaowei Li, Xu Qi, Dong Zhang, Hang Song, YiQing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Van Tu, Zhida Huang, Tao Wang\n",
    "Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches\n",
    "Islam Eldifrawi, Shengrui Wang, Amine Trabelsi\n",
    "Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages\n",
    "Carlos Mullov, Quan Pham, Alexander Waibel\n",
    "SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget\n",
    "Rui Kong, Yuanchun Li, qingtian feng, Weijun Wang, Xiaozhou Ye, Ye Ouyang, Linghe Kong, Yunxin Liu\n",
    "PixT3: Pixel-based Table-To-Text Generation\n",
    "Iñigo Alonso, Eneko Agirre, Mirella Lapata\n",
    "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers\n",
    "Gal Yona, Roee Aharoni, Mor Geva\n",
    "TAMS: Translation-Assisted Morphological Segmentation\n",
    "Enora Rice, Ali Marashian, Luke Gessler, Alexis Palmer, Katharina von der Wense\n",
    "Disambiguate Words like Composing Them: A Morphology-Informed Approach to Enhance Chinese Word Sense Disambiguation\n",
    "Yue Wang, Qiliang Liang, Yaqi Yin, Hansi Wang, Yang Liu\n",
    "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval\n",
    "Mohammad Abdullah Matin Khan, M Saiful Bari, Do Xuan Long, Weishi Wang, Md Rizwan Parvez, Shafiq Joty\n",
    "ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models\n",
    "Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song\n",
    "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia\n",
    "Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kiciman, Hamid Palangi, Barun Patra, Robert West\n",
    "Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA\n",
    "Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Yang Zhao, Xinze Guan, Xin Eric Wang\n",
    "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models\n",
    "Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu\n",
    "Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms\n",
    "Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak\n",
    "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations\n",
    "Ritam Dutt, Zhen Wu, Jiaxin Shi, Divyanshu Sheth, Prakhar Gupta, Carolyn Rose\n",
    "Robust Frame-Semantic Models with Lexical Unit Trees and Negative Samples\n",
    "Jacob Devasier, Yogesh Gurjar, Chengkai Li\n",
    "Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
    "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West\n",
    "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation\n",
    "Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, Faramarz Fekri\n",
    "Lightweight reranking for language model generations\n",
    "Siddhartha Jain, Xiaofei Ma, Anoop Deoras, Bing Xiang\n",
    "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews\n",
    "Mike D’Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, Doug Downey\n",
    "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks\n",
    "Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe\n",
    "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning\n",
    "Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri\n",
    "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning\n",
    "Inderjeet Jayakumar Nair, Lu Wang\n",
    "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
    "Justin Chen, Swarnadeep Saha, Mohit Bansal\n",
    "Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning\n",
    "Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He\n",
    "Where Do People Tell Stories Online? Story Detection Across Online Communities\n",
    "Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper\n",
    "Large Language Models Are No Longer Shallow Parsers\n",
    "Yuanhe Tian, Fei Xia, Yan Song\n",
    "Dialogue Summarization with Mixture of Experts based on Large Language Models\n",
    "Yuanhe Tian, Fei Xia, Yan Song\n",
    "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences\n",
    "Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Yongdong Zhang\n",
    "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs\n",
    "Daking Rai, Ziyu Yao\n",
    "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling\n",
    "Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex Pentland, Yoon Kim, Deb Roy, Jad Kabbara\n",
    "Intrinsic Task-based Evaluation for Referring Expression Generation\n",
    "Guanyi Chen, Fahime Same, Kees Van Deemter\n",
    "From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models\n",
    "Qisheng Hu, Geonsik Moon, Hwee Tou Ng\n",
    "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction\n",
    "Kunxun Qi, Jianfeng Du, Hai Wan\n",
    "Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?\n",
    "Qingkai Fang, Shaolei Zhang, Zhengrui Ma, Min zhang, Yang Feng\n",
    "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder\n",
    "Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min zhang, Zhiguo Zhang\n",
    "G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation\n",
    "Xingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Shanbo Cheng\n",
    "CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers\n",
    "Longwei Zou, Qingyang Wang, Han Zhao, jiangangkong, YI YANG, Yangdong Deng\n",
    "Prompt Optimization via Adversarial In-Context Learning\n",
    "Do Xuan Long, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Shieh, Junxian He\n",
    "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion\n",
    "Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Lei Xie, Yuping Wang\n",
    "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering\n",
    "Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren\n",
    "Multimodal Contextualized Semantic Parsing from Speech\n",
    "Jordan Voas, David Harwath, Ray Mooney\n",
    "LaMP: When Large Language Models Meet Personalization\n",
    "Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani\n",
    "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters\n",
    "Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, Jesse Dodge\n",
    "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues\n",
    "Ge Bai, Jie Liu, Xingyuan Bu, yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang\n",
    "EFSA: Towards Event-Level Financial Sentiment Analysis\n",
    "Tianyu Chen, Yiming Zhang, Guoxin Yu, Dapeng Zhang, Li Zeng, Qing He, Xiang Ao\n",
    "Media Framing: A typology and Survey of Computational Approaches Across Disciplines\n",
    "Yulia Otmakhova, Shima Khanehzar, Lea Frermann\n",
    "What Evidence Do Language Models Find Convincing?\n",
    "Alexander Wan, Eric Wallace, Dan Klein\n",
    "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models\n",
    "Qihang Ai, Jiafan Li, Jincheng Dai, Jianwu Zhou, Lemao Liu, Haiyun Jiang, Shuming Shi\n",
    "LangBridge: Multilingual Reasoning Without Multilingual Supervision\n",
    "Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, Minjoon Seo\n",
    "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs\n",
    "Siyuan Wang, zhongyu wei, Yejin Choi, Xiang Ren\n",
    "SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving\n",
    "Xueliang Zhao, Xinting Huang, Wei Bi, Lingpeng Kong\n",
    "Unlocking the Power of Large Language Models for Entity Alignment\n",
    "Xuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan Li, Jian Guo, Huawei Shen, Yuanzhuo Wang\n",
    "SPZ: A Semantic Perturbation-based Data Augmentation Method with Zonal-Mixing for Alzheimer’s Disease Detection\n",
    "FangFang Li, Cheng Huang, PuZhen Su, Jie Yin\n",
    "Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents\n",
    "Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin\n",
    "ReFT: Reasoning with Reinforced Fine-Tuning\n",
    "Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, peng sun, Xiaoran Jin, Hang Li\n",
    "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment\n",
    "yunxin li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang\n",
    "FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation\n",
    "Zijian Feng, Hanzhang Zhou, Kezhi Mao, Zixiao Zhu\n",
    "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition\n",
    "Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang\n",
    "Conundrums in Cross-Prompt Automated Essay Scoring: Making Sense of the State of the Art\n",
    "Shengjie Li, Vincent Ng\n",
    "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution\n",
    "Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Cercas Curry, Gavin Abercrombie, Dirk Hovy\n",
    "Label Augmentation for Zero-Shot Hierarchical Text Classification\n",
    "Lorenzo Paletto, Valerio Basile, Roberto Esposito\n",
    "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch\n",
    "Yiqun Zhang, Fanheng Kong, Peidong Wang, Shuang Sun, SWangLing, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song\n",
    "EIT: Enhanced Interactive Transformer\n",
    "Tong Zheng, Bei Li, Huiwen Bao, Tong Xiao, JingBo Zhu\n",
    "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\n",
    "Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr\n",
    "EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models\n",
    "Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, Preslav Nakov\n",
    "Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition\n",
    "Huiming Wang, Liying Cheng, Wenxuan Zhang, De Wen Soh, Lidong Bing\n",
    "Text Embedding Inversion Security for Multilingual Language Models\n",
    "Yiyi Chen, Heather Lent, Johannes Bjerva\n",
    "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment\n",
    "Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou\n",
    "Calibrating Large Language Models Using Their Generations Only\n",
    "Dennis Thomas Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, Seong Joon Oh\n",
    "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator\n",
    "Chuyi Kong, Yaxin FAN, Xiang Wan, Feng Jiang, Benyou Wang\n",
    "Synthesizing Text-to-SQL Data from Weak and Strong LLMs\n",
    "Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou\n",
    "Iterative Forward Tuning Boosts In-Context Learning in Language Models\n",
    "Jiaxi Yang, Binyuan Hui, Min Yang, Bailin Wang, Bowen Li, Binhua Li, Fei Huang, Yongbin Li\n",
    "STRUCTSUM Generation for Faster Text Comprehension\n",
    "Parag Jain, Andreea Marzoca, Francesco Piccinno\n",
    "Analysing The Impact of Sequence Composition on Language Model Pre-Training\n",
    "Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłoś, Yuxiang Wu, Pasquale Minervini\n",
    "NACL: A General and Effective KV Cache Eviction Framework for LLM at Inference Time\n",
    "Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu\n",
    "SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network\n",
    "Kexin Wang, Jiahong Zhang, Yong Ren, Man Yao, Di Shang, Bo XU, Guoqi Li\n",
    "Context-aware Difference Distilling for Multi-change Captioning\n",
    "Yunbin Tu, Liang Li, Li Su, Zheng-Jun Zha, Chenggang Yan, Qingming Huang\n",
    "Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion\n",
    "Wei Cheng, Yuhan Wu, Wei Hu\n",
    "Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation\n",
    "Haohao Luo, Yang Deng, Ying Shen, See-Kiong Ng, Tat-Seng Chua\n",
    "LLMEmbed: Rethinking Lightweight LLM’s Genuine Function in Text Classification\n",
    "ChunLiu, Hongguang Zhang, Kainan Zhao, Xinghai Ju, Lin Yang\n",
    "LEMON: Reviving Stronger and Smaller LMs from Larger LMs with Linear Parameter Fusion\n",
    "Yilong Chen, Junyuan Shang, Zhenyu Zhang, Shiyao Cui, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu\n",
    "Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation\n",
    "Tengfei Yu, Xuebo Liu, Liang Ding, Kehai Chen, Dacheng Tao, Min Zhang\n",
    "To be Continuous, or to be Discrete, Those are Bits of Questions\n",
    "Yiran Wang, Masao Utiyama\n",
    "Moûsai: Efficient Text-to-Music Diffusion Models\n",
    "Flavio Schneider, Ojasv Kamal, Zhijing Jin, Bernhard Schölkopf\n",
    "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering\n",
    "Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang\n",
    "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention\n",
    "Prince Jha, Raghav Jain, Konika Mandal, Aman Chadha, Sriparna Saha, Pushpak Bhattacharyya\n",
    "Efficient OCR for Building a Diverse Digital History\n",
    "Jacob Carlson, Tom Bryan, Melissa Dell\n",
    "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space\n",
    "Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu\n",
    "ANAH: Analytical Annotation of Hallucinations in Large Language Models\n",
    "Ziwei Ji, Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen\n",
    "Aligning Large Language Models for Controllable Recommendations\n",
    "Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, Xing Xie\n",
    "Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods\n",
    "Haeun Yu, Pepa Atanasova, Isabelle Augenstein\n",
    "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement\n",
    "Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang\n",
    "Full Parameter Fine-tuning for Large Language Models with Limited Resources\n",
    "Kai Lv, Yuqing Yang, Tengxiao Liu, Qipeng Guo, Xipeng Qiu\n",
    "M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n",
    "Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che\n",
    "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models\n",
    "Longze Chen, Ziqiang Liu, Wanwei He, Yinhe Zheng, Hao Sun, Yunshui Li, Run Luo, Min Yang\n",
    "Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation\n",
    "Keqi Deng, Phil Woodland\n",
    "Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL\n",
    "Yunseon Choi, Sangmin Bae, Seonghyun Ban, Minchan Jeong, Chuheng Zhang, Lei Song, Li Zhao, Jiang Bian, Kee-Eung Kim\n",
    "A Modular Approach for Multimodal Summarization of TV Shows\n",
    "Louis Mahon, Mirella Lapata\n",
    "Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities\n",
    "Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency\n",
    "BizBench: A Quantitative Reasoning Benchmark for Business and Finance\n",
    "Michael Krumdick, Rik Koncel-Kedziorski, Viet Dac Lai, Varshini Reddy, Charles Lovering, Chris Tanner\n",
    "Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization\n",
    "Takumi Takada, Yuma Suzuki, Hiroki Takushima, Hayato Tanoue, Haruki Sato, Aiswariya Manoj Kumar, Hiroki Nishihara, Takayuki Hori, Kazuya Ueki\n",
    "Deciphering Hate: Identifying Hateful Memes and Their Targets\n",
    "Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah Masud Preum\n",
    "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings\n",
    "Yichen Jiang, Xiang Zhou, Mohit Bansal\n",
    "Label-Efficient Model Selection for Text Generation\n",
    "Shir Ashury Tahan, Ariel Gera, Benjamin Sznajder, Leshem Choshen, Liat Ein-Dor, Eyal Shnarch\n",
    "Machine Unlearning of Pre-trained Large Language Models\n",
    "Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue\n",
    "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\n",
    "Francesco Ortu, Zhijing Jin, Diego Doimo, Mrinmaya Sachan, Alberto Cazzaniga, Bernhard Schölkopf\n",
    "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence\n",
    "Sebastian Antony Joseph, Lily Chen, Jan Trienes, Hannah Louisa Göke, Monika Coers, Wei Xu, Byron C Wallace, Junyi Jessy Li\n",
    "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction\n",
    "Yinhao Bai, Yalan Xie, Xiaoyi Liu, Yuhua Zhao, Zhixin Han, Mengting Hu, Hang Gao, Renhong Cheng\n",
    "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack\n",
    "Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong\n",
    "Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn’t\n",
    "Chihiro Taguchi, David Chiang\n",
    "Speech language models lack important brain-relevant semantics\n",
    "SUBBA REDDY OOTA, Emin Çelik, Fatma Deniz, Mariya Toneva\n",
    "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding\n",
    "Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n",
    "Bypassing LLM Watermarks with Color-Aware Substitutions\n",
    "Qilong Wu, Varun Chandrasekaran\n",
    "Parallel Structures in Pre-training Data Yield In-Context Learning\n",
    "Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He\n",
    "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models\n",
    "Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He\n",
    "Towards Privacy-Aware Sign Language Translation at Scale\n",
    "Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgoz, Jean Maillard\n",
    "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards\n",
    "Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang\n",
    "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters\n",
    "Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Shirong Ma, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen\n",
    "Steering Llama 2 via Contrastive Activation Addition\n",
    "Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan J Hubinger, Alexander Matt Turner\n",
    "RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\n",
    "Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger\n",
    "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\n",
    "Zekun Li, Zhiyu Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, Paul A. Crook\n",
    "Faithful Chart Summarization with ChaTS-Pi\n",
    "Syrine Krichene, Francesco Piccinno, Fangyu Liu, Julian Martin Eisenschlos\n",
    "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation\n",
    "Cheng Niu, Xingguang Wang, Xuxin Cheng, Juntong Song, Tong Zhang\n",
    "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking\n",
    "Ting-Chih Chen, Chia-Wei Tang, Chris Thomas\n",
    "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction\n",
    "Zixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su, Yucan Guo, Yantao Liu, lixiang, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan Yang, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng\n",
    "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis\n",
    "Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, Xuhong Zhang\n",
    "EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities\n",
    "Nian Li, Chen Gao, Mingyu Li, Yong Li, Qingmin Liao\n",
    "On the Multi-turn Instruction Following for Conversational Web Agents\n",
    "Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, Tat-Seng Chua\n",
    "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents\n",
    "Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Liujianfeng, Ang Li, Jian Luan, Bin Wang, Rui Yan, Shuo Shang\n",
    "MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China\n",
    "Chen Zhang, Mingxu Tao, Quzhe Huang, Jiuheng Lin, Zhibin Chen, Yansong Feng\n",
    "Decoder-only Streaming Transformer for Simultaneous Translation\n",
    "Shoutao Guo, Shaolei Zhang, Yang Feng\n",
    "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization\n",
    "Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang\n",
    "I am a Strange Dataset: Metalinguistic Tests for Language Models\n",
    "Tristan Thrush, Jared Moore, Miguel Monares, Christopher Potts, Douwe Kiela\n",
    "SafetyBench: Evaluating the Safety of Large Language Models\n",
    "Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang\n",
    "Deciphering Oracle Bone Language with Diffusion Models\n",
    "Haisu Guan, Huanxin Yang, Xinyu Wang, Shengwei Han, Yongge Liu, Lianwen Jin, Xiang Bai, Yuliang Liu\n",
    "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space\n",
    "Shaolei Zhang, Tian Yu, Yang Feng\n",
    "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training\n",
    "Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Jianan Zhao, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang\n",
    "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning\n",
    "Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min zhang, Yang Feng\n",
    "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models\n",
    "Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu\n",
    "Why Don’t Prompt-Based Fairness Metrics Correlate?\n",
    "Abdelrahman Zayed, Goncalo Mordido, Ioana Baldini, Sarath Chandar\n",
    "NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data\n",
    "Manuel Tonneau, Pedro Vitor Quinta de Castro, Karim Lasri, Ibrahim Sambo Farouq, Lakshmi Subramanian, Victor Orozco-Olvera, Samuel Fraiberger\n",
    "M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset\n",
    "Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, Yanfeng Wang\n",
    "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination\n",
    "Nakyeong Yang, Taegwan Kang, Stanley Jungkyu Choi, Honglak Lee, Kyomin Jung\n",
    "Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning\n",
    "Yufeng Zhang, Jianxing Yu, Yanghui Rao, Libin Zheng, Qinliang Su, Huaijie Zhu, Jian Yin\n",
    "Revisiting Demonstration Selection Strategies in In-Context Learning\n",
    "Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao\n",
    "Multimodal Table Understanding\n",
    "Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, Weiping Wang\n",
    "Ex\\textsuperscript{3}: Automatic Novel Writing by Extracting, Excelsior and Expanding\n",
    "Huang Lei, Jiaming Guo, Guanhua He, Xishan Zhang, Rui Zhang, Shaohui Peng, Shaoli Liu, Tianshi Chen\n",
    "Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning\n",
    "Mayur Patidar, Riya Sawhney, Avinash Kumar Singh, Biswajit Chatterjee, Mausam ., Indrajit Bhattacharya\n",
    "WatME: Towards Lossless Watermarking Through Lexical Redundancy\n",
    "Liang CHEN, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao, Kam-Fai Wong\n",
    "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation\n",
    "Yang Zhang, Keqin Bao, Ming Yan, Wenjie Wang, Fuli Feng, Xiangnan He\n",
    "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception\n",
    "Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yanfeng Wang, Yu Wang\n",
    "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning\n",
    "Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao\n",
    "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation\n",
    "Yi Liu, Xiangyu Liu, Xiangrong Zhu, Wei Hu\n",
    "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models\n",
    "Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Yuxin Jiang, Lifeng Shang, Qun Liu, Kam-Fai Wong\n",
    "Reward-based Input Construction for Cross-document Relation Extraction\n",
    "Byeonghu Na, Suhyeon Jo, Yeongmin Kim, Il-chul Moon\n",
    "Hyperspherical Multi-Prototype with Optimal Transport for Event Argument Extraction\n",
    "Guangjun Zhang, Hu zhang, YuJie Wang, Ru Li, Hongye Tan, Jiye Liang\n",
    "Understanding Retrieval Robustness for Retrieval-augmented Image Captioning\n",
    "Wenyan Li, Jiaang Li, Rita Ramos, Raphael Tang, Desmond Elliott\n",
    "Semi-Supervised Spoken Language Glossification\n",
    "Huijie Yao, Wengang Zhou, Hao Zhou, Houqiang Li\n",
    "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents\n",
    "Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, Zhiyong Wu\n",
    "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers\n",
    "Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein\n",
    "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods\n",
    "Yu Sun, keyuchen, Shujie Wang, Peiji Li, Qipeng Guo, Hang Yan, Xipeng Qiu, Xuanjing Huang, Dahua Lin\n",
    "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning\n",
    "Philipp Mondorf, Barbara Plank\n",
    "Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback\n",
    "Maria Emilia Agis Lerner, Florian E. Dorner, Elliott Ash, Naman Goel\n",
    "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\n",
    "Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui\n",
    "Large Language Models are not Fair Evaluators\n",
    "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui\n",
    "Improving Large Language Models in Event Relation Logical Prediction\n",
    "Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, Dongsheng Li\n",
    "Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline\n",
    "Dingyi Yang, Chunru Zhan, Ziheng Wang, Biao Wang, Tiezheng Ge, Bo Zheng, Qin Jin\n",
    "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation\n",
    "Wenting Chen, Linlin Shen, Jingyang Lin, Jiebo Luo, Xiang Li, Yixuan Yuan\n",
    "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step\n",
    "Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, Feng Zhao\n",
    "Are LLM-based Evaluators Confusing NLG Quality Criteria?\n",
    "Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, TENG XU, Xiaojun Wan\n",
    "Synergistic Interplay between Search and Large Language Models for Information Retrieval\n",
    "Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang\n",
    "Linear Transformers with Learnable Kernel Functions are Better In-Context Models\n",
    "Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov\n",
    "Temperature-scaling surprisal estimates improve fit to human reading times – but does it do so for the “right reasons”?\n",
    "Tong Liu, Iza Škrjanec, Vera Demberg\n",
    "Beyond Recognising Entailment: Formalising Natural Language Inference from an Argumentative Perspective\n",
    "Ameer Saadat-Yazdi, Nadin Kökciyan\n",
    "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization\n",
    "Jaavid Aktar Husain J, Raj Dabre, Aswanth Kumar M, Jay Gala, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan\n",
    "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n",
    "Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, Xipeng Qiu\n",
    "CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models\n",
    "Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, Guang Chen\n",
    "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation\n",
    "Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Xiaoming Simon Wang, Jiulong Shan, Meng Cao, Lijie Wen\n",
    "Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines\n",
    "Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, Yonatan Belinkov\n",
    "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models\n",
    "Yuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Ruihua Song, Xin Zhao, Fuzheng Zhang, Di ZHANG, Kun Gai\n",
    "Robust Singing Voice Transcription Serves Synthesis\n",
    "Ruiqi Li, Yu Zhang, Yongqi Wang, Zhiqing Hong, Rongjie Huang, Zhou Zhao\n",
    "VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model\n",
    "Tianyu Chen, Lin Li, ZhuLiuchuan, Zongyang Li, Xueqing Liu, Guangtai Liang, Qianxiang Wang, Tao Xie\n",
    "Self-Modifying State Modeling for Simultaneous Machine Translation\n",
    "Donglei Yu, Xiaomian Kang, Yuchen Liu, Yu Zhou, Chengqing Zong\n",
    "MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation\n",
    "Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong\n",
    "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents\n",
    "Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian\n",
    "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy\n",
    "Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan\n",
    "LePaRD: A Large-Scale Dataset of Judicial Citations to Precedent\n",
    "Robert Mahari, Dominik Stammbach, Elliott Ash, Alex Pentland\n",
    "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering\n",
    "Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro, Zaiqiao Meng\n",
    "MERA: A Comprehensive LLM Evaluation in Russian\n",
    "Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid S Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Anastasia Minaeva, Denis Dimitrov, Alexander Panchenko, Sergey Markov\n",
    "SC2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer\n",
    "Jie Zhao, Ziyu Guan, Cai Xu, Wei Zhao, Yue Jiang\n",
    "Causal Estimation of Memorisation Profiles\n",
    "Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel\n",
    "CHECKWHY: Causal Fact Verification via Argument Structure\n",
    "Jiasheng Si, Yibo Zhao, Yingjie Zhu, Haiyang Zhu, Wenpeng Lu, Deyu Zhou\n",
    "Dodo: Dynamic Contextual Compression for Decoder-only LMs\n",
    "Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van Durme\n",
    "POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation\n",
    "Shilong Pan, Zhiliang Tian, Liang Ding, Haoqi Zheng, Zhen Huang, Zhihua Wen, Dongsheng Li\n",
    "NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism\n",
    "Miao Li, Ming-Bin Chen, Bo Tang, ShengbinHou, Pengyu Wang, Haiying Deng, Zhiyu li, Feiyu Xiong, Keming Mao, Cheng Peng, Yi Luo\n",
    "MAPO: Advancing Multilingual Reasoning through Multilingual-Alignment-as-Preference Optimization\n",
    "Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, Jiajun Chen\n",
    "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training\n",
    "Feiteng Fang, yuelin bai, Shiwen Ni, Min Yang, Xiaojun Chen, Ruifeng Xu\n",
    "Predicting Text Preference Via Structured Comparative Reasoning\n",
    "Jing Nathan Yan, Tianqi Liu, Justin T Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Charumathi Lakshmanan, Yair Kurzion, Alexander M Rush, Jialu Liu, Michael Bendersky\n",
    "CoELM: Construction-Enhanced Language Modeling\n",
    "Lvxiaowei Xu, Zhilin Gong, Jianhua Dai, Tianxiang Wang, Ming Cai, Jiawei Peng\n",
    "Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model\n",
    "Christian Tomani, David Vilar, Markus Freitag, Colin Cherry, Subhajit Naskar, Mara Finkelstein, Xavier Garcia, Daniel Cremers\n",
    "Uni-Dubbing: Zero-Shot Speech Synthesis from Visual Articulation\n",
    "Songju Lei, Xize Cheng, Mengjiao Lyu, Jianqiao Hu, Jintao Tan, Runlin Liu, Lingyu Xiong, Tao Jin, Xiandong Li, Zhou Zhao\n",
    "On the Impact of Calibration Data in Post-training Quantization and Pruning\n",
    "Miles Williams, Nikolaos Aletras\n",
    "SymKGQA: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution\n",
    "Prerna Agarwal, Nishant Kumar, Srikanta J. Bedathur\n",
    "Meta-Task Prompting Elicits Embeddings from Large Language Models\n",
    "Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, Andrew Yates\n",
    "A Sentiment Consolidation Framework for Meta-Review Generation\n",
    "Miao Li, Jey Han Lau, Eduard Hovy\n",
    "Revisiting Structured Sentiment Analysis as Latent Dependency Graph Parsing\n",
    "Chengjie Zhou, Bobo Li, Hao Fei, Fei Li, Chong Teng, Donghong Ji\n",
    "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification\n",
    "Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe\n",
    "Do Large Language Models Latently Perform Multi-Hop Reasoning?\n",
    "Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel\n",
    "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning\n",
    "Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou\n",
    "Harnessing Toulmin’s theory for zero-shot argument explication\n",
    "Ankita Gupta, Ethan Zuckerman, Brendan O’Connor\n",
    "BinaryAlign: Word Alignment as Binary Sequence Labeling\n",
    "Gaetan Lopez Latouche, Marc-André Carbonneau, Benjamin Swanson\n",
    "Quantifying the Persona Effect in LLM Simulations\n",
    "Tiancheng Hu, Nigel Collier\n",
    "On Efficient and Statistical Quality Estimation for Data Annotation\n",
    "Jan-Christoph Klie, Juan Haladjian, Marc Kirchner, Rahul Nair\n",
    "EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection\n",
    "Chenye Zhao, Cornelia Caragea\n",
    "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?\n",
    "Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger\n",
    "Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments\n",
    "Zhenrui Yue, Huimin Zeng, Lanyu Shang, Yifan Liu, Yang Zhang, Dong Wang\n",
    "SyllabusQA: A Course Logistics Question Answering Dataset\n",
    "Nigel Fernandez, Alexander Scarlatos, Andrew Lan\n",
    "American Sign Language Handshapes Reflect Pressures for Communicative Efficiency\n",
    "Kayo Yin, Terry Regier, Dan Klein\n",
    "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models\n",
    "Yilin Wen, Zifeng Wang, Jimeng Sun\n",
    "AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts\n",
    "Daniel Braun, Florian Matthes\n",
    "Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks\n",
    "Charlotte Siska, Katerina Marazopoulou, Melissa Ailem, James Bono\n",
    "Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning\n",
    "Eric Pasewark, Kyle Montgomery, Kefei Duan, Dawn Song, Chenguang Wang\n",
    "Bridging the Preference Gap between Retrievers and LLMs\n",
    "Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky\n",
    "Large Language Models Can Learn Temporal Reasoning\n",
    "Siheng Xiong, Ali Payani, Ramana Rao Kompella, Faramarz Fekri\n",
    "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\n",
    "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo\n",
    "Learning Relational Decomposition of Queries for Question Answering from Tables\n",
    "Raphaël Mouravieff, Benjamin Piwowarski, sylvain lamprier\n",
    "Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People\n",
    "Dun-Ming Huang, Pol Van Rijn, Ilia Sucholutsky, Raja Marjieh, Nori Jacoby\n",
    "Pareto Optimal Learning for Estimating Large Language Model Errors\n",
    "Theodore Zhao, Mu Wei, J. Samuel Preston, Hoifung Poon\n",
    "Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models\n",
    "Victor Agostinelli III, Max Wild, Matthew Raffel, Kazi Ahmed Asif Fuad, Lizhong Chen\n",
    "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM\n",
    "Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen\n",
    "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models\n",
    "Guanming Xiong, Junwei Bao, Wen Zhao\n",
    "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error\n",
    "Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su\n",
    "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts\n",
    "Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu\n",
    "Aligning Large Language Models with Human Preferences through Representation Engineering\n",
    "Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Zhu JianHao, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang\n",
    "CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models\n",
    "Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu\n",
    "ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation\n",
    "Chen Huang, Yiping Jin, Ilija Ilievski, Wenqiang Lei, Jiancheng Lv\n",
    "PolCLIP: A Unified Image-Text Word Sense Disambiguation Model via Generating Multimodal Complementary Representations\n",
    "Qihao Yang, Yong Li, Xuelin Wang, Fu Lee Wang, Tianyong Hao\n",
    "Prompted Aspect Key Point Analysis for Quantitative Review Summarization\n",
    "An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Erik Cambria\n",
    "Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment\n",
    "Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia\n",
    "CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models\n",
    "Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua\n",
    "Multimodal Reasoning with Multimodal Knowledge Graph\n",
    "Junlin Lee, Yequan Wang, Jing Li, Min Zhang\n",
    "Confidence is not Timeless: Modeling Temporal Validity for Rule-based Temporal Knowledge Graph Forecasting\n",
    "Rikui Huang, Wei Wei, Xiaoye Qu, Shengzhe Zhang, Dangyang Chen, Yu Cheng\n",
    "CARE: A Clue-guided Assistant for CSRs to Read User Manuals\n",
    "Weihong Du, Jia Liu, zujie wen, Dingnan Jin, Hongru Liang, Wenqiang Lei\n",
    "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes\n",
    "Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che\n",
    "PAGED: A Benchmark for Procedural Graphs Extraction from Documents\n",
    "Weihong Du, Wenrui Liao, Hongru Liang, Wenqiang Lei\n",
    "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors\n",
    "Ying Zhou, Ben He, Le Sun\n",
    "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models\n",
    "Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun SHUM, Randy Zhong, Juntong Song, Tong Zhang\n",
    "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models\n",
    "Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen\n",
    "Revisiting Knowledge Distillation for Autoregressive Language Models\n",
    "Qihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng Tao\n",
    "OLMo: Accelerating the Science of Language Models\n",
    "Dirk Groeneveld, Iz Beltagy, Evan Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William H. Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi\n",
    "Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation\n",
    "Yunlong Liang, Fandong Meng, Jiaan Wang, Jinan Xu, Yufeng Chen, Jie Zhou\n",
    "Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners\n",
    "Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Jinchuan Tian, Zhenhui Ye, Luping Liu, Zehan Wang, Ziyue Jiang, Xuankai Chang, Jiatong Shi, CHAO WENG, Zhou Zhao, Dong Yu\n",
    "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages\n",
    "Shih-Cheng Huang, Pin-Zu Li, YU-CHI HSU, Kuang-Ming Chen, Yu Tung Lin, Shih-Kai Hsiao, Richard Tzong-Han Tsai, Hung-yi Lee\n",
    "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!\n",
    "Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao\n",
    "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails\n",
    "Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, Atul Prakash\n",
    "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLMs-Powered Assistance\n",
    "Bo Yuan, Yulin Chen, Yin Zhang, Wei Jiang\n",
    "CLOMO: Counterfactual Logical Modification with Large Language Models\n",
    "Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng YANG, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song\n",
    "Exploring Hybrid Question Answering via Program-based Prompting\n",
    "Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu\n",
    "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages\n",
    "Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, Partha Talukdar\n",
    "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion\n",
    "Rui Ying, Mengting Hu, Jianfeng Wu, Yalan Xie, Xiaoyi Liu, Zhunheng Wang, Ming Jiang, Hang Gao, Linlin Zhang, Renhong Cheng\n",
    "Uncertainty Aware Learning for Language Model Alignment\n",
    "Yikun Wang, Rui Zheng, Liang Ding, Qi Zhang, Dahua Lin, Dacheng Tao\n",
    "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models\n",
    "Ying-Chun Lin, Jennifer Neville, Jack W Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, saurabh tiwary, Brent Hecht, Jaime Teevan\n",
    "Fundamental Capabilities of Large Language Models and their Applications in Domain Scenarios: A Survey\n",
    "Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, 林一冠, Bin Xu, Ren bowen, Chong Feng, Yang Gao, Heyan Huang\n",
    "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages\n",
    "Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad B, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, Mitesh M Khapra\n",
    "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said\n",
    "Yejin Bang, Delong Chen, Nayeon Lee, Pascale Fung\n",
    "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use\n",
    "Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, Rui Yan\n",
    "Layer-Condensed KV Cache for Efficient Inference of Large Language Models\n",
    "Haoyi Wu, Kewei Tu\n",
    "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models\n",
    "Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu\n",
    "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages\n",
    "Yuanchi Zhang, Yile Wang, Zijun Liu, Shuo Wang, Xiaolong Wang, Peng Li, Maosong Sun, Yang Liu\n",
    "Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations\n",
    "Jiaxing Sun, weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He\n",
    "Browse and Concentrate: Comprehending Multimodal Content via Prior-LLM Context Fusion\n",
    "Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu\n",
    "Model Composition for Multimodal Large Language Models\n",
    "Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu\n",
    "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding\n",
    "Jun Zhang, Jue WANG, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra\n",
    "Soul-Mix: Enhancing Multimodal Machine Translation with Manifold Mixup\n",
    "Xuxin Cheng, Ziyu Yao, Yifei Xin, Hao An, Hongxiang Li, Yaowei Li, Yuexian Zou\n",
    "Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models\n",
    "Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang\n",
    "MIST: Mutual Information Maximization for Short Text Clustering\n",
    "Krissanee Kamthawee, Can Udomcharoenchaikit, Sarana Nutanong\n",
    "Self-chats from Large Language Models Make Small Emotional Support Chatbot Better\n",
    "Zhonghua Zheng, Lizi Liao, Yang Deng, Libo Qin, Liqiang Nie\n",
    "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment\n",
    "Janghwan Lee, Seongmin Park, Sukjin Hong, Minsoo Kim, Du-Seong Chang, Jungwook Choi\n",
    "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs\n",
    "Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut\n",
    "An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing\n",
    "Ziwei Chai, Guoyin Wang, Jing Su, Tianjie Zhang, Xuanwen Huang, Xuwu Wang, Jingjing Xu, Jianbo Yuan, Hongxia Yang, Fei Wu, Yang Yang\n",
    "Learning to Plan and Generate Text with Citations\n",
    "Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, Mirella Lapata\n",
    "Exploring Precision and Recall to assess the quality and diversity of LLMs\n",
    "Florian Le Bronnec, Alexandre Verine, benjamin negrevergne, Yann Chevaleyre, Alexandre Allauzen\n",
    "Aligning Large Language Models by On-Policy Self-Judgment\n",
    "Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu\n",
    "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning\n",
    "Abhinav Joshi, Shounak Paul, Akshat Sharma, Pawan Goyal, Saptarshi Ghosh, Ashutosh Modi\n",
    "JumpCoder: Go Beyond Autoregressive Coder via Online Modification\n",
    "Mouxiang Chen, Hao Tian, Zhongxin Liu, Xiaoxue Ren, Jianling Sun\n",
    "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning\n",
    "Shivalika Singh, Freddie Vargus, Daniel D’souza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura O’Mahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Ivantha Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, Sara Hooker\n",
    "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks\n",
    "Anwoy Chatterjee, Eshaan Tanwar, Subhabrata Dutta, Tanmoy Chakraborty\n",
    "Split and Rephrase with Large Language Models\n",
    "Antonio David Ponce Martínez, Thierry Etchegoyhen, Jesus Javier Calleja Perez, Harritxu Gete\n",
    "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition\n",
    "Lu Ye, Ze Tao, Yong Huang, Yang Li\n",
    "AlignBench: Benchmarking Chinese Alignment of Large Language Models\n",
    "Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Andrew Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Xiaotao Gu, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang\n",
    "SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\n",
    "Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che\n",
    "DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution\n",
    "Yulong Mao, Kaiyu Huang, Changhao Guan, Ganglin Bao, Fengran Mo, Jinan Xu\n",
    "Cross-Lingual Knowledge Editing in Large Language Models\n",
    "Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu, Fandong Meng\n",
    "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model\n",
    "Ahmet Üstün, Viraat Aryabumi, Zheng Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker\n",
    "Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques\n",
    "Anar Yeginbergen, Maite Oronoz, Rodrigo Agerri\n",
    "Learning Task Decomposition to Assist Humans in Competitive Programming\n",
    "Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, Minlie Huang\n",
    "An Entropy-based Text Watermarking Detection Method\n",
    "Yijian LU, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King\n",
    "Enhancing Explainable Rating Prediction through Annotated Macro Concepts\n",
    "Huachi Zhou, Shuang Zhou, Hao Chen, Ninghao Liu, Fan Yang, Xiao Huang\n",
    "How to Engage your Readers? Generating Guiding Questions to Promote Active Reading\n",
    "Peng Cui, Vilém Zouhar, Xiaoyu Zhang, Mrinmaya Sachan\n",
    "Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective\n",
    "Zihao Yue, Liang Zhang, Qin Jin\n",
    "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation\n",
    "Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li\n",
    "More frequent verbs are associated with more diverse valency frames: Efficient principles at the lexicon-grammar interface\n",
    "Siyu Tao, Lucia Donatelli, Michael Hahn\n",
    "BatchEval: Towards Human-like Text Evaluation\n",
    "Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Yao Hu, Kan Li\n",
    "Quantifying Generalizations: Exploring the Divide Between Human and LLMs’ Sensitivity to Quantification\n",
    "Claudia Collacciani, Giulia Rambelli, Marianna Bolognesi\n",
    "Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds\n",
    "Giulia Rambelli, Emmanuele Chersoni, Claudia Collacciani, Marianna Bolognesi\n",
    "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation\n",
    "Quan Tu, Shilong Fan, Zihang Tian, Tianhao Shen, Shuo Shang, Xin Gao, Rui Yan\n",
    "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond\n",
    "Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, Tat-Seng Chua\n",
    "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction\n",
    "Yice Zhang, Jie Zeng, Weiming Hu, Ziyi Wang, Shiwei Chen, Ruifeng Xu\n",
    "ToMBench: Benchmarking Theory of Mind in Large Language Models\n",
    "Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, Minlie Huang\n",
    "Learning to Generate Answers with Citations via Factual Consistency Models\n",
    "Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis\n",
    "Improving Text Embeddings with Large Language Models\n",
    "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei\n",
    "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning\n",
    "Tianduo Wang, Shichen Li, Wei Lu\n",
    "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset\n",
    "Haoyu Wang, Shuo Wang, Yukun Yan, Xujia Wang, Zhiyu Yang, Yuzhuang Xu, Zhenghao Liu, Liner Yang, Ning Ding, Xu Han, Zhiyuan Liu, Maosong Sun\n",
    "Document-level Claim Extraction and Decontextualisation for Fact-Checking\n",
    "Zhenyun Deng, Michael Sejr Schlichtkrull, Andreas Vlachos\n",
    "PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning\n",
    "Xiaoqi Qiu, Yongjie Wang, Xu Guo, Zhiwei Zeng, Yu Yue, Yuhong Feng, Chunyan Miao\n",
    "LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction\n",
    "Hanzhang Zhou, Junlang Qian, Zijian Feng, Hui Lu, Zixiao Zhu, Kezhi Mao\n",
    "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models\n",
    "Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin\n",
    "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind\n",
    "Jincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand Sabour, Helen M. Meng, Minlie Huang\n",
    "mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models\n",
    "Huiyuan Lai, Malvina Nissim\n",
    "GunStance: Stance Detection for Gun Control and Gun Regulation\n",
    "Nikesh Gyawali, Iustin Sirbu, Tiberiu Sosea, Sarthak Khanal, Doina Caragea, Traian Rebedea, Cornelia Caragea\n",
    "Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation\n",
    "Zdeněk Kasner, Ondrej Dusek\n",
    "Don’t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection\n",
    "Min Zhang, Jianfeng He, Taoran Ji, Chang-Tien Lu\n",
    "Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation\n",
    "Giorgos Vernikos, Andrei Popescu-Belis\n",
    "Generating and Evaluating Plausible Explanations for Knowledge Graph Completion\n",
    "Antonio Di Mauro, Zhao Xu, Wiem Ben Rim, Timo Sztyler, Carolin Lawrence\n",
    "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation\n",
    "Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera\n",
    "MultiPICo: Multilingual Perspectivist Irony Corpus\n",
    "Silvia Casola, Simona Frenda, Soda Marem Lo, Erhan Sezerer, Antonio Uva, Valerio Basile, Cristina Bosco, Alessandro Pedrani, Chiara Rubagotti, Viviana Patti, Davide Bernardi\n",
    "LANDeRMT: Dectecting and Routing Language-Aware Neurons for Selectively Finetuning LLMs to Machine Translation\n",
    "shaolin Zhu, Leiyu Pan, Bo Li, Deyi Xiong\n",
    "A Joint Coreference-Aware Approach to Document-Level Target Sentiment Analysis\n",
    "Hongjie Cai, Heqing Ma, Jianfei Yu, Rui Xia\n",
    "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models\n",
    "Qingxing Cao, Junhao Cheng, Xiaodan Liang, Liang Lin\n",
    "AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints\n",
    "Yu-Zhe Shi, Haofei Hou, Zhangqian Bi, Fanxu Meng, Xiang Wei, Lecheng Ruan, Qining Wang\n",
    "Multipath parsing in the brain\n",
    "Berta Franzluebbers, Donald Dunagan, Miloš Stanojević, Jan Buys, John T. Hale\n",
    "Search-Adaptor: Embedding Customization for Information Retrieval\n",
    "Jinsung Yoon, Yanfei Chen, Sercan O Arik, Tomas Pfister\n",
    "Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs\n",
    "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker\n",
    "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation\n",
    "Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, Wenhu Chen\n",
    "Tree Transformer’s Disambiguation Ability of Prepositional Phrase Attachment and Garden Path Effects\n",
    "Lingling Zhou, Suzan Verberne, Gijs Wijnholds\n",
    "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs\n",
    "Elan Sopher Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan\n",
    "Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing\n",
    "Freda Shi, Kevin Gimpel, Karen Livescu\n",
    "ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation\n",
    "Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi Dave, Rida Qadri, Chandan K. Reddy, Sunipa Dev\n",
    "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents\n",
    "Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian\n",
    "Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking\n",
    "Xiaokang Zhang, Zijun Yao, Jing Zhang, Kaifeng Yun, Jifan Yu, Juanzi Li, Jie Tang\n",
    "What Do Language Models Learn in Context? The Structured Task Hypothesis.\n",
    "Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell\n",
    "Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n",
    "Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, Bill Yuchen Lin\n",
    "Investigating Cultural Alignment of Large Language Models\n",
    "Badr AlKhamissi, Muhammad ElNokrashy, Mai Alkhamissi, Mona T. Diab\n",
    "More Victories, Less Cooperation: Assessing Cicero’s Diplomacy Play\n",
    "Wichayaporn Wongkamjan, Feng Gu, Yanze Wang, Ulf Hermjakob, Jonathan May, Brandon M. Stewart, Jonathan K. Kummerfeld, Denis Peskoff, Jordan Lee Boyd-Graber\n",
    "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild\n",
    "Puyuan Peng, Po-Yao Huang, Shang-Wen Li, Abdelrahman Mohamed, David Harwath\n",
    "RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors\n",
    "Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh magnus Ludan, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch\n",
    "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles\n",
    "Julia Kruk, Michela Marchini, Rijul Magu, Caleb Ziems, David Muchlinski, Diyi Yang\n",
    "On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning\n",
    "Franz Nowak, Anej Svete, Alexandra Butoi, Ryan Cotterell\n",
    "Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\n",
    "Sanjana Ramprasad, Elisa Ferracane, Zachary Chase Lipton\n",
    "MMToM-QA: Multimodal Theory of Mind Question Answering\n",
    "Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum, Tianmin Shu\n",
    "LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n",
    "Keivan Alizadeh, Seyed Iman Mirzadeh, Dmitry Belenko, S. Karen Khatamifard, Minsik Cho, Carlo C del Mundo, Mohammad Rastegari, Mehrdad Farajtabar\n",
    "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models\n",
    "Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Fahad Shahbaz Khan\n",
    "To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation\n",
    "Abdul Waheed, Karima Kadaoui, Muhammad Abdul-Mageed\n",
    "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents\n",
    "Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan\n",
    "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding\n",
    "Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu\n",
    "Unintended Impacts of LLM Alignment on Global Representation\n",
    "Michael J Ryan, William Barr Held, Diyi Yang\n",
    "Classist Tools: Social Class Correlates with Performance in NLP\n",
    "Amanda Cercas Curry, Giuseppe Attanasio, Zeerak Talat, Dirk Hovy\n",
    "ActionIE: Action Extraction from Scientific Literature with Programming Languages\n",
    "Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, Jiawei Han\n",
    "A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech\n",
    "Gaurav Verma, Rynaa Grover, Jiawei Zhou, Binny Mathew, Jordan Kraemer, Munmun De Choudhury, Srijan Kumar\n",
    "Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
    "Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, Jinsong Su\n",
    "COSMIC: Mutual Information for Task-Agnostic Summarization Evaluation\n",
    "Maxime DARRIN, Philippe Formont, Jackie CK Cheung, Pablo Piantanida\n",
    "ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer\n",
    "Arkadiy Saakyan, Smaranda Muresan\n",
    "EUROPA: A Legal Multilingual Keyphrase Generation Dataset\n",
    "Olivier Salaün, Frédéric Piedboeuf, Guillaume Le Berre, David Alfonso-Hermelo, Philippe Langlais\n",
    "GLIMPSE: Pragmatically Informative Multi-Document Summarization for Scholarly Reviews\n",
    "Maxime DARRIN, Ines Arous, Pablo Piantanida, Jackie CK Cheung\n",
    "MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy\n",
    "Davis Yoshida, Kartik Goyal, Kevin Gimpel\n",
    "Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks\n",
    "Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, Muhammad Abdul-Mageed\n",
    "Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks\n",
    "João Bordalo, Vasco Ramos, Rodrigo Valério, Diogo Glória-Silva, Yonatan Bitton, Michal Yarom, Idan Szpektor, Joao Magalhaes\n",
    "Cheetah: Natural Language Generation for 517 African Languages\n",
    "Ife Adebara, AbdelRahim A. Elmadany, Muhammad Abdul-Mageed\n",
    "TaPERA: Enhancing Faithfulness and Interpretability in Long-Form Table QA by Content Planning and Execution-based Reasoning\n",
    "Yilun Zhao, Lyuhao Chen, Arman Cohan, Chen Zhao\n",
    "KnowledgeFMath: A Knowledge-Intensive Math Reasoning Dataset in Finance Domains\n",
    "Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan\n",
    "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs\n",
    "Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Vernon Austel, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, Luis A. Lastras\n",
    "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks\n",
    "Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, Maosong Sun\n",
    "Harder Task Needs More Experts: Dynamic Routing in MoE Models\n",
    "Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng\n",
    "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception\n",
    "HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang\n",
    "SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents\n",
    "Ruiyi Wang, Haofei Yu, Wenxin Sharon Zhang, Zhengyang Qi, Maarten Sap, Yonatan Bisk, Graham Neubig, Hao Zhu\n",
    "${\\mathcal X}$FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts\n",
    "Yifeng Ding, Jiawei Liu, Yuxiang Wei, LINGMING ZHANG\n",
    "Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning\n",
    "Tuc Van Nguyen, Thai Le\n",
    "Learning to Decode Collaboratively with Multiple Language Models\n",
    "Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag\n",
    "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models\n",
    "Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun LIU\n",
    "Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?\n",
    "Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, Min zhang\n",
    "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
    "Pei Ke, Bosi Wen, Andrew Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang\n",
    "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments\n",
    "Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Lijie Wen\n",
    "Small But Funny: A Feedback-Driven Approach to Humor Distillation\n",
    "Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Vered Shwartz, Arash Einolghozati\n",
    "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models\n",
    "Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu\n",
    "From Sights to Insights: Towards Summarization of Multimodal Clinical Documents\n",
    "Akash Ghosh, Mohit Singh Tomar, Abhisek Tiwari, Sriparna Saha, JATIN AVINASH SALVE, Setu Sinha\n",
    "When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models\n",
    "Jiaxin Wang, Lingling Zhang, Wee Sun Lee, Yujie Zhong, Liwei Kang, Jun Liu\n",
    "Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation\n",
    "Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, Peter Brusilovsky\n",
    "Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in Arabic\n",
    "Yassine El Kheir, Hamdy Mubarak, Ahmed Ali, Shammur Absar Chowdhury\n",
    "Document-Level Machine Translation with Large-Scale Public Parallel Corpora\n",
    "Proyag Pal, Alexandra Birch, Kenneth Heafield\n",
    "Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!\n",
    "Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Edoardo Barba, Roberto Navigli\n",
    "NounAtlas: Filling the Gap in Nominal Semantic Role Labeling\n",
    "Roberto Navigli, Marco Lo Pinto, Pasquale Silvestri, Dennis Rotondi, Simone Ciciliano, Alessandro Scirè\n",
    "Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length\n",
    "Nur Lan, Emmanuel Chemla, Roni Katzir\n",
    "Context versus Prior Knowledge in Language Models\n",
    "Kevin Du, Vésteinn Snæbjarnarson, Niklas Stoehr, Jennifer C. White, Aaron Schein, Ryan Cotterell\n",
    "Word Matters: What Influences Domain Adaptation in Summarization?\n",
    "Yinghao Li, Siyu Miao, Heyan Huang, Yang Gao\n",
    "Visualization Recommendation with Prompt-based Reprogramming of Large Language Models\n",
    "Xinhang Li, Jingbo Zhou, Wei Chen, Derong Xu, Tong Xu, Enhong Chen\n",
    "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs\n",
    "Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, Prathosh AP\n",
    "Toward In-Context Teaching: Adapting Examples to Students’ Misconceptions\n",
    "Alexis Ross, Jacob Andreas\n",
    "Bridging Word-Pair and Token-Level Metaphor Detection with Explainable Domain Mining\n",
    "Yuan Tian, Ruike Zhang, Nan Xu, Wenji Mao\n",
    "Faithful Logical Reasoning via Symbolic Chain-of-Thought\n",
    "Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu\n",
    "S$^2$GSL: Incorporating Segment to Syntactic Enhanced Graph Structure Learning for Aspect-based Sentiment Analysis\n",
    "Bingfeng chen, qihan ouyang, yongqi luo, Boyan Xu, Ruichu Cai, Zhifeng Hao\n",
    "Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends\n",
    "Giuliano Martinelli, Edoardo Barba, Roberto Navigli\n",
    "ESCoT: Towards Interpretable Emotional Support Dialogue Systems\n",
    "Tenggan Zhang, Xinjie Zhang, Jinming Zhao, Li Zhou, Qin Jin\n",
    "PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering\n",
    "Fangzhi Xu, Qika Lin, Tianzhe Zhao, JiaweiHan, Jun Liu\n",
    "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection\n",
    "Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu\n",
    "Advancing Parameter Efficiency in Fine-tuning via Representation Editing\n",
    "Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Zhu JianHao, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang\n",
    "Context Consistency between Training and Inference in Simultaneous Machine Translation\n",
    "Meizhi Zhong, Lemao Liu, Kehai Chen, Mingming Yang, Min Zhang\n",
    "Using Natural Language Explanations to Improve Robustness of In-context Learning\n",
    "Xuanli He, Yuxiang Wu, Oana-Maria Camburu, Pasquale Minervini, Pontus Stenetorp\n",
    "The Earth is Flat because…: Investigating LLMs’ Belief towards Misinformation via Persuasive Conversation\n",
    "Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu\n",
    "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers\n",
    "Jiawen Xie, Pengyu Cheng, Xiao Liang, Yong Dai, nan du\n",
    "LooGLE: Can Long-Context Language Models Understand Long Contexts?\n",
    "Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang\n",
    "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models\n",
    "Hojae Han, Jaejin Kim, Jaeseok Yoo, Youngwon Lee, seung-won hwang\n",
    "Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation\n",
    "Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeonghun Yeo, Yong Man Ro\n",
    "Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels\n",
    "Zixia Jia, Junpeng Li, Shichuan Zhang, Anji Liu, Zilong Zheng\n",
    "MULFE: A Multi-Level Benchmark for Free Text Model Editing\n",
    "Chenhao Wang, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao\n",
    "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech\n",
    "Shengpeng Ji, Ziyue Jiang, Wang Hanting, Jialung Zuo, Zhou Zhao\n",
    "Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation\n",
    "Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, David Suter\n",
    "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position\n",
    "Kechi Zhang, Ge Li, Huangzhao Zhang, Zhi Jin\n",
    "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training\n",
    "Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, LiuYiBo, qianguosun, Yuxin Liang, Hao Wang, Enming Zhang, Jiaxing Zhang\n",
    "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges\n",
    "Kechi Zhang, Jia Li, Ge Li, xianjie Shi, Zhi Jin\n",
    "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator\n",
    "Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, Huan Sun\n",
    "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models\n",
    "Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral\n",
    "ECBD: Evidence-Centered Benchmark Design for NLP\n",
    "Yu Lu Liu, Su Lin Blodgett, Jackie CK Cheung, Vera Liao, Alexandra Olteanu, Ziang Xiao\n",
    "Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding\n",
    "Ruohao Guo, Wei Xu, Alan Ritter\n",
    "Reducing Privacy Risks in Online Self-Disclosures with Language Models\n",
    "Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu\n",
    "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models\n",
    "Zihao Lin, Mohammad Beigi, Hongxuan Li, Yufan Zhou, Yuxiang Zhang, Qifan Wang, Wenpeng Yin, Lifu Huang\n",
    "REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset\n",
    "Vaidehi Patil, Leonardo F. R. Ribeiro, Mengwen Liu, Mohit Bansal, Markus Dreyer\n",
    "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards\n",
    "Norah A. Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan AlRashed, Shaykhah Z. Alsubaie, Yousef Almushayqih, Faisal Abdulrahman Mirza, Nouf M. Alotaibi, Nora Al-Twairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan\n",
    "LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts\n",
    "Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, Chris Kedzie\n",
    "LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition\n",
    "Xiaomeng Zhu, Robert Frank\n",
    "Evaluating Very Long-Term Conversational Memory of LLM Agents\n",
    "Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang\n",
    "Prototypical Reward Network for Data-Efficient Model Alignment\n",
    "Jinghan Zhang, Xiting Wang, Yiqiao Jin, Changyu Chen, Xinhao Zhang, Kunpeng Liu\n",
    "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms\n",
    "Jonathan Zheng, Alan Ritter, Wei Xu\n",
    "Impacts of Misspelled Queries on Translation and Product Search\n",
    "Greg Hanneman, Natawut Monaikul, Taichi Nakatani\n",
    "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\n",
    "Tarek Naous, Michael J Ryan, Alan Ritter, Wei Xu\n",
    "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs\n",
    "Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin\n",
    "The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition\n",
    "Enshi Zhang, Rafael Trujillo, Christian Poellabauer\n",
    "Transparent and Scrutable Recommendations Using Natural Language User Profiles\n",
    "Jerome Ramos, Hossein A. Rahmani, Xi Wang, Xiao Fu, Aldo Lipani\n",
    "Fora: A corpus and framework for the study of facilitated dialogue\n",
    "Hope Schroeder, Deb Roy, Jad Kabbara\n",
    "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning\n",
    "Yue Yu, Jiaming Shen, Tianqi Liu, Zhen Qin, Jing Nathan Yan, Jialu Liu, Chao Zhang, Michael Bendersky\n",
    "What is the Best Way for ChatGPT to Translate Poetry?\n",
    "Shanshan Wang, Derek F. Wong, Jingming Yao, Lidia S. Chao\n",
    "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling\n",
    "Pratyush Maini, Skyler Seto, Richard He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly\n",
    "DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention\n",
    "Junda Wu, Tong Yu, Xiang Chen, Haoliang Wang, Ryan A. Rossi, Sungchul Kim, Anup Rao, Julian McAuley\n",
    "Representation Learning with Conditional Information Flow Maximization\n",
    "Dou Hu, Lingwei Wei, Wei Zhou, Songlin Hu\n",
    "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction\n",
    "Virginia K. Felkner, Jennifer A. Thompson, Jonathan May\n",
    "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models\n",
    "Martin Riddell, Ansong Ni, Arman Cohan\n",
    "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic\n",
    "Rishabh Bhardwaj, Duc Anh Do, Soujanya Poria\n",
    "Tracking the Newsworthiness of Public Documents\n",
    "Alexander Spangher, Serdar Tumgoren, Ben Welsh, Nanyun Peng, Emilio Ferrara, Jonathan May\n",
    "EWEK-QA : Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems\n",
    "Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye HAO, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, Mehdi Rezagholizadeh\n",
    "Explicating the Implicit: Argument Detection Beyond Sentence Boundaries\n",
    "Paul Roit, Aviv Slobodkin, Eran Hirsch, Arie Cattan, Ayal Klein, Valentina Pyatkin, Ido Dagan\n",
    "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models\n",
    "Shengzhi LI, Rongyu Lin, Shichao Pei\n",
    "Word Embeddings Are Steers for Language Models\n",
    "Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek F. Abdelzaher, Heng Ji\n",
    "Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation\n",
    "Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum\n",
    "Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor\n",
    "Sangwon Yu, Changmin Lee, Hojin Lee, Sungroh Yoon\n",
    "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP\n",
    "Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, Taylor Berg-Kirkpatrick\n",
    "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning\n",
    "Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou\n",
    "Confabulation: The Surprising Value of Large Language Model Hallucinations\n",
    "Peiqi Sui, Eamon Duede, Sophie Wu, Richard Jean So\n",
    "IAPT: Instance-Aware Prompt Tuning for Large Language Models\n",
    "Wei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan Ni, Xiaoling Wang, Guotong Xie\n",
    "Can Language Models Serve as Text-Based World Simulators?\n",
    "Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, Peter Jansen\n",
    "FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models\n",
    "Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch\n",
    "Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance\n",
    "Yewei Song, Cedric Lothritz, Daniel Tang, Tegawendé F. Bissyandé, Jacques Klein\n",
    "Resisting the Lure of the Skyline: Grounding Practices in Active Learning for Morphological Inflection\n",
    "Saliha Muradoglu, Michael Ginn, Miikka Silfverberg, Mans Hulden\n",
    "Speculative Contrastive Decoding\n",
    "Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou\n",
    "RDRec: Rationale Distillation for LLM-based Recommendation\n",
    "Xinfeng Wang, Jin Cui, Yoshimi Suzuki, Fumiyo Fukumoto\n",
    "Isotropy, Clusters, and Classifiers\n",
    "Timothee Mickus, Stig-Arne Grönroos, Joseph Attieh\n",
    "Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks\n",
    "Andrew Gambardella, Yusuke Iwasawa, Yutaka Matsuo\n",
    "Cleaner Pretraining Corpus Curation with Neural Web Scraping\n",
    "Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Ge Yu, Chenyan Xiong\n",
    "Simpson’s Paradox and the Accuracy-Fluency Tradeoff in Translation\n",
    "Zheng Wei Lim, Ekaterina Vylomova, Trevor Cohn, Charles Kemp\n",
    "UltraSparseBERT: 99% Conditionally Sparse Language Modelling\n",
    "Peter Belcak, Roger Wattenhofer\n",
    "SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark\n",
    "Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, Xiangliang Zhang\n",
    "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models\n",
    "Dongyang Li, Junbing Yan, Taolin Zhang, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue’, Jun Huang\n",
    "IEPile: Unearthing Large Scale Schema-Conditioned Information Extraction Corpus\n",
    "Honghao Gui, Lin Yuan, Hongbin Ye, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen\n",
    "Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model\n",
    "Haowei Du, Chen Li, Dinghao Zhang, Dongyan Zhao\n",
    "Code-Switching Can be Better Aligners: Advancing Cross-Lingual SLU through Representation-Level and Prediction-Level Alignment\n",
    "Zhihong Zhu, Xuxin Cheng, Zhanpeng Chen, Xianwei Zhuang, Zhiqi Huang, Yuexian Zou\n",
    "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models\n",
    "Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, Peter Anthony Beerel\n",
    "DDPrompt: Differential Diversity Prompting in Large Language Models\n",
    "Lin Mu, Wenhao Zhang, Yiwen Zhang, Peiquan Jin\n",
    "Monotonic Representation of Numeric Attributes in Language Models\n",
    "Benjamin Heinzerling, Kentaro Inui\n",
    "Two Issues with Chinese Spelling Correction and A Refinement Solution\n",
    "Changxuan Sun, Linlin She, Xuesong Lu\n",
    "Linear-time Minimum Bayes Risk Decoding with Reference Aggregation\n",
    "Jannis Vamvas, Rico Sennrich\n",
    "DynaSemble: Dynamic Ensembling of Textual and Structure-Based Models for Knowledge Graph Completion\n",
    "Ananjan Nandi, Navdeep Kaur, Parag Singla, Mausam .\n",
    "Fine-Tuning Pre-Trained Language Models with Gaze Supervision\n",
    "Shuwen Deng, Paul Prasse, David Robert Reich, Tobias Scheffer, Lena Ann Jäger\n",
    "Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency Parsing of Speech\n",
    "Adrien Pupier, Maximin Coavoux, Jérôme Goulian, Benjamin Lecouteux\n",
    "Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access\n",
    "Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, Robert West\n",
    "On the Semantic Latent Space of Diffusion-Based Text-To-Speech Models\n",
    "Miri Varshavsky, Roy Hirsch, Regev Cohen, Tomer Golany, Daniel Freedman, Ehud Rivlin\n",
    "Learnable Privacy Neurons Localization in Language Models\n",
    "Ruizhe Chen, Tianxiang Hu, YANG FENG, Zuozhu Liu\n",
    "Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs\n",
    "Akhila Yerukola, Saujas Vaduguru, Daniel Fried, Maarten Sap\n",
    "Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing\n",
    "Shafiuddin Rehan Ahmed, Zhiyong Wang, George Arthur Baker, Kevin Stowe, James H. Martin\n",
    "Soft Self-Consistency Improves Language Models Agents\n",
    "Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal\n",
    "RecGPT: Generative Pre-training for Text-based Recommendation\n",
    "Hoang Ngo, Dat Quoc Nguyen\n",
    "MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations\n",
    "Gia-Bao Dinh Ho, Chang Wei Tan, Zahra Zamanzadeh Darban, Mahsa Salehi, Reza Haf, Wray Buntine\n",
    "What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects\n",
    "Verena Blaschke, Christoph Purschke, Hinrich Schuetze, Barbara Plank\n",
    "What Does Parameter-free Probing Really Uncover?\n",
    "Tommi Buder-Gröndahl\n",
    "ATLAS: Improving Lay Summarisation with Attribute-based Control\n",
    "Zhihao Zhang, Tomas Goldsack, Carolina Scarton, Chenghua Lin\n",
    "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models\n",
    "Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, zhongyu wei\n",
    "Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark\n",
    "Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi, Oskar Holmström\n",
    "Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval\n",
    "João Coelho, Bruno Martins, Joao Magalhaes, Jamie Callan, Chenyan Xiong\n",
    "That’s Optional: A Contemporary Exploration of “that” Omission in English Subordinate Clauses\n",
    "Ella Rabinovich\n",
    "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?\n",
    "Haozhe An, Christabel Acquaye, Colin Wang, Zongxia Li, Rachel Rudinger\n",
    "Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster\n",
    "Agostina Calabrese, Leonardo Neves, Neil Shah, Maarten W. Bos, Björn Ross, Mirella Lapata, Francesco Barbieri\n",
    "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models\n",
    "Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava, Robert West, Zhou Yu, Kathleen McKeown\n",
    "Estimating the Level of Dialectness Predicts Inter-annotator Agreement in Multi-dialect Arabic Datasets\n",
    "Amr Keleg, Walid Magdy, Sharon Goldwater\n",
    "Born Differently Makes a Difference: Counterfactual Study of Bias in Biography Generation from a Data-to-Text Perspective\n",
    "Biaoyan Fang, Ritvik Dinesh, Xiang Dai, Sarvnaz Karimi\n",
    "Greed is All You Need: An Evaluation of Tokenizer Inference Methods\n",
    "Omri Uzan, Craig W Schmidt, Chris Tanner, Yuval Pinter\n",
    "Sign Language Translation with Sentence Embedding Supervision\n",
    "HAMIDULLAH Yasser, Josef van Genabith, Cristina España-Bonet\n",
    "STREAM: Simplified Topic Retrieval, Exploration, and Analysis Module\n",
    "Anton Frederik Thielmann, Arik Reuter, Christoph Weisser, Gillian Kant, Manish Kumar, Benjamin Säfken\n",
    "DocFinQA: A Long-Context Financial Reasoning Dataset\n",
    "Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, Chris Tanner\n",
    "MaskLID: Code-Switching Language Identification through Iterative Masking\n",
    "Amir Hossein Kargaran, François Yvon, Hinrich Schuetze\n",
    "An Empirical Analysis on Large Language Models in Debate Evaluation\n",
    "Xinyi Liu, Pinxin Liu, Hangfeng He\n",
    "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains\n",
    "Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, Brian Thompson\n",
    "IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages\n",
    "Saiful Haq, Ashutosh Sharma, Omar Khattab, Niyati Chhaya, Pushpak Bhattacharyya\n",
    "AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization\n",
    "Yunxiao Zhao, Zhiqiang Wang, Xiaoli Li, Jiye Liang, Ru Li\n",
    "Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research\n",
    "Surangika Ranathunga, Nisansa de Silva, Dilith Jayakody, Aloka Fernando\n",
    "The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models\n",
    "Noah Yamamoto Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz\n",
    "Don’t Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models\n",
    "Anna Bavaresco, Alberto Testoni, Raquel Fernández\n",
    "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs\n",
    "Alberto Testoni, Juell Sprott, Sandro Pezzelle\n",
    "Are LLMs classical or nonmonotonic reasoners? Lessons from generics\n",
    "Alina Leidinger, Robert Van Rooij, Ekaterina Shutova\n",
    "ConstitutionalExperts: Training a Mixture of Principle-based Prompts\n",
    "Savvas Petridis, Ben Wedin, Ann Yuan, James Wexler, Nithum Thain\n",
    "Time Sensitive Knowledge Editing through Efficient Finetuning\n",
    "Xiou Ge, Ali Mousavi, Edouard Grave, Armand Joulin, Kun Qian, Benjamin Han, Mostafa Arefiyan, Yunyao Li\n",
    "PRewrite: Prompt Rewriting with Reinforcement Learning\n",
    "Weize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky\n",
    "SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes\n",
    "Mukul Bhutani, Kevin Robinson, Vinodkumar Prabhakaran, Shachi Dave, Sunipa Dev\n",
    "Paraphrasing in Affirmative Terms Improves Negation Understanding\n",
    "MohammadHossein Rezaei, Eduardo Blanco\n",
    "Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios\n",
    "Bin Sun, Jianfeng Li, Hao Zhou, Fandong Meng, Kan Li, Jie Zhou\n",
    "Consistency Training by Synthetic Question Generation for Conversational Question Answering\n",
    "Hamed Hematian Hemati, Hamid Beigy\n",
    "How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?\n",
    "Anushka Singh, Ananya B. Sai, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Mitesh M Khapra\n",
    "Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages\n",
    "Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, Jimmy Lin\n",
    "Cross-Modal Projection in Multimodal LLMs Doesn’t Really Project Visual Attributes to Textual Space\n",
    "Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, Srijan Kumar\n",
    "Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition\n",
    "Hyeonseok Kang, Hyein Seo, Jeesu Jung, Sangkeun Jung, Du-Seong Chang, Riwoo Chung\n",
    "Aligning Large Language Models via Fine-grained Supervision\n",
    "Dehong Xu, Liang Qiu, Minseok Kim, Faisal Ladhak, Jaeyoung Do\n",
    "Annotating FrameNet via Structure-Conditioned Language Generation\n",
    "Xinyue Cui, Swabha Swayamdipta\n",
    "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms\n",
    "Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang\n",
    "Towards Artwork Explanation in Large-scale Vision Language Models\n",
    "Kazuki Hayashi, Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe\n",
    "On the Hallucination in Simultaneous Machine Translation\n",
    "Meizhi Zhong, Kehai Chen, Zhengshan Xue, Lemao Liu, Mingming Yang, Min Zhang\n",
    "Self-Augmented In-Context Learning for Unsupervised Word Translation\n",
    "Yaoyiran Li, Anna Korhonen, Ivan Vulić\n",
    "RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records\n",
    "Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May Dongmei Wang, Joyce C. Ho, Carl Yang\"\"\"\n",
    "# split demo_papers with each line as a list element\n",
    "main_papers = mainpapers.split(\"\\n\")\n",
    "# 选出奇数行 of mainpapers\n",
    "main_papers =  main_papers[0::2]\n",
    "\n",
    "main_papers,len(main_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACL 2024 Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fb1da101eb4334ae687c5cd036c739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-7B-Instruct\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 定义分类函数\n",
    "def classify_papers(papers, domain, output_file=\"classified_papers.json\"):\n",
    "    classified_data = []\n",
    "    for paper in tqdm(papers):\n",
    "        prompt = f\"判断以下论文摘要是否与{domain}相关：\\n\\n{paper}\\n\\n请回答是或否。如果相关，则回答是，如果不相关，则回答否，并给出是或者否的理由和解读。\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # 设置attention mask\n",
    "        attention_mask = model_inputs['attention_mask'].to(device)\n",
    "        input_ids = model_inputs['input_ids'].to(device)\n",
    "        \n",
    "        generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=512, do_sample=True)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        classified_data.append({\"paper\": paper, \"response\": response.strip()})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(classified_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    return classified_data\n",
    "\n",
    "# # 示例论文摘要列表\n",
    "# papers = [\n",
    "#     \"This paper presents a new approach to natural language processing using transformer models.\",\n",
    "#     \"The study explores the effects of climate change on marine ecosystems.\",\n",
    "#     \"A novel method for image recognition in computer vision is introduced in this research.\",\n",
    "#     \"We analyze the interpretability of large language models and their inner mechanisms.\"\n",
    "# ]\n",
    "\n",
    "# 分类论文\n",
    "domain = \"Large lauguage models的可解释性或理解LLM内在机制,例如，从LLM的内部block、attention结构上解释LLM的相关能力。帮助人理解LLM的内部结构和解释的。\"\n",
    "classified_papers = classify_papers(main_papers, domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与该领域相关的论文摘要共有124篇:\n",
      "- Inference to the Best Explanation in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到“推理到最佳解释”（Inference to the Best Explanation）在大型语言模型中的应用。这与Large language models的可解释性或理解其内在机制有关。在人工智能领域，特别是深度学习和自然语言处理中，解释模型的决策过程和内部工作原理是一个重要但又具有挑战性的任务。大型语言模型（LLMs）因其强大的生成和理解能力而受到关注，但它们的决策过程往往是黑箱性质的，缺乏透明度。\n",
      "\n",
      "推理到最佳解释是一种科学方法论，它试图通过寻找最能解释观察结果的假设来形成理论。将这种方法应用于大型语言模型意味着尝试理解模型是如何基于输入数据生成输出的，以及为什么它选择了特定的输出。这可能涉及分析模型内部的块（如Transformer层）、注意力机制等，以揭示模型决策背后的逻辑和模式。\n",
      "\n",
      "因此，这个论文的主题与帮助人们理解LLM的内部结构和解释能力紧密相关，属于研究LLM可解释性的一部分。通过这样的研究，可以提高模型的透明度和可信度，为更广泛的应用提供支持。\n",
      "\n",
      "- SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这个论文标题“SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs”表明它探讨的是如何将文本和数值数据融合在一起以理解大型语言模型（Large Language Models, LLMs）中的信息融合机制。这涉及到对LLMs内部结构的理解，特别是它们如何处理和整合不同形式的数据来生成输出。这种研究方向直接与LLMs的可解释性和理解其内在机制有关。通过研究信息融合过程，作者可能试图揭示LLMs如何从多个输入源中提取关键信息，以及这些过程背后的机制，这有助于提升我们对LLMs内部工作的理解。因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关。\n",
      "\n",
      "- Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这篇论文标题《引擎盖下的信心：大型语言模型中信心概率一致性调查》表明它关注的是大型语言模型（Large Language Models, LLMs）中的信心概率一致性问题。在深度学习模型尤其是大型语言模型中，理解模型的预测自信度是非常重要的，因为这关系到模型输出的可靠性和模型在不同情况下的表现。通过研究模型在生成文本时的信心与实际概率之间的关系，可以更深入地了解模型的决策过程和内部机制。\n",
      "\n",
      "论文可能从多个角度探讨这一主题，包括但不限于：\n",
      "1. **内部结构分析**：可能通过分析模型的特定组件（如Transformer架构中的多头注意力机制、前馈网络等）来探究它们如何影响模型的预测自信度。\n",
      "2. **特征重要性**：研究哪些输入特征对模型的自信度有显著影响，从而提供对模型决策过程的理解。\n",
      "3. **数据分布与偏差**：探讨训练数据分布如何影响模型的预测自信度，以及是否存在潜在的偏见或不一致性。\n",
      "4. **解释性工具**：开发或应用现有的解释性工具和技术，如SHAP、LIME等，以可视化和量化模型的预测过程，增强模型的可解释性。\n",
      "\n",
      "总之，这篇论文旨在深入探讨大型语言模型内部的工作原理，特别是它们如何评估自身预测的可信度，这对于提高模型的透明度和可靠性至关重要。因此，它确实与大型语言模型的可解释性以及理解其内部机制相关。\n",
      "\n",
      "- OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following\n",
      "  回复: 是。\n",
      "\n",
      "理由：根据提供的论文摘要标题 \"OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following\"（OPEx：基于大型语言模型的实体指令跟随中以大型语言模型为中心的代理组件级分析），该论文似乎关注于对基于大型语言模型（LLMs）的代理进行详细的、组件级别的分析。这表明研究者在探索和解释LLMs在特定任务（如实体指令跟随）中的工作原理和能力。通过分析模型的不同组件（可能包括内部块、注意力机制等），研究可能旨在提高我们对LLMs内在机制的理解，以及它们如何被设计来处理特定类型的任务。这种分析有助于人类更好地理解LLMs的工作方式，以及它们在执行复杂任务时的能力和限制。因此，该论文摘要与大型语言模型的可解释性或理解其内部机制的主题直接相关。\n",
      "\n",
      "- Token-wise Influential Training Data Retrieval for Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《大型语言模型中的基于token的影响力训练数据检索》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，论文可能探讨了如何通过分析特定token（即语言模型中的词汇单元）在训练数据中的影响力，来提高对LLMs行为的理解。这种研究方法可以帮助人们更好地理解LLMs是如何学习和处理信息的，特别是在处理文本数据时的决策过程。通过关注训练数据中各个token的影响，研究者可能揭示了模型如何形成特定的预测模式、上下文依赖性以及注意力分配等关键方面，这些都是LLMs内部结构和工作原理的重要组成部分。因此，这篇论文旨在增强我们对LLMs内部机制的理解，符合题目的要求。\n",
      "\n",
      "- The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到对预训练语言模型（Pretrained Language Models, LLMs）中子网络泛化（Subnetwork Generalization）的理解。在大型语言模型领域，可解释性和理解模型的内部机制是当前研究的重要方向之一。通过对模型内部结构、组件（如blocks、attention机制等）的研究，可以更好地理解模型如何工作，以及它们是如何学习到特定任务的表示的。\n",
      "\n",
      "\"Understanding Subnetwork Generalization\"部分表明，该论文旨在探索预训练语言模型中的特定子网络如何能够泛化到不同的任务和数据集。这与LLM的可解释性紧密相关，因为它涉及到理解模型决策过程中的内在逻辑和机制。通过分析模型的不同部分如何协同工作以及它们在不同情境下的表现，研究人员可以提供更深入的见解，帮助人类更好地理解这些复杂模型的内部结构和工作原理。\n",
      "\n",
      "因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制相关，因为它关注于模型内部的子网络如何泛化并影响整体性能，从而为理解LLM提供了新的视角。\n",
      "\n",
      "- AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它探讨了如何通过可信度估计的解释调整（Explanation Tuning with Plausibility Estimation）来激发大型语言模型（Large Language Models, LLMs）的抽象能力（Abstraction Ability）。这涉及到对LLMs内部结构的理解以及如何解释它们的工作机制，特别是关注于从LLMs的内部块（blocks）、注意力结构（attention mechanisms）等层面去分析和解释模型的能力。通过这种研究方法，作者旨在帮助人们更好地理解LLMs的内部运作原理，提高对这些复杂模型的可解释性。因此，该论文摘要与Large language models的可解释性和理解其内在机制密切相关。\n",
      "\n",
      "- Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这个论文标题表明它关注的是一个名为“Multimodal ArXiv”的数据集，该数据集旨在通过改进大型视觉-语言模型（Large Vision-Language Models）对科学文献的理解能力来提升科学研究的可理解性。这里的“大型视觉-语言模型”（Large Vision-Language Models）指的是能够同时处理文本和图像信息的大规模预训练模型，如ViLBERT、M4等。\n",
      "\n",
      "该研究的核心在于通过构建或利用现有的数据集来增强这些模型在理解和解释科学内容方面的性能。这涉及到对模型内部结构和机制的探索，特别是如何处理多模态输入（即文本和图像信息）以及如何整合这些信息以产生更准确、更有意义的输出。因此，它直接关联到大型语言模型的可解释性和理解其内在机制的主题。\n",
      "\n",
      "具体来说，研究可能涉及以下方面：\n",
      "1. **多模态融合**：探索如何有效地将文本和图像信息结合起来，使得模型能够更好地理解科学文本中的视觉元素。\n",
      "2. **注意力机制**：分析模型在处理不同模态信息时的注意力分配，揭示哪些部分的信息对于理解特定概念或解释至关重要。\n",
      "3. **内部结构分析**：研究模型的多层次结构，包括从输入层到输出层的每一层如何影响最终的理解结果。\n",
      "4. **解释性技术**：应用可解释性方法，如可视化注意力图、梯度解释器等，来提高对模型决策过程的理解。\n",
      "\n",
      "综上所述，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关，因为它旨在通过实证研究来提升模型在处理复杂、多模态科学信息时的理解能力，并探讨实现这一目标的关键技术路径。\n",
      "\n",
      "- ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它涉及了一种名为“ConSiDERS”的框架，该框架用于对生成式大型语言模型（Large Language Models, LLMs）进行人类评估。由于其关注点在于评估方法，这与大型语言模型的可解释性和理解其内部机制相关。特别是，如果该框架考虑了从LLM的内部组件（如块、注意力结构等）来评价这些模型的能力，那么它就直接关联到解释LLM的内部结构和提供对其工作原理的理解这一主题。因此，这个论文摘要与大型语言模型的可解释性以及理解它们的内在机制相关。\n",
      "\n",
      "- DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《DeepSeekMoE：在混合专家语言模型中的终极专家专业化》似乎与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。混合专家（MoE, Mixture-of-Experts）模型是一种将复杂任务分解为多个子任务，每个子任务由一个专门的“专家”模型来处理的方法。在语言模型中应用这种策略可以提高模型的性能，尤其是在处理具有不同特性的文本时。\n",
      "\n",
      "论文名称中的“DeepSeekMoE”表明研究可能涉及深度学习技术，探索如何在混合专家框架下实现更深层次的语言模型。通过专家的“专业化”，模型能够针对特定类型的输入进行优化，这可能涉及到对模型内部的block、attention结构等关键组件的深入理解和改进，以增强其可解释性和性能。\n",
      "\n",
      "因此，该论文很可能是关于如何通过混合专家模型架构来增强大型语言模型的能力，同时试图提供对这些模型内部工作原理的洞察，包括它们如何处理不同的输入、如何分配计算资源给不同的任务，以及如何通过更精细的控制来提高整体性能和可解释性。这种研究对于帮助人们理解大型语言模型的内部结构和工作方式是非常有价值的。\n",
      "\n",
      "- Active Prompting with Chain-of-Thought for Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文探讨了如何通过“链式思维”（Chain-of-Thought）主动提示（Active Prompting）的方法来增强大型语言模型的能力。这种方法涉及构建一个引导性的输入序列，使得模型能够按照人类可以理解的方式逐步推理和生成答案。通过这种方式，研究者试图提高模型输出的可解释性，让最终结果不仅仅是基于统计学习的预测，而是包含了模型执行的逻辑步骤。这有助于帮助人们更好地理解模型是如何做出决策的，从而增加对模型信任度和应用范围。\n",
      "\n",
      "因此，虽然论文可能不会深入讨论LLM的内部块（如Transformer块）或注意力机制的具体细节，但它确实关注于提高模型的可解释性，这是理解LLM内部结构和机制的重要方面。通过提高可解释性，人们可以更好地理解模型的工作原理，这对于进一步改进模型设计、应用以及在更复杂任务上的使用至关重要。\n",
      "\n",
      "- Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，它通过“20问题游戏”来探索LLMs的多轮规划能力。这种实验方法旨在评估和理解模型在决策过程中的思考逻辑和策略选择，从而提供对LLMs内部工作原理的洞察。通过观察模型如何在限定的提问次数内逐步缩小答案范围，研究者可以尝试揭示模型的理解深度、推理能力和决策过程。这有助于增进人类对LLMs内部结构和工作机理的理解，特别是与模型的规划、决策和信息整合能力相关的方面。\n",
      "\n",
      "- Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题“Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models”表明它探讨了如何将依赖结构（dependency structures）整合到转换器语言模型（Transformer Language Models）中。这涉及到对大型语言模型（Large Language Models，LLMs）内部机制的理解和解释，特别是关注于它们如何处理和表示语法结构的信息。通过在Transformer架构中集成依赖解析，研究者旨在增强模型对自然语言句子结构的理解，从而提升其生成、理解和解释文本的能力。这种工作直接关联到LLMs的可解释性和理解能力，因为它涉及到分析和解释模型如何处理语言中的依赖关系，以及这些处理方式如何影响模型的整体性能和输出。此外，通过改进模型对依赖结构的处理，研究者也提供了洞察LLMs内部工作机制的途径，有助于构建更强大的、可解释的自然语言处理系统。\n",
      "\n",
      "- Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这个论文标题表明它探讨的是使用大型语言模型（Large Language Models, LLMs）来分析具有时间复杂性的事件。在深度学习领域，尤其是自然语言处理中，大型语言模型往往以其强大的生成和理解文本的能力而闻名。然而，它们的内部工作原理，包括如何处理长上下文信息和理解时间序列数据，通常被认为是黑盒模型，即人们很难理解其决策过程或内部结构。\n",
      "\n",
      "该论文标题中的“Benchmark”一词暗示了它可能旨在建立一个标准或评估框架，用于测试和改进大型语言模型在理解和处理包含时间复杂性的事件方面的性能。这涉及到探索模型如何在时间维度上处理信息、如何整合长时间跨度内的上下文以及如何解释其预测或生成的文本。因此，该论文似乎直接关注于提升大型语言模型的可解释性和理解能力，特别是在涉及时间序列和长期依赖性的场景中。通过这样的研究，可以帮助人们更好地理解这些模型的内部结构和工作原理，从而增强对它们的信任和应用。\n",
      "\n",
      "- ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“ValueBench：全面评估价值观取向和大型语言模型理解能力的方向”表明它关注于大型语言模型（Large Language Models, LLMs）的理解能力，特别是它们对价值观的理解。这涉及到LLM的可解释性和对内部机制的理解，因为评估这些模型的能力和理解力需要深入到模型的工作原理中去。论文可能探讨了如何通过特定的测试集或评估框架来衡量和理解LLMs在处理不同价值观和伦理问题时的表现，从而提供了一种方法论上的视角来增强我们对LLMs内部结构和决策过程的理解。因此，该论文与大型语言模型的可解释性以及理解其内在机制密切相关。\n",
      "\n",
      "- SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations\n",
      "  回复: 是。\n",
      "\n",
      "这篇论文标题“SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations”与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制密切相关。论文名称中提到了“Fine-tuning”，这通常与训练大型预训练语言模型有关，以适应特定任务。在大型语言模型的上下文中，“细调”可以指调整模型参数以提高在特定任务上的性能。\n",
      "\n",
      "此外，论文还强调了“Generating Predictions and Natural Language Explanations”，即生成预测和自然语言解释。这表明研究可能涉及如何让模型不仅能够做出预测，还能以人类可读的方式解释这些预测是如何得出的。这种解释对于理解模型的决策过程至关重要，特别是对于大型语言模型，它们的决策过程通常是黑盒性质的。\n",
      "\n",
      "综上所述，这篇论文似乎旨在探索如何通过稀疏细调来实现快速的提示学习，以同时生成预测和自然语言解释。这种工作有助于增进我们对大型语言模型内部结构的理解，以及如何使这些模型的决策过程更加透明和可解释。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks\n",
      "  回复: 是。\n",
      "\n",
      "这个论文题目“通过可逆神经网络学习解释的分离语义空间”表明了它关注的是如何通过使用可逆神经网络（Invertible Neural Networks）来学习和理解大型语言模型（Large Language Models, LLMs）的内部结构以及它们生成解释的能力。可逆神经网络是一种能够学习可逆映射的深度学习模型，这在一定程度上可以增强我们对模型决策过程的理解，因为它们允许我们追踪输入到输出的转换过程。\n",
      "\n",
      "在大型语言模型的背景下，研究者可能探索如何利用这些网络来分解模型生成的解释（即，模型是如何从输入文本中推断出特定概念或信息的），从而构建更清晰、可解释的语义空间。这样的工作有助于提高LLMs的透明度和可解释性，使得人们能够更好地理解模型的决策过程，这对于许多实际应用来说是非常重要的，尤其是在需要模型解释其决策的领域，如医疗、法律等。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性或理解其内部机制相关，特别是从LLM的内部结构（如block、attention机制）上进行解释，以帮助人们理解模型的工作原理。\n",
      "\n",
      "- CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文标题\"CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning\"（CANDLE：用于常识推理的大规模语言模型的迭代概念化和实例化提炼）表明，该研究主要关注如何从大规模语言模型（Large Language Models, LLMs）中提取和理解其内部机制，特别是针对常识推理任务。通过“迭代概念化”和“实例化提炼”的方法，研究者试图深入挖掘和解释LLMs在处理常识问题时的内在逻辑和决策过程。这涉及到对LLMs内部结构的理解以及如何从这些模型中提取出有意义的概念和规则，从而增强我们对LLMs工作原理的认识，提升其可解释性和透明度。因此，该论文摘要与大型语言模型的可解释性或理解其内在机制相关。\n",
      "\n",
      "- Surgical Feature-Space Decomposition of LLMs: Why, When and How?\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明了它探讨的主题是大型语言模型（Large Language Models, LLMs）的特征空间分解在手术级的应用。\"Why, When and How?\" 的部分暗示了论文将详细讨论为何需要进行这种分解、何时应用以及如何实施这一过程。这种讨论的核心在于理解并解释LLMs的内部工作机制，包括它们的内部块（blocks）和注意力机制（attention structures）。通过这样的分解，研究人员和开发者可以更好地理解LLMs如何处理和生成文本，从而提高模型的可解释性。因此，这个论文摘要与Large language models的可解释性或理解其内在机制相关，特别是从模型的内部组件和工作原理角度出发，以帮助人们更好地理解LLM的内部结构和工作方式。\n",
      "\n",
      "- Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题和摘要提到了\"Enhancing Large Language Models Reasoning\"（增强大型语言模型推理），这直接关联到Large Language Models（大语言模型）的可解释性和理解其内在机制的主题。特别是，“通过不确定性意识的自适应指导增强推理”这部分强调了对模型推理过程的理解和提升，可能涉及到模型内部的结构、决策过程以及如何通过理解和管理不确定性来改进模型的表现。因此，这篇论文似乎关注于深入理解大型语言模型的工作原理，包括它们如何处理信息、进行决策，以及如何通过特定策略（如不确定性意识的自适应指导）来优化这些过程，这些都是大型语言模型可解释性和理解其内部机制的关键方面。\n",
      "\n",
      "- VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由及解读：这篇论文的标题“VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models”表明它探讨了通过大型语言模型（Large Language Models, LLMs）进行基于知识推理的命名实体识别（Named Entity Recognition, NER）的验证增强方法。命名实体识别是一种自然语言处理任务，涉及到识别文本中的特定实体类型（如人名、地名、组织名等）。通过结合知识图谱等外部知识源进行推理，可以提高命名实体识别的准确性和可解释性。\n",
      "\n",
      "论文中的关键词“验证”（Verification）暗示了对模型决策过程的验证和解释，这与大型语言模型的可解释性和理解其内部机制的主题直接相关。通过知识引导的推理，作者试图增强模型的决策过程，使其在执行命名实体识别时能够提供更合理的解释和更高的可信度。这种增强不仅关注于模型性能的提升，还侧重于增加模型的透明度和可理解性，这对于理解大型语言模型的内部工作原理和结构具有重要意义。\n",
      "\n",
      "因此，这篇论文与大型语言模型的可解释性或理解其内在机制密切相关，尤其是从模型的内部结构、注意力机制（Attention）等方面进行分析和解释，以帮助人们更好地理解这些模型是如何工作的。\n",
      "\n",
      "- Making Long-Context Language Models Better Multi-Hop Reasoners\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它关注的是提升长上下文语言模型（Long-Context Language Models）作为多跳推理工具的能力。在深度学习领域，尤其是自然语言处理（NLP），大型语言模型（Large Language Models, LLMs）因其强大的生成和理解文本的能力而受到广泛关注。然而，这些模型的内部工作原理往往非常复杂，缺乏透明度，即所谓的“黑盒”问题。这限制了我们对模型决策过程的理解，也影响了它们在实际应用中的可解释性和可靠性。\n",
      "\n",
      "提升长上下文语言模型作为多跳推理工具的能力意味着论文可能探讨了如何改进模型处理长序列文本信息的能力，以及如何让模型在进行推理时能够跨多个步骤或多个实体进行思考。这种多跳推理能力对于解决复杂的问题和理解复杂的语境至关重要，尤其是在需要理解文本中隐含关系或执行逻辑推理的任务中。\n",
      "\n",
      "论文可能通过引入新的架构设计、优化现有模型的训练过程、改进注意力机制（Attention Mechanism）或其他技术来提高模型的多跳推理能力。通过增强模型在处理长上下文时的理解力和推理能力，该研究有助于增加模型的可解释性，使人们能够更好地理解模型是如何做出特定决策的，从而提高模型在实际应用中的可信度和效率。\n",
      "\n",
      "因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制密切相关，特别是从模型的内部块（如注意力机制）和整体结构上解释LLM的相关能力，以帮助人理解LLM的内部结构和解释的方面。\n",
      "\n",
      "- Time is Encoded in the Weights of Finetuned Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文标题“Time is Encoded in the Weights of Finetuned Language Models”表明研究者探索了时间信息如何被编码在经过微调的语言模型的权重中。这涉及到对语言模型内部结构的理解，特别是关注于时间信息如何影响模型的决策过程以及模型权重的分布。\n",
      "\n",
      "大型语言模型通常包含复杂的结构，如多层的神经网络、注意力机制等，这些结构允许模型处理序列数据并生成上下文相关的文本输出。理解模型如何利用时间信息来生成准确的预测或输出，对于提高模型的性能和增加模型的可解释性至关重要。通过研究时间信息如何在模型的权重中被编码，研究人员可以提供更深入的洞察，帮助人们理解模型的内部工作原理，以及模型如何学习和应用时间相关的模式。\n",
      "\n",
      "因此，这个研究主题不仅探讨了特定的模型特性（即时间信息的编码），还涉及到了更广泛的领域，包括模型的可解释性和内部机制的理解。这正是与大型语言模型的可解释性和理解其内在机制相关的重要研究方向。\n",
      "\n",
      "- Long-Context Language Modeling with Parallel Context Encoding\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《具有并行上下文编码的长上下文语言建模》与大型语言模型（Large Language Models）的可解释性或理解其内在机制相关。具体来说，论文可能探讨了如何通过并行上下文编码技术来提高语言模型在处理长上下文文本时的性能。这种技术涉及对模型输入的不同部分进行并行处理，从而增强模型理解和生成长文本的能力。从LLM的内部block、attention结构上，这样的方法可能涉及到改进模型的注意力机制，使得模型能够更有效地捕捉到不同位置之间长距离依赖关系，这对于理解LLM的内部结构和解释其工作原理至关重要。\n",
      "\n",
      "此外，通过并行化处理，论文也可能探索了如何优化计算效率，减少训练和推理时间，这与提高模型的可解释性和理解其内部机制紧密相关。通常，更高效的模型架构和训练过程更容易分析和解释，因为它们往往遵循更清晰的设计原则和数学逻辑。\n",
      "\n",
      "总之，这个论文主题直接关联到大型语言模型的内部结构、机制及其处理长文本的能力提升，因此与LLM的可解释性和理解其内在机制相关。\n",
      "\n",
      "- Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《生成预训练结构转换器：大规模无监督语法语言模型》似乎与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，它关注于构建大规模的无监督语法语言模型，这可能涉及到对LLMs内部结构的理解和解释。\n",
      "\n",
      "在自然语言处理领域，理解大型语言模型如何工作以及它们的决策过程对于提高模型性能、确保模型的可靠性和公平性至关重要。结构转换器（Structured Transformers）这一术语暗示了研究者可能正在探索如何通过设计特定的结构来改进或解释Transformer架构在语言建模任务中的表现。\n",
      "\n",
      "通常，这类研究可能会涉及以下几个方面：\n",
      "1. **结构设计**：探讨特定的模型结构如何影响语言模型的性能和可解释性。\n",
      "2. **注意力机制**：分析注意力机制在不同上下文中的应用，以及如何通过调整注意力权重来提高模型的解释度。\n",
      "3. **可解释性技术**：开发或应用技术来增强我们对模型决策过程的理解，比如使用可视化工具、特征重要性分析等。\n",
      "4. **大规模训练**：讨论大规模数据集对模型泛化能力和解释性的影响。\n",
      "\n",
      "因此，这个题目表明作者们可能正在深入探讨大型语言模型的内部结构、它们的工作原理，以及如何提高这些模型的可解释性，这些都是与大型语言模型的可解释性和理解其内在机制直接相关的主题。\n",
      "\n",
      "- ItD: Large Language Models Can Teach Themselves Induction through Deduction\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明了它探讨的主题是大型语言模型（Large Language Models, LLMs）如何通过归纳（Induction）自我学习。\"ItD\"可能是论文的缩写名，而标题中的“教自己”暗示了自我学习的过程。通常，这样的研究会深入探讨语言模型的内部机制，特别是它们如何处理和生成文本、学习模式以及进行推理等任务。通过“归纳通过演绎”这一方法，论文可能在探索语言模型如何从输入数据中学习抽象概念和规则，从而提升其理解能力和生成高质量文本的能力。\n",
      "\n",
      "这种研究对于理解大型语言模型的内在机制是高度相关的，因为它涉及到模型如何处理信息、执行推理以及自我提升的过程。这不仅包括模型的块（blocks）、注意力机制（attention structures）等技术细节，也涵盖了模型如何利用这些组件来实现更高级的认知任务，如理解上下文、生成连贯的对话或创作故事等。\n",
      "\n",
      "因此，这篇论文关注的主题与大型语言模型的可解释性和理解其内部机制密切相关，为帮助人们更好地理解这些复杂系统提供了理论基础和实践洞见。\n",
      "\n",
      "- Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与 Large Language Models（LLM）的可解释性和理解其内在机制相关。标题“Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration”（不要幻觉，退出：通过多LLM协作识别LLM知识缺口）暗示了研究者正在探讨LLM在知识获取和表现上的局限性。这涉及到分析和理解LLM的内部结构，特别是它们如何处理信息、学习模式以及可能产生的错误或不确定性。\n",
      "\n",
      "研究通过多LLM协作来识别这些知识缺口，这表明作者在探索LLM之间的差异、交互和互补性，以提供更全面的理解。这种方法有助于揭示LLM在特定任务或主题上的盲点或不足，从而增强我们对模型工作原理的理解。此外，通过比较不同模型的表现，可以更好地理解LLM的内在机制，包括它们如何分配注意力、处理信息以及在不同上下文中的表现差异。\n",
      "\n",
      "总之，这个论文关注的是通过多模型协作来增强对LLM的理解，特别是它们的知识边界和内部工作机制，因此与大型语言模型的可解释性和理解其内在机制直接相关。\n",
      "\n",
      "- Mission: Impossible Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题 \"Mission: Impossible Language Models\" 提示该研究可能探讨了语言模型的一些复杂或难以理解的问题，尤其是大型语言模型（Large Language Models, LLMs）。大型语言模型通常指的是参数量巨大、能够生成高质量文本的预训练模型，如GPT系列、通义千问等。这类模型由于其复杂的架构和庞大的参数量，往往在使用时表现出强大的语言生成能力，但在理解和解释其内部工作原理方面存在挑战。\n",
      "\n",
      "因此，该论文摘要如果涉及从模型的内部组件（blocks）、注意力机制（attention structures）等角度出发，探讨如何增强对大型语言模型的理解和解释性，那么它确实与大型语言模型的可解释性和理解其内在机制相关。这有助于帮助人类更好地理解这些模型的工作原理，以及它们是如何产生特定输出的。这种研究对于提高模型的透明度、可信度以及在实际应用中的安全性至关重要。\n",
      "\n",
      "- ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling”表明其主要关注点在于通过自我指导的自适应相关性标签来对黑盒大型语言模型进行检索器的对齐。这个过程涉及到理解大型语言模型（LLM）的内部工作原理和机制，特别是如何在没有直接访问其内部结构的情况下（即“黑盒”模型），提升检索器（用于搜索和提取信息的系统）与LLM的协同作用。\n",
      "\n",
      "论文的主旨在于提高检索器和大型语言模型之间的性能一致性，这涉及到对LLM的可解释性和理解其内在机制的研究。通过使用自适应相关性标签，作者试图探索和增强模型对于特定任务的理解能力，特别是那些依赖于复杂语义理解和上下文推理的任务。这种方法不仅有助于提高模型的实用性，还能为研究者提供关于LLM内部决策过程的洞察，从而更好地理解这些模型的工作原理。\n",
      "\n",
      "因此，该论文与大型语言模型的可解释性、理解其内部机制以及解释相关能力紧密相关，旨在帮助人们更深入地理解LLM的内部结构和工作方式。\n",
      "\n",
      "- CaMML: Context-Aware Multimodal Learner for Large Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“CaMML: Context-Aware Multimodal Learner for Large Models”表明它探讨的是大型模型在多模态学习方面的上下文感知能力。大型语言模型（Large Language Models, LLMs）通常处理文本数据，但它们也可以扩展到处理多种模态（如文本、图像、音频等）的信息。因此，研究如何使这些模型对上下文有意识，特别是在处理多模态数据时，直接与LLMs的可解释性和理解其内在机制相关。\n",
      "\n",
      "上下文感知能力对于理解LLMs的工作原理至关重要，因为这涉及到模型如何整合来自不同模态的信息，并根据特定上下文生成有意义的输出。这种研究有助于揭示LLMs在处理复杂任务时的决策过程，从而提高我们对这些模型内部结构的理解。通过关注模型如何在不同的输入模式之间进行交互和整合，这项工作可能为开发更强大、更可解释的LLMs提供见解，同时也为提升模型的透明度和用户信任度做出贡献。\n",
      "\n",
      "总之，这个论文的主题与大型语言模型的可解释性和理解其内部机制密切相关，因为它探索了模型如何在多模态环境中处理和利用上下文信息。\n",
      "\n",
      "- D2LLM: Decomposed and Distilled Large Language Models for Semantic Search\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题中的\"D2LLM\"（Decomposed and Distilled Large Language Models）直接暗示了对大型语言模型（Large Language Models，LLMs）的分解（Decomposed）和提炼（Distilled）过程。这表明论文可能深入探讨了LLMs的内部结构、组件以及它们如何协同工作以实现特定任务，如语义搜索（Semantic Search）。通过分解和提炼模型，研究者可能试图揭示LLMs的内在机制，包括但不限于内部block、attention结构等关键组件如何影响模型的性能和决策过程。这种分析有助于人们更好地理解LLMs的工作原理，提高模型的可解释性，并可能为优化和改进这些模型提供理论基础。因此，该论文摘要与Large language models的可解释性和理解LLM内在机制相关。\n",
      "\n",
      "- Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到预训练语言模型（Pre-trained Language Models, LLMs）的增量学习（Incremental Learning），这与大型语言模型的可解释性和理解其内部机制密切相关。论文可能探讨了如何在现有的预训练模型基础上进行微调以实现特定任务，从而揭示或增强模型在不同任务中的理解和学习过程。通过关注“Learn or Recall？”这一问题，研究者可能试图区分预训练模型是基于记忆（Recall）还是基于理解（Learn）来进行任务处理，这直接关联到模型内部的block、attention结构以及这些组件如何共同作用以支持模型的学习和推理能力。因此，该论文旨在提供对预训练语言模型内在机制的深入理解，特别是它们如何在面对新任务时展现出学习或回忆的能力，这对于开发更可解释、更高效且能够更好地适应特定应用场景的LLMs具有重要意义。\n",
      "\n",
      "- Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs”（小模型，大见解：利用精简代理模型为LLMs决定何时及何物进行检索）表明，它探讨的是如何使用较小的模型来提高大型语言模型（LLMs）的性能，特别是在决定何时以及从何处检索信息方面。这实际上涉及到了对LLMs内部机制的理解和可解释性。\n",
      "\n",
      "论文的重点在于“代理模型”（Proxy Models），这通常指的是用于简化复杂问题、帮助理解和优化原始模型的辅助模型。在这一上下文中，代理模型被用来帮助LLMs更好地决定何时需要访问外部资源（如知识库或数据库）以补充其生成的文本，以及具体需要从这些资源中检索哪些信息。\n",
      "\n",
      "通过研究和应用这样的代理模型，可以增加对LLMs决策过程的理解，特别是它们如何处理特定任务时的信息需求和选择。这种分析不仅有助于优化模型性能，还能够提供有关模型内部工作原理的洞察，包括注意力机制、不同组件之间的交互以及模型如何分配计算资源等关键方面。\n",
      "\n",
      "因此，这个论文确实与大型语言模型的可解释性和理解其内在机制相关，尤其是从代理模型的角度探索LLMs的内部结构和决策过程。\n",
      "\n",
      "- Interpretability of Language Models via Task Spaces\n",
      "  回复: 是。\n",
      "\n",
      "理由：这个论文标题表明它探讨的是语言模型的可解释性问题，具体通过“任务空间”（Task Spaces）来进行分析。在大型语言模型（Large Language Models, LLMs）的研究中，提高模型的可解释性是一个重要方向，因为它可以帮助研究人员和开发者更好地理解模型的工作原理、决策过程以及为何模型在特定任务上的表现优于其他模型。通过分析任务空间，研究者可以揭示模型在处理不同任务时的行为模式、偏好和局限性，从而提供对LLMs内部结构和机制的洞察。\n",
      "\n",
      "解读：论文标题中的“Interpretability”直接指向了语言模型的可解释性问题，这是理解LLMs的关键方面之一。通过“Task Spaces”，作者可能试图从不同的任务视角来解析模型的行为，这有助于构建一个更全面的模型理解框架。这样的研究对于促进LLMs的发展、应用以及潜在的改进至关重要，因为它不仅提供了理论上的见解，还为实际应用中模型的优化和调整提供了指导。\n",
      "\n",
      "- Navigating the OverKill in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《在大型语言模型中的过度杀戮》表明它可能探讨了大型语言模型（Large Language Models，LLMs）中的某些复杂或过拟合的问题。\"OverKill\"一词在数学和计算机科学领域中通常被用来描述解决一个问题时使用了过于复杂或强大的工具，而实际上问题可以通过更简单的方法解决。在机器学习和深度学习的语境下，这可能意味着作者在研究大型语言模型是否存在过度复杂化的情况，即模型的大小或复杂度是否超过了其实际任务需求。\n",
      "\n",
      "由于论文题目提及了“OverKill”，这暗示着论文可能深入探讨了大型语言模型的内部结构、工作原理，以及如何优化这些模型以避免不必要的复杂性。这直接与大型语言模型的可解释性有关，因为理解模型如何做出决策、为何选择特定的参数配置或架构对于提高模型效率和性能至关重要。此外，论文也可能探讨了如何通过分析模型的内部组件（如blocks、attention机制等）来提高我们对LLMs的理解，进而改进模型的设计和应用。\n",
      "\n",
      "因此，根据题目内容推测，这篇论文很可能涉及了大型语言模型的可解释性、理解其内部机制，以及优化模型设计等方面的研究，符合关于LLM内在机制、解释以及帮助人理解LLM内部结构的主题。\n",
      "\n",
      "- NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents\n",
      "  回复: 是。\n",
      "\n",
      "理由及解读：这篇论文的标题“NextLevelBERT”表明它可能是在现有BERT模型的基础上进行了一些改进或扩展。在摘要中提到的“Masked Language Modeling with Higher-Level Representations for Long Documents”，即针对长文档的具有更高层次表示的掩码语言建模，这一主题直接与大型语言模型（Large language models）的可解释性和理解其内部机制相关。\n",
      "\n",
      "在深度学习领域，尤其是自然语言处理（NLP）中，大型语言模型（如BERT、GPT等）通常被设计为能够处理长文本输入，并通过复杂的神经网络结构生成高质量的文本输出。然而，这些模型的内部工作原理往往较为复杂，缺乏直观的解释，这限制了我们对它们决策过程的理解和优化。\n",
      "\n",
      "针对这一点，“NextLevelBERT”这样的研究尝试引入“更高层次的表示”来改善模型对长文档的处理能力，同时可能也试图提高模型的可解释性。通过使用更高级别的抽象表示，研究者可能希望使得模型在处理长文档时能够更好地捕捉上下文信息，同时也可能提供了一种途径来理解模型是如何做出特定决策的。这种提升不仅增强了模型的功能性，还增加了我们对其内部工作原理的理解，这对于进一步优化模型性能和应用至关重要。\n",
      "\n",
      "综上所述，该论文摘要与大型语言模型的可解释性和理解其内在机制密切相关，因为它涉及到通过引入更高层次的表示来改善模型处理长文档的能力，并可能涉及提供更深入的见解以增强我们对模型决策过程的理解。\n",
      "\n",
      "- When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它探讨了如何通过“重启增量性”（Restart-Incrementality）来解释Transformer模型在处理局部歧义时的工作原理。Transformer模型是大型语言模型（Large Language Models, LLMs）的一种典型实现，其内部结构包括多层自注意力机制（attention-based structure）以及前馈神经网络等组件。因此，这篇论文关注于解释Transformer模型的内部工作过程，特别是如何处理文本中的模糊或歧义部分。\n",
      "\n",
      "“重启增量性”可能指的是在模型处理文本序列时，通过在特定阶段重新开始（即“重启”）并逐步增加信息（即“增量性”）的方式来提高模型对局部信息的理解和处理能力。这种策略有助于模型更准确地捕捉到上下文中的细微差异，从而更好地解析具有歧义的短语或句子。\n",
      "\n",
      "论文的主旨与大型语言模型的可解释性和理解其内在机制密切相关，因为它试图深入探索和揭示Transformer模型如何在复杂语言环境中进行决策的过程。通过这样的研究，可以帮助人们更好地理解这些模型的工作原理，增强我们对它们的信心，并指导未来的模型设计和应用。\n",
      "\n",
      "- SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“SpaRC 和 SpaRP：空间推理表征与路径生成，用于理解大型语言模型的空间推理能力”明确表明了其关注点在于理解和分析大型语言模型（Large Language Models, LLMs）在空间推理任务上的表现和能力。具体来说，它探讨了如何通过“Spatial Reasoning Characterization”（空间推理表征）和“Path Generation”（路径生成）来评估和解释LLMs在处理空间推理问题时的内在机制和性能。这直接涉及到了对LLMs内部结构的理解以及如何解释它们在特定任务（如空间推理）上的决策过程，因此与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- Spectral Filters, Dark Signals, and Attention Sinks\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，“谱滤波器”（Spectral Filters）、“暗信号”（Dark Signals）和“注意陷阱”（Attention Sinks）这几个概念都与神经网络（包括语言模型）的行为和性能分析有关。\n",
      "\n",
      "1. **谱滤波器**：在神经网络中，谱滤波器常用于理解网络如何对输入数据进行频率分解，以及不同层在网络中的作用。对于语言模型而言，这可能意味着研究模型如何处理不同长度或频率的语言模式，从而提高生成文本的质量或理解力。\n",
      "\n",
      "2. **暗信号**：在深度学习模型中，暗信号通常指的是那些对模型输出贡献较小或几乎不贡献的信号。在LLMs中，理解暗信号可以帮助我们识别哪些输入特征或中间表示对最终预测贡献不大，从而优化模型效率或理解模型决策过程。\n",
      "\n",
      "3. **注意陷阱**：注意机制是许多LLMs的关键组件，允许模型在处理输入时聚焦于特定部分。注意陷阱可能是指某些情况下，模型可能会过度依赖特定的注意力分配而忽视其他重要信息，导致模型性能下降或难以解释其决策过程。\n",
      "\n",
      "因此，这篇论文似乎探讨了通过分析这些概念来增强对LLMs的理解和解释，特别是关于它们的频率处理能力、信号贡献度以及注意力机制的有效性等方面。这种研究有助于提升我们对LLMs内部工作原理的认识，从而改进模型设计和应用。\n",
      "\n",
      "- RelayAttention for Efficient Large Language Model Serving with Long System Prompts\n",
      "  回复: 是。\n",
      "\n",
      "理由及解读：这篇论文的标题“RelayAttention for Efficient Large Language Model Serving with Long System Prompts”表明它探讨的是如何使用一种名为“RelayAttention”的方法来提高大型语言模型（Large Language Models, LLMs）在处理长系统提示时的服务效率。这一主题直接与大型语言模型的内部结构和工作原理有关，特别是与注意力机制（attention mechanism）如何影响模型性能和可解释性相关。\n",
      "\n",
      "大型语言模型通常依赖于复杂的多层神经网络架构，其中注意力机制是一种关键组件，用于决定哪些输入信息对生成的输出最相关。通过优化注意力机制，可以提高模型的计算效率，同时保持或增强其性能。因此，“RelayAttention”可能旨在提供一种更高效的方式来处理信息流，特别是在面对长文本输入时，这通常对模型的解释性和处理复杂任务的能力有重大影响。\n",
      "\n",
      "论文的主题与大型语言模型的内部块（如Transformer架构中的多头注意力机制）和注意力结构有关，这些都是理解LLM内在机制的关键方面。通过改进这些核心组件，研究者可以提升模型的可解释性，使人们能够更好地理解模型是如何作出决策的，以及为什么在特定情况下表现出某种行为。此外，这样的改进也可能有助于开发出更易于理解和维护的模型，这对于实际应用中的广泛部署至关重要。\n",
      "\n",
      "总之，该论文的主题与大型语言模型的可解释性及其内部机制紧密相关，尤其是关注如何通过优化注意力机制来提高模型的效率和服务质量。这不仅对于理论研究具有重要意义，也为实际应用提供了改进模型性能和增强用户理解的途径。\n",
      "\n",
      "- Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《通过链式知识提示增强语言模型推理能力》表明了它探讨的是如何提升大型语言模型（Large Language Models, LLMs）的推理能力。在LLMs的领域中，提高可解释性和理解模型内部机制是一个关键的研究方向。论文提到的“链式知识提示”方法可能是为了展示或增强模型对特定任务的理解过程，比如通过引导模型访问或利用额外的知识源来改善其决策过程。这种方法有助于研究者和实践者更好地理解模型是如何做出决策的，以及它们在不同情境下的表现和局限性。\n",
      "\n",
      "通常，这样的工作会涉及分析模型的内部组件，如注意力机制（attention mechanisms）、模型的层次结构（hierarchical structure）等，以揭示模型处理信息的方式和其推理能力的来源。因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制相关，旨在帮助人们更好地理解这些模型的内部结构和工作原理。\n",
      "\n",
      "- Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目中提到“关注相关性的转移”以及“自由形式大型语言模型的预测不确定性量化”，这表明研究可能探讨了如何更深入地理解LLMs在处理不同任务时的决策过程，特别是它们在预测时的不确定性。这种不确定性量化对于评估模型的可靠性和可靠性至关重要，尤其是在应用到重要领域如医疗、法律等时。\n",
      "\n",
      "通常，大型语言模型通过注意力机制来分配对输入序列中不同元素的关注度，以决定哪些信息对生成的输出最重要。因此，关注相关性的转移可能涉及研究模型是如何根据输入内容调整其注意力分布的，以及这些变化如何影响模型的预测输出和不确定性。\n",
      "\n",
      "论文可能还探讨了如何通过量化预测不确定性来提高LLMs的透明度和可解释性，这对于用户和研究人员更好地理解模型的工作原理、识别潜在的偏差和错误至关重要。通过这种方式，研究有助于推动LLMs从黑箱模型向更加可解释和可控的模型发展，从而增强人类对这些复杂系统的理解和信任。\n",
      "\n",
      "总之，该论文聚焦于提升大型语言模型的可解释性，通过关注相关性的转移和预测不确定性量化，为理解LLMs的内部工作机理提供了新的视角。\n",
      "\n",
      "- Marathon: A Race Through the Realm of Long Context with Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文标题《Marathon: 长度为马拉松的长上下文中的大型语言模型》暗示了研究重点可能涉及大型语言模型（Large Language Models, LLM）在处理长文本或长上下文时的表现和能力。题目中“马拉松”一词通常用来形容长距离或长时间的活动，这里可能是用来比喻模型处理长文本的能力。因此，该论文似乎旨在探讨大型语言模型如何理解和生成长文本内容，这直接与LLM的可解释性和理解其内部机制有关。\n",
      "\n",
      "解释：在自然语言处理领域，大型语言模型的性能往往受到它们能够处理的信息量（即上下文长度）的影响。较长的上下文长度意味着模型可以考虑更多的输入信息来生成输出，这对于理解复杂文本、维持对话一致性或生成连贯的长文本至关重要。然而，增加上下文长度也带来了挑战，包括但不限于计算成本的增加、模型训练难度的提升以及解释模型决策过程的复杂性。因此，研究此类问题有助于提高我们对大型语言模型内部结构的理解，以及如何优化它们以更好地处理和解释长文本任务。\n",
      "\n",
      "- Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到“多模态关联”和“多模态序列学习”，这通常与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。尤其是，“注意力机制”（Attention Mechanism）是现代神经网络模型中用于处理序列数据的关键组件之一，在LLMs中尤其重要，因为它帮助模型在生成文本时考虑不同位置的单词之间的关系。\n",
      "\n",
      "论文题目中的“通用化注意结果对齐”（Generalizable Attentional Results Alignment）表明研究可能集中在如何通过调整或优化注意力机制来提高模型在处理不同模态数据（如文本、图像等）时的一致性和有效性。这种对齐可能是为了增强模型在跨模态任务（如文本到图像生成、问答系统等）中的表现，从而提供更深入的理解和解释LLMs在这些任务上的工作原理。\n",
      "\n",
      "因此，该论文似乎关注的是如何通过改进和理解LLMs中的注意力机制来提高它们在多模态学习任务中的性能和可解释性，这与LLMs的内部结构和解释相关。\n",
      "\n",
      "- What Do Language Models Hear? Probing for Auditory Representations in Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题和摘要表明它探讨了语言模型（Language Models）中的听觉表示（Auditory Representations）。虽然“听觉”可能在直觉上与大型语言模型（Large Language Models，LLMs）的文本处理功能相联系，但这里的“听觉”实际上指的是通过分析模型对声音或语音信号的反应来探查其内部表示和决策过程。这种研究方法通常涉及到将语言模型应用于处理音频数据，比如语音识别任务，或者通过改变输入的声音特征来观察模型输出的变化，以此来理解模型如何处理这些声音信息。\n",
      "\n",
      "这样的研究有助于提升我们对LLM内部工作机制的理解，特别是它们如何处理非文本形式的信息，以及这些处理方式与人类语言理解之间的相似性和差异性。通过这样的研究，可以为改进语言模型的设计、增强模型的可解释性，以及开发更通用的语言处理技术提供理论基础和实践指导。\n",
      "\n",
      "因此，这个论文与大型语言模型的可解释性或理解其内在机制直接相关，因为它提供了关于模型如何处理和理解非传统文本形式信息的洞察。\n",
      "\n",
      "- Jailbreak Open-Sourced Large Language Models via Enforced Decoding\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到几个关键概念与大型语言模型（Large Language Models, LLMs）的理解和解释相关性。具体来说，“Jailbreak”通常意味着绕过限制或保护措施来访问系统内部。在这个上下文中，它可能指的是通过某种方法解锁或揭露大型语言模型内部的工作机制，特别是通过强制解码（Enforced Decoding）技术。强制解码可能是指一种策略或过程，用于迫使模型以特定方式工作或揭示其决策过程中的某些细节。\n",
      "\n",
      "大型语言模型的可解释性和理解其内在机制是一个重要研究领域，因为这有助于提高我们对这些模型如何生成文本、理解和处理语言的理解，以及它们在不同任务上的决策过程。这种理解对于改进模型性能、确保其可靠性和安全性、以及开发新的应用（如教育、心理咨询等）都至关重要。\n",
      "\n",
      "因此，一个关于通过强制解码公开大型语言模型的研究，很可能会探讨如何提高LLMs的可解释性，提供对模型内部结构和工作原理的洞察，以及展示如何使用这些见解来增强模型的透明度和用户信任。这符合题目中提到的“解释LLM的内部结构和解释的”这一要求。\n",
      "\n",
      "- Digital Socrates: Evaluating LLMs through Explanation Critiques\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题中的 \"Digital Socrates\" 暗示了通过对话或交互的方式评估大型语言模型（Large Language Models, LLMs）。论文名称和主题表明，研究可能涉及利用解释方法来评估和理解大型语言模型的能力。这包括从模型的内部组件（如block、attention结构）出发，探讨模型如何处理信息、生成文本以及决策过程。通过“Explanation Critiques”，研究者可能旨在提供一种方法论，帮助人们更好地理解大型语言模型的内部工作原理，揭示其决策背后的逻辑，以及评估其在不同任务上的表现和局限性。因此，该论文摘要与大型语言模型的可解释性和理解其内在机制密切相关。\n",
      "\n",
      "- Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题中明确提到了“Language-Specific Neurons”，这表明研究内容可能涉及在大型语言模型（Large Language Models, LLMs）内部寻找与特定语言相关的神经元，以揭示其在多语言能力方面的关键机制。这一主题直接与大型语言模型的可解释性和理解其内部机制相关，特别是如何从模型的内部组件（如特定的神经元、block或attention结构）来解释模型的能力和行为。通过关注这些语言特定的神经元，研究可以帮助人们更好地理解大型语言模型是如何处理不同语言的输入和输出的，从而提高我们对LLMs内在工作原理的理解。\n",
      "\n",
      "- Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这篇论文标题“Masked Thought”表明它探讨了通过简单地隐藏部分推理步骤来提高语言模型在数学推理学习中的能力。这实际上涉及到了对大型语言模型（Large Language Models, LLMs）内部机制的理解和解释。在深度学习领域，特别是自然语言处理中，语言模型通常会通过复杂的内部结构（如多层神经网络、注意力机制等）进行信息处理和生成任务。将部分推理步骤“掩码化”（masking），即隐藏某些推理过程，是一种探索模型如何理解和处理信息的方法，有助于揭示模型在处理特定任务时的内在逻辑和决策过程。这种方法可以帮助研究者更好地理解模型的工作原理，包括其内部的block和attention结构如何协同作用以完成任务，从而提高模型的学习效率和性能。因此，这篇论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题“SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning”（SEER：通过强化学习促进结构化推理和解释）表明其主要关注于如何通过强化学习（Reinforcement Learning, RL）来促进大型语言模型（Large Language Models, LLMs）的结构化推理和解释。这直接关联到LLMs的可解释性和理解其内部机制，尤其是如何通过RL优化模型的决策过程，使得模型的推理步骤和最终结果能够被更好地理解和解释。因此，此论文与LLMs的可解释性和理解其内在机制相关。\n",
      "\n",
      "解读：在深度学习领域，特别是自然语言处理任务中，大型语言模型常常因其强大的生成能力和复杂的内部结构而受到关注。然而，这些模型通常被认为是“黑箱”，即缺乏透明度和可解释性，使得人们难以理解它们是如何做出特定决策的。通过引入强化学习等方法，研究者试图增强模型的决策过程的可解释性，使其更易于理解和分析。SEER论文正是在这一背景下进行的研究，它尝试利用强化学习来提升大型语言模型的结构化推理能力，并提供一种机制来解释模型的决策过程。这样的工作对于提高我们对LLMs的理解、改进模型设计以及增强模型在实际应用中的信任度具有重要意义。\n",
      "\n",
      "- TasTe: Teaching Large Language Models to Translate through Self-Reflection\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文的标题“TasTe: 教育大型语言模型通过自我反思进行翻译”表明它探讨了如何通过某种形式的教学方法来提升大型语言模型（Large Language Models, LLMs）的翻译能力。在这个过程中，“自我反思”这一概念暗示了作者可能试图揭示或增强LLMs在处理语言任务时的内部决策过程和机制。通过自我反思，模型可以更好地理解其生成输出的逻辑和上下文，从而提高翻译质量。这与大型语言模型的可解释性、理解其内在机制以及帮助人类理解LLMs的内部结构和工作原理相关联。因此，这篇论文很可能深入探讨了LLMs的内部结构、工作原理以及如何通过特定训练策略来改善它们的理解和解释能力。\n",
      "\n",
      "- Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《并非所有专家都是平等的：混合专家大型语言模型中的高效专家修剪和跳过》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内部机制相关。题目中提到的“专家”在深度学习领域通常指的是在特定任务或数据子集上有专门知识的模型部分，而“混合专家”模型则是将多个这样的专家组合在一起，以实现更强大的泛化能力和解决复杂任务的能力。通过修剪和跳过不重要的专家，可以提高模型的效率和性能。\n",
      "\n",
      "论文可能探讨了如何通过分析和理解这些专家的贡献来优化大型语言模型的结构，从而提高模型的效率和对某些任务的处理能力。这种研究有助于深入理解大型语言模型的工作原理，特别是它们如何在处理文本生成、问答、翻译等任务时作出决策。通过提供对模型内部机制的洞察，该研究能够帮助研究人员和开发者更好地设计和优化此类模型，使其更加高效且易于理解。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制相关，因为它涉及到了通过分析和优化模型组件来改进模型性能的方法，这正是理解LLM内部结构和解释其工作方式的关键步骤。\n",
      "\n",
      "- Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内部机制相关。具体来说，它探讨了当知识冲突时，语言模型如何融合生成的上下文与检索到的上下文的问题。这涉及到对LLMs内部结构的理解，特别是它们如何处理不同来源的信息以形成一致的输出。研究可能关注的是LLMs中的特定组件（如内部块、注意力机制等），以及这些组件在处理知识冲突时的交互方式。因此，这篇论文旨在增进我们对LLMs内部工作原理的理解，尤其是它们在决策过程中如何整合不同类型信息的能力，这对于开发更可解释和可靠的AI系统至关重要。\n",
      "\n",
      "- Unveiling Linguistic Regions in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目“Unveiling Linguistic Regions in Large Language Models”直译为“揭示大型语言模型中的语言区域”，这表明作者试图探索和展示LLMs中特定的语言结构或区域。这样的研究通常涉及到对LLMs内部工作机制的理解，比如通过分析模型的不同组件（blocks）、注意力机制（attention structures）等，来解释模型如何处理和生成语言。这种研究有助于人们更好地理解LLMs是如何工作以及它们在不同任务上的表现背后的原理，从而提高我们对这些复杂模型的可解释性和透明度。\n",
      "\n",
      "- Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解LLM内在机制相关。具体来说，它探讨了语言模型对话语关系的理解忠实度（faithfulness）。在自然语言处理领域，语言模型通常被用来生成文本、回答问题或是进行对话等任务。然而，由于这些模型通常是基于大量数据训练的复杂神经网络，它们的决策过程往往缺乏透明度，即我们很难理解模型是如何得出特定输出的。\n",
      "\n",
      "“Discursive Socratic Questioning”方法通过提出一系列问题，旨在评估模型在处理不同话语关系时的准确性和一致性。这种评估方式可以帮助研究者和开发者更好地理解模型在处理特定类型语料时的性能，以及它对话语规则和逻辑的掌握程度。通过这种方式，可以增加模型的可解释性，使得人们能够更深入地理解模型的内部结构和决策过程，这对于改进模型、提高其性能以及在实际应用中安全、可靠地使用这些模型至关重要。\n",
      "\n",
      "因此，这个论文题目和内容与探索和解释大型语言模型的内部结构和机制紧密相关，特别是关注于提升模型在理解和生成符合特定语法规则和逻辑关系文本方面的准确性。\n",
      "\n",
      "- Unlearning Traces the Influential Training Data of Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它探讨的是语言模型中\"遗忘\"训练数据的影响。在大型语言模型（Large Language Models, LLMs）的背景下，这涉及到模型如何学习和处理信息，以及它如何“忘记”或不再依赖于某些特定的训练数据。这种研究对于理解LLMs的可解释性至关重要，因为它揭示了模型决策背后的机制。通过追踪模型对训练数据的依赖，研究人员可以提供关于模型内部工作原理的见解，包括模型的注意力机制、内部块（如Transformer中的多头自注意力层）等关键组件如何影响模型的表现和决策过程。这样的分析有助于提升我们对LLMs复杂内部结构的理解，从而增强其可解释性和可控性。\n",
      "\n",
      "- Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。具体来说，论文探讨了输入长度对大型语言模型推理性能的影响，这涉及到对模型内部结构的理解和解释。在深度学习领域，特别是对于大规模语言模型，理解模型如何处理不同长度的输入以及这些输入如何影响模型的输出是一个关键问题。通过研究输入长度如何影响模型的推理性能，研究者可以提供关于模型内部机制的见解，比如模型在处理不同长度的输入时是如何分配注意力的，以及在哪些部分发生了信息丢失或增强。这种分析有助于提高我们对大型语言模型工作原理的理解，从而促进更有效的模型设计和应用。\n",
      "\n",
      "- LLaMA Pro: Progressive LLaMA with Block Expansion\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题 \"LLaMA Pro: Progressive LLaMA with Block Expansion\" 提示了这篇论文可能探讨的是如何通过逐步扩展块（Block Expansion）来改进 LLaMA 模型。由于大型语言模型（Large Language Models，简称LLMs）的可解释性和理解其内部机制是一个研究热点，尤其是因为这些模型通常设计复杂，包含多层结构和注意力机制等组件，使得它们的决策过程难以直接理解和解释。因此，如果这篇论文关注于分析、改进或增强LLM的内部结构（如 block 和 attention 结构），并提供了对这些结构如何影响模型性能的洞察，那么它确实与大型语言模型的可解释性和理解其内在机制有关。\n",
      "\n",
      "简而言之，如果论文内容涉及对 LLaMA 或类似模型内部组件的详细分析、如何通过调整这些组件（比如通过扩展块）来改善模型性能，以及这些调整如何影响模型的可解释性或我们对模型内部工作的理解，那么它就符合题干中提到的要求。因此，根据这个标题，我们可以合理推测这篇论文与大型语言模型的可解释性或理解其内在机制相关。\n",
      "\n",
      "- The Hidden Space of Transformer Language Adapters\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《Transformer语言适配器的隐藏空间》表明它探讨的是Transformer模型（一种大型语言模型）的内部结构和特定组件（在这里是“适配器”）的作用。Transformer模型是基于注意力机制构建的，通过多个编码器和解码器层来处理序列数据。适配器则是一种在这些层之上添加的轻量级结构，旨在增强模型对特定任务的学习能力，而无需重新训练整个模型。\n",
      "\n",
      "论文可能深入研究了以下几个方面：\n",
      "1. **Transformer内部机制**：解释Transformer如何通过注意力机制处理输入序列，以及每个层如何共同作用以生成最终的输出。\n",
      "2. **适配器的作用**：探讨适配器是如何被设计和插入到Transformer架构中，以及它们如何能够提高模型在特定任务上的表现而不增加过多的参数。\n",
      "3. **隐藏空间的分析**：分析适配器如何影响Transformer的“隐藏空间”，即模型内部表示学习的空间。这涉及到理解模型如何根据输入生成输出，以及这种生成过程中的非线性变换如何被适配器所影响。\n",
      "4. **可解释性和理解机制**：提供关于如何通过适配器和隐藏空间来更好地理解和解释Transformer模型内部工作原理的方法。这可能包括可视化技术、特征重要性分析等手段，以揭示模型决策背后的原因。\n",
      "\n",
      "因此，这篇论文与大型语言模型的可解释性或理解其内在机制直接相关，尤其是从Transformer的内部组件和结构角度出发。\n",
      "\n",
      "- A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制有关。论文标题中的 \"Locating and Detecting Language Model Grounding\" 暗示了研究可能涉及识别和检测语言模型在现实世界中的对应物或基础知识，这通常涉及到模型的可解释性问题。通过使用假维基百科（Fakepedia）这一工具，作者可能试图探索和揭示模型是如何获取、处理和生成文本信息的，以及这些过程背后的机制。这样的研究有助于增进人类对LLMs内部工作原理的理解，包括它们如何处理语言输入、执行任务以及生成输出文本的方式。因此，这个论文关注点与大型语言模型的内部结构、解释性和理解机制直接相关。\n",
      "\n",
      "- ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。标题“ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs”表明该研究旨在通过让不同的大型语言模型在圆桌会议上进行讨论并达成共识来提高推理能力。这涉及到对LLMs内部结构的理解以及如何通过这些模型之间的交互来解释它们的工作原理。研究可能探索了LLMs的不同组件（如内部block和attention结构）如何协同工作以实现特定任务，并且可能提出了一种方法来增强这些模型的决策过程的透明度和可解释性。因此，它与试图帮助人类理解LLMs内部工作机理的目标保持一致。\n",
      "\n",
      "- Large Language Models Are No Longer Shallow Parsers\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“大型语言模型不再是浅层解析器”表明它探讨了大型语言模型（Large Language Models，LLMs）在处理语言任务时的深度和复杂性，这与LLM的可解释性和理解其内在机制紧密相关。大型语言模型通常被认为是基于Transformer架构的，它们通过多层编码器和解码器来处理输入序列，每一层都包含了注意力机制（attention），使得模型能够关注输入序列的不同部分以生成输出。这种深层结构和注意力机制是LLM能够理解上下文、生成连贯文本的关键所在。\n",
      "\n",
      "论文标题中的“不再是浅层解析器”暗示了LLMs已经超越了简单的词法分析阶段，能够进行更深层次的语义理解和生成复杂的语言结构。这意味着研究可能深入探讨了LLM内部的结构和工作原理，比如不同层之间的信息传递、注意力机制如何增强模型的理解能力等，这些都是解释LLM内在机制的重要方面。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制高度相关，它旨在帮助人们更好地理解这些模型是如何构建和工作的。\n",
      "\n",
      "- An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，摘要提到使用神经元激活作为统一视角来解释大型语言模型在解决连贯思考引导算术推理任务时的运作过程。这涉及到对LLMs内部结构的理解，特别是如何通过分析神经元激活模式来解释模型在处理特定任务时的决策过程。这种研究方法有助于人类更好地理解LLMs是如何处理信息、进行推理以及做出预测的，从而提升我们对这些模型内部工作机制的认知。因此，该论文聚焦于LLMs的可解释性，与主题相关。\n",
      "\n",
      "- Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要提到的研究主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，研究关注的是大型语言模型是否能够使用规则进行推理（Reasoning with Rules），并通过逻辑支架（Logic Scaffolding）来进行压力测试（Stress-Testing）和改进（Improving）这些模型。这涉及到深入理解LLMs如何处理逻辑结构和规则，以及它们在执行任务时的决策过程。因此，这项研究旨在帮助人们更好地理解大型语言模型的内部结构和工作原理，提高它们的性能和可靠性。通过分析模型对逻辑规则的处理能力，研究者可以提供更深入的见解，帮助提升LLMs在实际应用中的表现，同时也增强我们对这些复杂模型的理解。\n",
      "\n",
      "- EIT: Enhanced Interactive Transformer\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题“EIT: Enhanced Interactive Transformer”表明该研究可能关注于提升交互式Transformer模型的性能或特性。考虑到大型语言模型（Large Language Models, LLMs）通常基于Transformer架构构建，此论文摘要与大型语言模型的可解释性或理解其内部机制相关。特别是，如果该研究探讨了从Transformer的内部组件（如块、注意力机制）来解释LLMs的能力，或者旨在改进这些模型以增强人类对其决策过程的理解，那么它确实与大型语言模型的内部结构和解释相关。通过提升模型的交互性，研究者可能试图提供更直观的解释，使人们能够更好地理解模型如何处理和生成文本，以及在不同情境下的决策逻辑。\n",
      "\n",
      "- MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题\"MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\"（MARS：基于意义的响应评分用于生成式LLM的不确定性估计）表明它关注于如何通过评估生成文本的意义来估计大型语言模型（LLMs）的不确定性。这意味着该论文涉及了对LLM内部结构的理解和解释，特别是从生成文本的质量和意义角度出发，探讨了模型在不同情况下的表现和不确定性的评估方法。\n",
      "\n",
      "具体来说，论文可能探索了以下几个方面：\n",
      "1. **内部Block和Attention结构**：通过分析模型的内部组件（如Transformer架构中的多头注意力机制、前馈网络等），了解这些结构如何影响生成文本的意义和不确定性。\n",
      "2. **意义感知的响应评分**：开发了一种评分机制，能够根据生成文本的意义来评估模型的置信度或不确定性。这涉及到理解模型如何处理不同词汇和语法结构，并如何在不同的上下文中产生意义。\n",
      "3. **解释LLM的内部结构**：通过研究上述评分机制及其与模型内部结构的关系，论文可能提供了一些关于LLM如何工作以及它们在处理不同类型任务时表现差异的见解。\n",
      "\n",
      "总之，该论文与大型语言模型的可解释性和理解其内部机制直接相关，因为它不仅关注于提高模型的性能，还深入探讨了模型的决策过程和不确定性评估，这对于促进对LLM内在机制的理解至关重要。\n",
      "\n",
      "- Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment\n",
      "  回复: 是。\n",
      "\n",
      "这篇论文的标题《大型语言模型是所有字符的叠加：通过自我对齐实现任意角色扮演》表明它探讨了大型语言模型（Large Language Models, LLMs）在角色扮演方面的特性，这涉及到LLM内部结构和工作机制的理解。具体来说，论文关注的是LLMs如何能够以任意方式执行角色扮演任务，这通常需要模型理解上下文、生成符合特定角色的语言输出。这种能力的实现往往与模型内部的注意力机制（attention）、多层结构等关键组件紧密相关。因此，论文内容与大型语言模型的可解释性以及理解其内在机制相关，特别是从内部结构和工作原理的角度来解释模型的能力。\n",
      "\n",
      "解释：\n",
      "- **角色扮演**：在自然语言处理中，角色扮演通常指的是模型能够根据给定的角色和场景生成相应的话语或文本，这要求模型具有强大的语义理解和生成能力。\n",
      "- **自我对齐**：这一概念可能指的是模型内部如何通过某种方式调整或优化其参数，使得在面对不同任务或角色时，能够有效地整合和利用其内部知识和能力。这可能涉及到注意力机制的调整、模型内部表示的重新组织等。\n",
      "- **大型语言模型的内部结构**：大型语言模型通常由多层神经网络组成，每一层都可能包含多种机制，如注意力机制，用于捕捉输入序列之间的依赖关系。理解这些内部结构对于解释模型的行为至关重要。\n",
      "- **可解释性**：对于研究领域而言，提高模型的可解释性意味着提供方法和工具，帮助研究人员和用户理解模型是如何做出决策的，这对于改进模型性能、确保其可靠性和公平性非常重要。\n",
      "\n",
      "综上所述，该论文的主题与大型语言模型的内部结构、解释性以及它们在复杂任务如角色扮演中的表现密切相关，因此是与LLM的可解释性和理解其内在机制相关的。\n",
      "\n",
      "- Calibrating Large Language Models Using Their Generations Only\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到使用大型语言模型（Large Language Models, LLMs）的生成内容来校准它们。这实际上是在探讨如何通过分析LLMs的内部工作原理和生成过程来提高其性能和可解释性。具体来说，研究可能关注于以下几个方面：\n",
      "\n",
      "1. **内部结构理解**：通过观察和分析模型在生成特定文本时的行为，可以更好地理解模型内部的block、注意力机制等关键组件是如何协同工作的。这有助于揭示模型在不同任务上的决策过程和偏好，从而提供对模型内部结构的洞察。\n",
      "\n",
      "2. **解释能力提升**：论文可能探讨了如何利用生成的内容来构建更有效的解释工具，使得模型的决策过程能够被人类理解。例如，通过分析模型在生成特定文本时所依赖的输入特征或注意力模式，可以为模型的决策提供一种“透明度”，使人们能够理解模型是如何做出某些预测或生成特定输出的。\n",
      "\n",
      "3. **参数优化**：基于对生成内容的分析，研究者可能会提出策略来调整模型的参数，以改善模型的表现和泛化能力。这种基于生成内容的校准方法可以帮助模型更准确地适应特定任务需求，同时可能也提供了对其内部机制更深入的理解。\n",
      "\n",
      "4. **模型可靠性评估**：论文还可能关注如何通过生成内容来评估模型的可靠性或不确定性，这对于理解模型在不同情境下的表现以及潜在的偏差或局限性至关重要。\n",
      "\n",
      "综上所述，这篇论文的主题确实与大型语言模型的可解释性和理解其内在机制紧密相关，特别是从LLM的内部block、attention结构的角度出发，旨在提高模型的性能并增强其对人类的可解释性。\n",
      "\n",
      "- ANAH: Analytical Annotation of Hallucinations in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与Large language models（大语言模型）的可解释性或理解其内在机制相关。论文标题“ANAH: Analytical Annotation of Hallucinations in Large Language Models”表明，它探讨了在大型语言模型中幻觉的分析注释。这涉及到对大型语言模型内部结构的理解以及解释它们的工作方式，特别是在生成错误输出或“幻觉”时。\n",
      "\n",
      "大型语言模型通常基于深度学习技术，具有复杂的多层结构，包括注意力机制等高级组件。理解这些模型如何生成文本，以及它们在哪些情况下可能产生错误或不准确的输出，对于提高模型性能和应用安全性至关重要。通过分析模型在生成特定文本时的行为，研究人员可以揭示模型的决策过程，增强我们对模型内在机制的理解。\n",
      "\n",
      "因此，该论文的主题与大型语言模型的可解释性和理解其内部工作原理直接相关。通过研究模型在生成幻觉时的行为，可以提供有关模型内部结构和功能的重要见解，从而帮助人们更好地理解和控制这些模型。\n",
      "\n",
      "- Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“揭示语言模型的参数知识：一种统一的归因方法框架”表明它探讨了语言模型（Large Language Models, LLMs）的参数知识，这直接关联到LLMs的可解释性和理解其内在机制。通过提出一种统一的归因方法框架，作者旨在提供一种工具或方法来解释LLMs的工作原理，特别是如何处理和生成文本。这种研究对于帮助人们理解LLMs的内部结构和解释其决策过程非常重要，因为当前的LLMs通常被认为是“黑盒”模型，其内部工作原理对人类来说是不透明的。因此，这个论文与LLMs的可解释性相关，有助于增进我们对这些强大模型的理解。\n",
      "\n",
      "- Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这篇论文的标题《长上下文其实并不长：大型语言模型中的长期依赖数据勘探》表明它探讨了大型语言模型（Large Language Models, LLMs）处理长依赖关系的能力。在深度学习领域，特别是自然语言处理中，大型语言模型通常需要理解和生成包含多个时间步或句子长度的文本序列。然而，处理长距离依赖（即在序列中相距较远的元素之间的关系）是这些模型面临的一个挑战。\n",
      "\n",
      "论文标题中提到的“长上下文”可能指的是模型能够处理的输入序列长度。通常，大型语言模型可以处理的上下文长度有限，这限制了它们捕获长距离依赖关系的能力。然而，通过数据增强、特定的架构设计或其他技术，研究人员可能已经发现方法来提高模型处理长距离依赖关系的效率或效果。\n",
      "\n",
      "因此，这篇论文似乎关注的是如何改善大型语言模型对长依赖关系的理解和处理能力，这与大型语言模型的可解释性或对其内部机制的理解直接相关。它可能探索了模型内部的结构组件（如block、attention机制等），以解释模型如何在处理长序列时表现出其能力。这种研究对于帮助人们更好地理解大型语言模型的工作原理以及提升其性能具有重要意义。\n",
      "\n",
      "- Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明了它关注于通过让Transformer模型关注结构化量化嵌入来诱导系统性。Transformer是一种大型语言模型（Large Language Models，LLMs）的核心架构，它们在自然语言处理任务中表现出了强大的性能。论文的主题直接涉及到了LLM内部机制的理解和解释，特别是通过分析模型如何处理和利用特定类型的嵌入（即结构化量化嵌入）来增强其系统性（即模型能够泛化到未见过的数据的能力）。这种研究有助于人们更深入地理解LLM的工作原理，以及它们如何通过特定的设计决策来提升性能和泛化能力。因此，这个摘要与大型语言模型的可解释性和理解LLM内在机制密切相关。\n",
      "\n",
      "- Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“机制的竞争：追踪语言模型如何处理事实和反事实”似乎与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。标题中提到了“语言模型如何处理事实和反事实”，这表明研究可能探讨了LLMs在处理真实情况信息（事实）和假设情况信息（反事实）时的行为模式或决策过程。这种研究通常涉及到对模型内部结构、工作原理以及它们在特定任务上的表现进行深入分析。\n",
      "\n",
      "具体来说，论文可能会从LLM的内部组件（如块、注意力机制等）出发，探索这些组件如何协同作用来处理不同类型的输入，特别是在理解和生成事实和反事实内容时的表现。通过这样的研究，可以帮助人们更好地理解LLMs的内部结构和工作原理，增强对模型决策过程的透明度和可解释性。因此，该论文摘要与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题 \"OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models\" 直接指向了大型语言模型（Large Language Models, LLMs）的理论思维推理能力评估。理论思维（Theory-of-Mind, ToM）是一种认知能力，指的是个体理解他人观点、意图、信念以及情感的能力。在人工智能领域，特别是自然语言处理中，评估大型语言模型是否具备理论思维能力是一个重要的研究方向，因为它涉及到模型如何理解和生成具有复杂语境、意图和情感色彩的对话或文本。\n",
      "\n",
      "该论文旨在创建一个全面的基准测试，用于评估大型语言模型在理论思维推理方面的表现。这表明它关注于大型语言模型的可解释性和理解其内在机制。通过这样的评估，可以深入理解模型是如何处理和生成包含理论思维元素的内容的，以及它们在不同任务上的表现差异。这有助于研究人员和开发者更好地理解大型语言模型的内部结构、注意力机制（attention structures）和其他关键组件如何共同作用以支持这些高级推理能力。\n",
      "\n",
      "因此，该论文与大型语言模型的可解释性或理解其内在机制直接相关，提供了评估和分析模型在理解复杂人类行为和思想方面能力的方法，同时也为提升模型性能和优化其设计提供了有价值的洞见。\n",
      "\n",
      "- Steering Llama 2 via Contrastive Activation Addition\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题 \"Steering Llama 2 via Contrastive Activation Addition\" 提示了该研究可能涉及如何通过对比激活添加（contrastive activation addition）来引导或控制大型语言模型（LLM）的行为或性能。这暗示了研究者对 LLM 的内部工作原理有深入探索，尤其是关注其激活（activation）过程——这是神经网络中神经元响应输入信号后产生的输出值，在 LLM 中扮演着关键角色。\n",
      "\n",
      "解释 LLMS 内部结构和机制通常涉及理解它们是如何处理输入信息、执行语言生成任务以及在训练过程中学习到特定模式或概念的。通过对比激活添加方法，研究者可能旨在揭示 LLM 在不同任务或情境下激活模式的变化规律，从而提供一种手段来更精细地控制模型的行为。这种方法可能涉及到对 LLM 内部组件（如 block 或 attention 结构）的分析，以理解它们如何影响模型的决策过程和最终输出。\n",
      "\n",
      "因此，这个摘要与大型语言模型的可解释性和理解其内在机制相关，特别是从 LLM 的内部组件（如 block 和 attention 结构）的角度出发，探讨如何增强我们对这些模型的理解和控制能力。\n",
      "\n",
      "- RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题\"RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\"表明其主要关注于评估解释性方法在分离语言模型表示方面的应用。这直接涉及到对大型语言模型（Large Language Models, LLMs）的可解释性问题，即如何理解和解释LLMs的内部工作原理和决策过程。通过研究如何分解（disentangle）LLMs的表示，该论文旨在帮助人们更好地理解这些复杂模型的内部结构和决策机制，从而提高我们对LLMs内在机制的理解。\n",
      "\n",
      "解读：论文标题中的“Interpretability”一词明确指出了其内容与LLMs的可解释性有关，而“Disentangling Language Model Representations”则进一步说明了研究将聚焦于LLMs表示的分解，这正是探索LLMs内部结构和解释其工作原理的关键步骤。因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- I am a Strange Dataset: Metalinguistic Tests for Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《我是一个奇怪的数据集：语言模型的元语言测试》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目中提到的“元语言测试”（Metalinguistic Tests）通常涉及到对模型在理解和使用语言规则、语法、修辞等方面的能力进行评估。通过设计特定的数据集或任务来测试模型的这些元语言能力，研究者可以更好地理解模型是如何处理语言的复杂性、语境信息以及不同语言现象的。\n",
      "\n",
      "大型语言模型的内部结构通常包括多层的神经网络结构，如Transformer模型中的自注意力机制（Attention Mechanism）。这些模型通过学习大量文本数据，能够生成连贯的文本并解决各种自然语言处理任务。然而，由于它们的复杂性和“黑盒”性质，理解模型是如何做出特定决策或生成特定输出的往往具有挑战性。因此，开发方法来解释模型的行为、决策过程以及它们如何处理不同类型的输入变得尤为重要。\n",
      "\n",
      "元语言测试可以提供一种途径，通过评估模型在处理特定语言任务时的表现，来间接了解其内部机制。例如，测试模型在生成遵循特定语法规则的句子、识别或创造复杂的语言结构等方面的能力，可以帮助研究者洞察模型对于语言规则的理解深度，以及它如何利用上下文信息进行推断。这种测试结果不仅有助于提高我们对LLMs的理解，还能为改进模型的可解释性和设计更高效、更可靠的自然语言处理系统提供指导。\n",
      "\n",
      "- Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。标题中提到的“Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models”涉及到LLMs在知识编辑过程中利用多跳事实捷径的能力。这实际上是在探讨如何通过修改或编辑LLM的知识库来增强它们在处理复杂问题时的性能和效率。重点在于理解LLM如何基于先前学习到的信息和模式进行推理，以及如何通过引入新的事实信息来优化其决策过程。这种研究有助于深入理解LLM的内部结构和工作原理，特别是它们在进行多步推理时是如何构建和利用知识的。因此，这个题目直接关联到了LLM的可解释性和对其内在机制的理解。\n",
      "\n",
      "- Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题“Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations”表明该研究旨在通过数学操作（Math-Shepherd）来验证和强化大型语言模型（LLMs）的性能，而无需人工注释。这直接涉及到对LLMs内部结构和机制的理解以及可解释性。通过逐步（Step-by-step）的方法，该研究可能探索了LLMs处理数学问题的能力，从而提供了对这些模型如何进行计算、推理和决策过程的洞察。此外，“without Human Annotations”强调了自动化验证和增强方法，这通常意味着对模型内部逻辑和决策过程的深入分析，以提高模型的可靠性和透明度。因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关，特别是从模型的内部组件、注意力机制等方面进行探讨，以帮助人们更好地理解LLMs的工作原理。\n",
      "\n",
      "- Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“通过自我奖励对比提示精炼实现大型语言模型的直接对齐”表明它探讨了如何通过特定的技术方法（自我奖励对比提示精炼）来增强大型语言模型（LLM）的性能和可解释性。论文的重点似乎是将这些模型的输出与某个目标或期望的输出进行对齐，这通常涉及到调整模型参数以提高其在特定任务上的表现。在这一过程中，了解和解释模型内部的决策过程变得尤为重要，因为这不仅有助于提升模型的性能，还能增加人们对模型工作原理的理解。\n",
      "\n",
      "论文中的“自奖励对比提示精炼”这一技术可能涉及到了对模型内部组件（如block、attention结构等）的操作，以优化其在给定任务上的表现。这种技术可能包括调整模型的注意力机制、改变其内部层的权重，或者在训练过程中引入特定的激励信号，以便让模型更好地理解和生成符合预期的语言输出。\n",
      "\n",
      "因此，这个论文与大型语言模型的可解释性和理解其内在机制相关，因为它探索了如何通过具体的技术手段改进模型性能的同时，也提供了对模型内部结构和决策过程的洞察。\n",
      "\n",
      "- DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题和摘要表明了研究者试图通过增加某些元素（Augmenting）来增强大型语言模型（LLM）在逻辑推理任务上的表现，特别是从不确定性（Indeterminacy）提升到确定性（Determinacy）。这暗示了研究可能涉及到对LLM内部结构和机制的理解，以及如何提高这些模型在特定任务（如逻辑推理）上的可解释性和性能。\n",
      "\n",
      "通常，提高LLM在特定任务上的性能需要深入理解它们的内部工作原理，包括但不限于其架构（如Transformer block）、注意力机制（Attention structure）以及如何处理不确定性和逻辑推理等问题。因此，如果该论文探讨了这些方面，它确实与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "具体来说，研究可能探索了如何设计或调整模型组件（如引入特定的编码器或解码器块、改变注意力权重计算方式等），以使模型在面对逻辑推理任务时能够更准确地从不确定性的输入中推导出确定性的结论。这种工作不仅有助于提高模型在实际应用中的表现，而且还能增进我们对LLM如何进行推理、如何处理不同类型的不确定性以及如何构建更强大和可解释的AI系统的理解。\n",
      "\n",
      "- Meta-Task Prompting Elicits Embeddings from Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，题目中提到了“Meta-Task Prompting”（元任务提示），这可能是一种策略或技术，用于从大型语言模型中提取或引发特定类型的表示（Embeddings）。Embeddings在自然语言处理中通常指的是将文本或词汇转换为数值向量的过程，这些向量能够捕捉到语义和上下文信息。这种技术对于理解LLMs如何处理和表示语言数据至关重要。\n",
      "\n",
      "此外，“Large Language Models”部分明确指出了研究对象的类型，即大型语言模型，这是当前自然语言处理领域关注的核心。大型语言模型通常具有复杂的内部结构，包括多层的Transformer block、注意力机制等，这些组件共同决定了模型的性能和能力。\n",
      "\n",
      "因此，该论文摘要与理解LLMs的内部结构和解释其工作原理相关，因为它探讨了如何从这些模型中提取有意义的信息，这有助于深入理解模型是如何学习和应用语言知识的。\n",
      "\n",
      "- Do Large Language Models Latently Perform Multi-Hop Reasoning?\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题“大型语言模型在潜意识中执行多跳推理？”（Do Large Language Models Latently Perform Multi-Hop Reasoning?）直接指向了大型语言模型（Large Language Models, LLMs）的内部工作机理和它们如何处理复杂任务的能力。这个题目暗示了研究者试图探索和解释LLMs在执行需要多个步骤或跳跃的推理任务时的内在机制。这涉及到对LLMs内部组件如块（blocks）、注意力（attention）结构等的分析，以理解它们如何支持和实现这种多跳推理能力。因此，这篇论文的相关部分很可能集中在解释LLMs的内部结构、它们如何处理信息以及这些处理方式如何支持多步推理过程，这些都是理解LLMs内部机制的关键方面。\n",
      "\n",
      "- MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：这个论文标题“MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models”表明它探讨的是如何通过知识图谱提示激发大型语言模型中的思维图谱。这涉及到对大型语言模型（Large Language Models, LLMs）内部结构的理解以及它们在处理任务时的可解释性。知识图谱提示是一种策略，通过这种方式可以引导LLMs在生成文本时使用特定的知识或概念，从而提高输出的质量和相关性。这种研究有助于人们更好地理解LLMs是如何处理信息的，以及它们在不同情境下表现出的能力和局限性。因此，这个论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。标题“Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning”表明，该研究关注的是通过递归调用来克服大型语言模型在组合性方面的限制。这意味着作者试图探讨和改进LLMs如何处理复杂语言结构的能力，这通常涉及到对模型内部组件、注意力机制等关键元素的理解和优化。通过递归调用的方法，研究可能旨在深入理解模型是如何学习和表示语言的层次结构的，以及如何通过调整这些结构来提高模型的性能和可解释性。因此，该论文直接与大型语言模型的内部结构、工作原理及其可解释性有关。\n",
      "\n",
      "- HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题“HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts”（HyperMoE：通过专家之间的转移以实现更好的专家混合）涉及了大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制的主题。论文探讨的是如何通过在专家之间进行知识转移来改进混合专家（Mixture of Experts, MoE）模型的性能。专家模型是一种并行计算架构，其中多个子模型（即“专家”）独立处理数据的不同部分，然后将结果聚合起来形成最终输出。这种架构可以提高模型的效率和准确性，尤其是在处理大规模数据集时。\n",
      "\n",
      "在LLMs的背景下，理解模型的内部结构和决策过程对于提升模型性能、优化参数分配以及增强模型的可解释性至关重要。通过研究如何在专家之间有效地转移知识，HyperMoE论文为深入理解LLMs的内部工作原理提供了新的视角。这种知识转移策略有助于揭示模型如何处理不同的输入特征，从而增强我们对LLMs内在机制的理解。此外，通过提高模型的性能，该研究也间接促进了对LLMs可解释性的提升，因为更高效的模型通常更容易理解和分析。\n",
      "\n",
      "综上所述，HyperMoE论文与大型语言模型的可解释性或理解其内在机制紧密相关，因为它探索了改进模型性能的方法，这一过程涉及到对模型结构和决策过程的深入理解。\n",
      "\n",
      "- Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这个论文标题“再问一次，然后失败：大型语言模型在判断上的摇摆不定”涉及到大型语言模型（Large Language Models, LLMs）在决策或判断时的不确定性或不稳定性的研究。这种研究通常会深入探索LLMs的内部结构和工作原理，以理解它们如何处理信息、做出决策以及在特定情况下表现出来的行为模式。特别是，它可能关注于LLMs的内部块（如Transformer层）、注意力机制（Attention）以及其他关键组件如何影响其性能和输出的可预测性。\n",
      "\n",
      "在这个上下文中，“可解释性”通常指的是研究者试图提高模型决策过程的透明度，使得人们能够理解模型是如何从输入生成输出的。对于LLMs来说，这可能包括分析它们在遇到不同输入时的行为变化、识别哪些部分的模型结构对最终输出有显著影响、以及探讨注意力机制如何引导模型关注输入中的关键元素等。\n",
      "\n",
      "因此，这个论文标题表明了对LLMs内部工作机制及其决策过程不确定性的研究兴趣，这正是与LLMs的可解释性和理解其内在机制相关的领域。通过这样的研究，可以增进人类对这些复杂模型的理解，从而提升我们利用它们的能力并解决可能出现的伦理、安全等问题。\n",
      "\n",
      "- CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题“CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models”表明，该研究关注于大型语言模型（Large Language Models, LLMs）在处理模糊信息需求时的能力识别和澄清。这直接与LLMs的可解释性和理解其内部机制相关。具体而言，论文旨在创建一个基准测试系统来评估和改进LLMs在处理语义模糊、多义性高或不明确的信息查询时的表现。通过这样的研究，可以增进对LLMs内部结构和工作原理的理解，特别是它们如何处理和生成文本，以及在面对复杂或模糊输入时的决策过程。这种分析有助于提高LLMs的透明度和可解释性，使人们能够更好地理解这些模型是如何做出决策的，从而增强它们在实际应用中的可靠性和有效性。\n",
      "\n",
      "- Revisiting Knowledge Distillation for Autoregressive Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。知识蒸馏（Knowledge Distillation）是一种技术，它通过将一个大型预训练模型（通常称为教师模型）的知识传递给一个较小的模型（称为学生模型），以提高学生模型的性能和效率。在语言模型领域，这种技术可以用于改进模型的预测精度、推理速度以及资源利用效率。\n",
      "\n",
      "对于自回归语言模型（Autoregressive Language Models, ALMs），即一种生成文本序列的模型，知识蒸馏可以被用来更好地理解模型如何处理不同的语言结构和上下文信息。通过分析教师模型和学生模型之间的差异，研究人员可以揭示ALMs在处理特定类型任务时的行为模式和决策过程。这有助于增强我们对这些模型内部工作原理的理解，包括它们如何利用注意力机制（Attention Mechanisms）来分配不同单词或句子片段的重要性，以及它们在构建文本序列时的决策过程。\n",
      "\n",
      "因此，这篇论文通过探索知识蒸馏在自回归语言模型中的应用，为深入理解大型语言模型的内部结构和解释其行为提供了一个途径。它不仅关注于提升模型性能，同时也致力于提高模型的可解释性，这对于促进语言模型在实际应用中的可靠性和透明度至关重要。\n",
      "\n",
      "- CLOMO: Counterfactual Logical Modification with Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题\"CLOMO: Counterfactual Logical Modification with Large Language Models\"与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文标题中的关键词“Counterfactual Logical Modification”表明研究可能涉及到使用LLMs进行逻辑修改，而这种修改可能是基于假设性情境（counterfactual scenarios）的。在深度学习领域，尤其是自然语言处理中，对模型决策过程的解释和理解是一个重要且挑战性的课题。大型语言模型由于其复杂性和规模，往往在生成高质量文本的同时，也带来了预测结果难以解释的问题。\n",
      "\n",
      "因此，探讨如何通过修改输入或输出，从而更好地理解LLMs的决策过程或内部结构，具有重要的理论和应用价值。这样的研究有助于提升我们对LLMs工作原理的理解，同时也可能为提高模型的可解释性提供新的方法和技术。具体而言，论文可能会探索如何利用反事实逻辑修改技术来增强LLMs的可解释性，或者通过这种方式来揭示模型在处理特定任务时的内在机制。这符合研究LLMs内部结构和解释的范畴，旨在帮助人们更好地理解和信任这些强大的语言生成工具。\n",
      "\n",
      "- Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与Large language models（大型语言模型）的可解释性或理解其内在机制相关。标题中明确提到了“Interpretable User Satisfaction Estimation”（可解释的用户满意度估计），这表明研究可能探讨了如何通过解释模型的行为来评估用户对对话系统的满意度。这种分析通常涉及对模型决策过程的理解，特别是对于大型语言模型，它们的内部结构复杂，包括多层的block和attention机制，这些机制对于生成文本至关重要。\n",
      "\n",
      "论文的主旨可能是为了提高模型的透明度和可解释性，使人们能够更好地理解大型语言模型是如何工作以及它们在特定任务上的决策依据。这有助于构建更可靠、可信任的AI系统，尤其是在关键应用领域，如客户服务、教育咨询等，用户满意度的准确估计对于提升用户体验至关重要。\n",
      "\n",
      "因此，这个论文很可能深入探讨了如何利用可解释性技术来理解和改进大型语言模型在对话系统中的表现，特别是从模型的内部block、attention结构上进行分析，以提供更直观的见解，从而增强模型的解释力和用户的信任度。\n",
      "\n",
      "- Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“加强注意力中最短的一环：增强大型语言模型对有效工具使用的上下文意识”表明它探讨了如何改进大型语言模型（Large Language Models, LLMs）的上下文意识，从而提高它们在使用工具时的有效性。这涉及到对LLMs内部结构的理解和优化，特别是关注于“注意力”机制（Attention mechanism），这是LLMs的核心组件之一，用于决定输入序列中哪些部分对生成的输出影响最大。\n",
      "\n",
      "论文的标题中提到的“注意力”和“内部block”暗示了作者可能在研究如何通过调整或优化这些关键组成部分来提升模型的可解释性和理解力。这种工作通常涉及到深入分析模型的内部机制，如注意力权重分配的过程，以及如何通过修改模型结构或参数来改善其对特定任务（如理解和使用工具）的表现。\n",
      "\n",
      "因此，这篇论文与Large language models的可解释性或理解LLM内在机制有关，因为它旨在通过增强模型对上下文的敏感度来提高LLMs的性能和理解能力。这有助于人们更好地理解模型的工作原理，并可能为开发更强大、更易于解释的AI系统提供见解。\n",
      "\n",
      "- Layer-Condensed KV Cache for Efficient Inference of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，论文探讨的是如何通过层压缩键值缓存（Layer-Condensed KV Cache）来提高大型语言模型的推理效率。这一方法关注于优化模型在处理大规模文本数据时的计算资源使用和时间效率问题。\n",
      "\n",
      "大型语言模型通常由多个层次（layers）组成，每个层次执行特定的转换任务，如线性变换、激活函数应用、归一化等，最终生成输出序列。论文中提到的层压缩键值缓存技术可能旨在通过更有效地管理模型的内存状态（即键值对，用于存储中间计算结果），来减少模型推理过程中的计算开销和内存需求。\n",
      "\n",
      "这种技术对于理解LLM的内部结构和解释其工作原理具有潜在价值。通过优化内存管理和加速推理过程，研究者可以提供更多的洞察力，揭示模型在不同层面上如何进行信息处理和决策，以及这些过程如何影响最终的生成文本质量。此外，这种改进还可能有助于提升模型的可解释性，使得研究人员和开发者能够更好地理解模型的决策过程和行为模式。\n",
      "\n",
      "因此，该论文主题与大型语言模型的可解释性和理解其内在机制相关联，它提供了一种可能的途径来增强我们对这类复杂模型的理解和控制。\n",
      "\n",
      "- Aligning Large Language Models by On-Policy Self-Judgment\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到几个与大型语言模型（Large Language Models, LLMs）相关的概念，包括“对策略自我判断”（On-Policy Self-Judgment）。这些关键词表明该论文可能探讨的是如何通过某种形式的自我评估或反馈机制来调整或优化大型语言模型的行为或性能。在这一过程中，可能会涉及对模型内部结构的理解、模型决策过程的解释以及模型能力的提升等方面，尤其是关注于模型的可解释性和内在机制的理解。因此，这个论文摘要与大型语言模型的可解释性或理解其内部机制的主题是相关的。\n",
      "\n",
      "具体而言，论文可能探索了如何通过模型自身对输出进行评价，从而指导模型学习过程中的调整，以提高其在特定任务上的表现。这种自我判断的过程，实际上是在尝试理解模型在不同决策点上的行为和结果，进而对模型的内部结构和决策逻辑进行深入分析。这有助于研究人员和开发者更好地理解模型是如何做出决策的，以及哪些因素影响了模型的表现，从而为模型的改进提供依据。\n",
      "\n",
      "总之，这个论文题目所蕴含的内容与大型语言模型的可解释性及理解其内部机制密切相关，它旨在通过一种自我评估的方法来优化模型性能，同时促进对模型内部工作原理的深入了解。\n",
      "\n",
      "- Split and Rephrase with Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：该论文标题“Split and Rephrase with Large Language Models”（大型语言模型的拆分与重述）表明研究可能涉及对大型语言模型（Large Language Models，LLMs）进行更深入的分析和操作，特别是如何通过拆分（splitting）和重述（rephrasing）文本来增强或改变LLMs生成的内容。这与大型语言模型的可解释性或理解其内在机制有关，因为研究可能探讨了LLMs在处理不同输入时的行为模式、决策过程以及它们是如何生成特定输出的。通过拆分和重述文本，研究人员可能试图揭示LLMs在理解和生成文本时的内部工作原理，以及它们如何捕捉和表达语义信息。这种研究有助于提高我们对LLMs内部结构的理解，并可能为提升模型性能、改进模型解释性和增强其应用提供见解。\n",
      "\n",
      "解读：该论文题目中的关键词“Split”和“Rephrase”暗示了研究可能关注于LLMs处理和生成文本的过程细节，包括它们如何分解输入、重组内容以及最终生成输出。这样的研究通常涉及到对LLMs的内部机制进行分析，比如它们如何使用注意力机制（attention mechanisms）、上下文依赖（contextual dependencies）以及语言模型的参数化方式等，来实现高效和准确的文本生成。因此，这一主题与LLMs的可解释性和理解其内在工作原理密切相关。\n",
      "\n",
      "- SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文的标题\"SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\"表明它探讨了一种名为SAPT的方法，该方法旨在通过共享注意力框架来提高大型语言模型在持续学习过程中的参数效率。这直接与大型语言模型（Large Language Models，LLMs）的可解释性和理解其内在机制相关联。具体来说，论文关注的是LLMs内部的注意力机制（attention structure），这是LLMs理解和生成文本的关键组成部分。通过提出一个共享注意力框架，研究者试图提供一种更深入的理解LLMs如何处理和学习新任务的方式，以及它们内部的工作原理。此外，参数效率也是LLMs设计和应用中的一个重要考虑因素，因为更高效的模型通常意味着更低的计算成本和更好的可扩展性，这对于解释和理解LLM的内部结构至关重要。因此，这个论文的主题与大型语言模型的可解释性、理解其内部机制以及优化其性能紧密相关。\n",
      "\n",
      "- Quantifying Generalizations: Exploring the Divide Between Human and LLMs’ Sensitivity to Quantification\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：该论文标题“量化泛化：探索人类与大型语言模型在量化敏感性方面的差异”表明它探讨了大型语言模型（Large Language Models, LLMs）在处理量化任务时的特性和与人类之间的比较。这涉及到对LLMs内部机制的理解，特别是它们如何处理数字信息、进行计算以及在不同上下文中应用量化知识的能力。通过研究这些差异，论文可能深入分析了LLMs的内部结构、block和attention机制，以及它们如何影响LLMs的决策过程和输出。这种研究有助于提升我们对LLMs工作原理的认识，特别是它们在理解和生成量化内容时的能力和局限性，从而促进更有效的模型设计和应用。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- ToMBench: Benchmarking Theory of Mind in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《ToMBench: 在大型语言模型中评估理论思维能力的基准》与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制有关。虽然该论文主要关注的是评估LLMs在理论思维（Theory of Mind, ToM）方面的表现，即模型是否能够理解他人的观点、意图和情感等心理状态，这实际上涉及到对LLMs内部结构和处理方式的理解。评估这些能力通常需要深入分析模型如何生成特定类型的文本、如何在上下文中推断和使用信息，以及它们在处理复杂情境时的决策过程。这种评估方法可以间接提供关于LLMs如何理解和解释信息的洞察，尤其是当研究者试图理解模型在遇到特定任务时的行为模式和决策逻辑时。因此，尽管这篇论文的焦点在于评估而不是直接解释LLMs的内部结构，但它确实与理解LLMs的能力和机制相关联，特别是从应用的角度来看。\n",
      "\n",
      "- LANDeRMT: Dectecting and Routing Language-Aware Neurons for Selectively Finetuning LLMs to Machine Translation\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“LANDeRMT: Detecting and Routing Language-Aware Neurons for Selectively Fine-tuning LLMs to Machine Translation”（LANDeRMT：检测和路由语言感知神经元以选择性地微调大型语言模型用于机器翻译）表明它探讨了如何通过特定的方法（即，检测和路由语言感知神经元）来调整大型语言模型（LLMs）以更好地服务于机器翻译任务。这涉及到对LLM内部结构的理解，特别是如何在不同的语言上下文中识别和利用关键的神经元（即，神经网络中的基本单元），以及如何基于这些识别的结果对模型进行微调，以提高其在特定任务（如机器翻译）上的性能。\n",
      "\n",
      "因此，这个论文与大型语言模型的可解释性和理解其内在机制相关，因为它关注于LLM内部结构的特定部分（语言感知神经元）及其在任务特定场景下的作用，以及如何通过调整这些部分来优化模型表现。此外，它还涉及到对LLM内部工作机制的理解，因为要实现这一目标，需要对模型如何处理不同语言信息有深入的了解。\n",
      "\n",
      "- What Do Language Models Learn in Context? The Structured Task Hypothesis.\n",
      "  回复: 是。\n",
      "\n",
      "理由：这个论文标题表明它探讨的是语言模型在上下文中学习的内容。\"Structured Task Hypothesis\"（结构化任务假设）可能涉及分析语言模型如何通过特定任务或结构化的数据集进行训练，以及这些训练过程如何影响模型的理解和预测能力。在大型语言模型（Large Language Models, LLMs）的背景下，研究它们在不同上下文中的学习方式有助于理解模型的可解释性，特别是关于它们如何处理文本、提取意义以及生成输出的过程。此外，这样的研究可以提供对LLMs内部机制的洞察，比如它们如何利用注意力机制（attention mechanisms）来聚焦于文本的不同部分以做出更准确的预测。因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制有关。\n",
      "\n",
      "- On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它探讨的是神经语言模型（Neural Language Models, NLMs）在链式思维推理（Chain-of-Thought Reasoning）方面的表征能力。链式思维推理是指通过一系列逻辑步骤来解决问题的能力，这种能力对于理解自然语言文本至关重要。大型语言模型（Large Language Models, LLMs）通常指的是参数量巨大、能够处理复杂语言任务的神经网络模型，如通义千问、GPT系列等。\n",
      "\n",
      "论文标题中提到的“表征容量”（Representational Capacity）意味着研究将关注模型如何编码和表示信息，以及这些表示如何支持链式思维推理。这涉及到模型内部的结构和机制，比如如何处理信息、执行推理过程以及最终生成合理的输出。因此，论文很可能会深入分析LLMs的内部组件（如block、attention结构），探讨它们如何协同工作以实现更复杂的语言理解和生成任务。\n",
      "\n",
      "总的来说，这篇论文与大型语言模型的可解释性或理解其内在机制直接相关，因为它旨在揭示模型如何处理和利用信息进行推理，从而提高对LLMs内部工作原理的理解。\n",
      "\n",
      "- Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与Large Language Models (LLMs)的可解释性和理解其内在机制相关。具体来说，摘要提到的是分析LLM在对话摘要化（Dialogue Summarization）任务中的行为，揭示情境幻觉趋势（Circumstantial Hallucination Trends）。情境幻觉是指模型在生成文本时可能出现的错误或不准确的描述，这些描述可能基于对输入数据的理解偏差或是模型自身学习过程中的一些局限性。通过研究这些幻觉趋势，可以更深入地理解LLM的工作原理，包括其内部的block和attention结构如何影响模型的输出。这种分析有助于提高LLM的解释性，使得人类能够更好地理解模型是如何做出决策和生成特定文本的，从而改进模型的设计和应用。\n",
      "\n",
      "- Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“在高压缩比下保留关键信息：基于查询的LLMs压缩器”表明它探讨了如何在不损失重要信息的情况下对大型语言模型（Large Language Models, LLMs）进行压缩。这涉及到理解LLMs的内部结构和机制，特别是如何通过查询来指导压缩过程，从而更好地保留模型的关键信息。这种研究有助于提升我们对LLMs的理解，以及它们处理和生成文本的方式。因此，该论文摘要与大型语言模型的可解释性或理解其内在机制有关，因为它关注于提高我们对这些模型工作原理的认识，特别是在压缩过程中如何保持模型性能和能力不下降。\n",
      "\n",
      "- MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题和摘要提到了对语言模型（Language Models）进行深入分析，特别是通过条件消除退化（conditioning away degeneracy）来揭示真正的语言模型模式。这涉及到对语言模型内部机制的理解，尤其是如何避免模型在预测时出现的退化问题，即在面对某些输入时产生过于泛化的输出。这与大型语言模型（Large Language Models）的可解释性和理解其内在机制紧密相关。\n",
      "\n",
      "大型语言模型通常包含复杂的结构，如多层神经网络、注意力机制（Attention）、块（Blocks）等，这些组件共同作用以生成文本。通过对这些模型的内部结构进行分析和优化，可以帮助我们更好地理解它们的工作原理，提高模型性能，并开发出更有效的训练和使用策略。因此，该论文探讨的主题与大型语言模型的内部结构、解释性和理解其内在机制具有直接关联。\n",
      "\n",
      "- Harder Task Needs More Experts: Dynamic Routing in MoE Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到多专家模型（MoE models），也称为门控专家（Dynamic Routing）模型，是一种在大型语言模型中引入可解释性和理解其内部机制的方法。在传统的并行模型中，输入数据会同时被所有模型（即“专家”）处理，而在MoE模型中，输入数据首先通过一组门控函数（通常是神经网络层）进行路由，决定数据应该由哪个专家处理。这使得模型能够根据任务的复杂度动态地分配资源，更高效地处理不同的任务。通过这种方式，MoE模型可以实现对不同任务的优化，同时也提供了一种方法来理解模型是如何做出决策的，因为我们可以观察到哪些专家在特定任务中被更多地使用。\n",
      "\n",
      "因此，这个论文主题与大型语言模型的可解释性直接相关，因为它探讨了如何通过动态路由机制来增强模型性能的同时，增加模型的透明度和理解其内部工作原理的可能性。这有助于帮助人们更好地理解大型语言模型的内部结构和决策过程。\n",
      "\n",
      "- CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
      "  回复: 是。\n",
      "\n",
      "理由：这个论文标题“Critique LLM: 向前推进大型语言模型生成评估的有信息批评生成模型”表明它关注于为大型语言模型（Large Language Models, LLMs）的生成结果提供评估和反馈。在讨论LLMs的可解释性和理解其内在机制时，通常会涉及到如何分析模型的决策过程、内部结构以及注意力机制等。论文可能探讨了如何通过生成批评性文本来更好地理解LLMs在特定任务上的表现，这涉及到对模型内部结构的理解和分析，尤其是可能涉及模型的块（blocks）、注意力机制（attention structures）等方面。通过这种批评生成模型，可以提供一种工具或方法来帮助人类更深入地理解LLMs的工作原理和性能局限，从而提高LLMs的可解释性和透明度。因此，该论文摘要与Large language models的可解释性或理解LLM内在机制的主题相关。\n",
      "\n",
      "- Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《Symbol-LLM：为大型语言模型建立基础的符号为中心的接口》似乎在探讨一种方法，即通过构建一个符号为中心的接口来更好地理解和与大型语言模型（LLMs）交互。虽然论文的具体内容未被提供，但题目中提到的“符号”和“大型语言模型”的结合，以及“基础的符号为中心的接口”，表明它可能涉及对LLM内部机制的解释或分析。\n",
      "\n",
      "大型语言模型通常基于复杂的架构，如Transformer，这些架构包含多层的编码器和解码器，以及自注意力机制等组件。通过创建一个符号为中心的接口，研究者可能试图提供一种更直观的方式来理解这些模型如何处理和生成文本。这种接口可能会帮助人们更好地理解LLM是如何根据输入符号进行上下文推理、语义理解以及生成输出的。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性和理解其内部机制相关，特别是从LLM的内部块（如Transformer块）和注意力结构的角度出发，以提高我们对这些模型工作原理的理解。\n",
      "\n",
      "- Visualization Recommendation with Prompt-based Reprogramming of Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题涉及到Large language models（大型语言模型），并且提到了可视化推荐和基于提示的大型语言模型重编程。这表明该论文可能探索了如何通过可视化工具来理解和解释大型语言模型的工作机制，特别是通过改变模型的输入（基于提示）来观察输出变化的方式。这种研究方法有助于人类更好地理解大型语言模型内部的决策过程和工作原理，特别是关注模型的可解释性和内在机制。因此，这个论文摘要与Large language models的可解释性或理解其内在机制相关，尤其是从模型的内部结构和工作原理上进行解释。\n",
      "\n",
      "- LooGLE: Can Long-Context Language Models Understand Long Contexts?\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：这篇论文的标题\"LooGLE: Can Long-Context Language Models Understand Long Contexts?\"（大G：长上下文语言模型能够理解长上下文吗？）直接涉及了大型语言模型（Large Language Models, LLMs）在处理长上下文信息时的能力和理解机制。题目中的\"Long-Context\"表明研究关注的是LLMs如何处理和理解在时间、空间或信息序列中较长的上下文信息。\n",
      "\n",
      "这种研究通常会深入探讨LLMs内部的结构和工作机制，如内部块（blocks）、注意力（attention）机制等，来解释模型是如何构建和利用这些长上下文信息的。通过实验设计、对比分析以及可能的可视化技术，研究者试图揭示LLMs在理解和生成文本时的内在逻辑和决策过程，这对于帮助人类更好地理解这些复杂模型的工作原理至关重要。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性和理解LLM内在机制相关，特别是从LLM的内部块、注意力结构等方面进行探索。\n",
      "\n",
      "- Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明了其主要关注点是在长上下文问题回答（Long-Context Question Answering）场景中使用位置无关的分解式训练方法。这种研究方向与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制紧密相关。具体而言，该论文可能探讨了如何通过分解式训练策略来改进LLMs在处理包含大量历史信息或长篇文本的问题时的表现。这种策略旨在让模型能够更好地理解和利用长上下文信息，这对于提高LLMs在复杂任务中的性能至关重要。\n",
      "\n",
      "位置无关的分解式训练方法可能涉及到将输入文本分解为更小的部分，然后在这些部分之间建立联系，而不仅仅是基于它们在原始文本中的位置。这种方法有助于模型学习到更深层次的语义关系，而不是仅仅依赖于文本的表面顺序。这样的研究对于深入理解LLMs是如何处理和整合不同长度的信息，以及如何通过训练增强模型的这一能力具有重要意义。\n",
      "\n",
      "总之，这个论文题目表明它可能探讨了提升LLMs处理长上下文问题能力的技术细节，特别是通过改进训练策略来增强模型的理解和预测能力。因此，它与大型语言模型的可解释性和理解其内在机制密切相关。\n",
      "\n",
      "- Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文的标题“Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs”表明它探讨的是大型语言模型（Large Language Models, LLMs）在多利益相关者对齐下的决策制定过程。这涉及到对LLMs内部结构和工作机制的理解，尤其是如何在多方面的考量下做出决策。论文通过这种方式试图增强我们对LLMs内部运作机制的理解，包括它们如何处理和整合不同利益相关者的信息，以及它们在决策过程中可能使用的特定结构或组件（如内部block、attention机制等）。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "解读：这篇论文可能关注的是如何使LLMs的决策过程更加透明和可解释，这对于提高人们对这些模型的信任度和在实际应用中的接受度至关重要。通过研究多利益相关者对齐的概念，作者可能试图探索如何设计或调整LLMs，使其决策更加符合社会伦理标准，同时保持技术效率和实用性。这种研究方向有助于推动LLMs领域的发展，特别是在确保模型决策的公正性、道德性和可解释性方面。\n",
      "\n",
      "- Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它探讨的是如何通过增强大型语言模型（Large Language Models, LLMs）的可解释性来提升它们在上下文学习（In-context Learning）中的表现。具体来说，“Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning”表示研究者提出了一种方法，即“可解释性软集合（Explanation-aware Soft Ensemble）”，旨在使大型语言模型能够更好地理解和适应给定情境下的任务，从而提高其在特定任务上的性能。\n",
      "\n",
      "大型语言模型的内部结构通常包括多层神经网络，如Transformer架构，其中包含自注意力机制（Attention Mechanism）等组件。这些结构允许模型在处理文本时考虑上下文信息，从而生成连贯且相关的响应。然而，尽管大型语言模型在许多自然语言处理任务上表现出色，但它们的决策过程往往难以解释，这限制了我们对模型行为的理解和优化。\n",
      "\n",
      "因此，这个论文关注的点在于提高大型语言模型的可解释性，使其内部工作原理更加透明，这有助于人们更好地理解模型如何做出决策以及为什么做出这样的决策。通过提高可解释性，研究者可能能够发现模型在特定任务上的优势和弱点，进一步优化模型设计和训练过程，以及改进其在实际应用中的性能。\n",
      "\n",
      "总之，这个论文标题与大型语言模型的可解释性和理解其内部机制有关，因为它探讨了如何通过一种新的方法来提升模型在上下文学习任务上的表现，同时增强我们对模型决策过程的理解。\n",
      "\n",
      "- DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention\n",
      "  回复: 是。\n",
      "\n",
      "理由及解读：论文标题“DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention”（DeCoT：通过因果干预在大型语言模型中为知识密集型任务去偏见化思考链）表明该研究关注于大型语言模型（Large Language Models, LLMs）的特定方面，即它们在处理知识密集型任务时可能存在的偏见问题。论文标题中提到的“Chain-of-Thought”（思考链）指的是模型生成答案时的思维过程，通常包括对问题的理解、相关信息的检索以及最终答案的形成等步骤。\n",
      "\n",
      "虽然论文标题并未直接提及“可解释性”或“理解LLM内在机制”，但考虑到去偏见化（debiasing）通常涉及到对模型决策过程的深入理解以及对模型内部结构和行为的分析，因此可以推断，该论文可能涉及对LLM的内部结构、特定组件（如block或attention机制）及其如何影响模型性能和决策过程的研究。通过因果干预（causal intervention），研究者试图理解和调整这些内部机制，以减少模型在知识密集型任务中的偏见。\n",
      "\n",
      "综上所述，尽管论文标题没有明确指出与LLM的可解释性和内在机制理解直接关联，但基于题目描述的内容推测，它很可能探讨了这些主题，因此答案是“是”。\n",
      "\n",
      "- Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor\n",
      "  回复: 是。\n",
      "\n",
      "这个论文摘要与 Large Language Models (LLMs) 的可解释性和理解其内在机制相关。标题中提到的 \"Black-box Language Models\" 指的是那些内部结构复杂且难以直接理解和解释的大型语言模型。\"Score-based Progressive Editor\" 这个方法似乎旨在通过逐步编辑模型生成文本的评分（score）来控制文本生成过程，从而提供一种方式来理解或影响黑盒模型的行为。这种方法可能涉及到对 LLM 内部的 block 或 attention 结构进行分析和调整，以更好地理解模型是如何处理输入并生成输出的。因此，该论文尝试通过具体的技术手段来提高对 LLM 内部工作原理的理解，符合题目要求的“解释 LLM 的内部结构和解释的”目标。\n",
      "\n",
      "- RDRec: Rationale Distillation for LLM-based Recommendation\n",
      "  回复: 是。\n",
      "\n",
      "理由和解读：论文标题“RDRec: Rationale Distillation for LLM-based Recommendation”表明该研究关注的是如何通过“理据蒸馏”（Rationale Distillation）来提升基于大型语言模型（Large Language Models, LLMs）的推荐系统的性能。在深度学习领域，特别是自然语言处理中，大型语言模型通常被认为是“黑箱”模型，即它们的决策过程对于人类来说往往难以理解和解释。然而，这种不透明性限制了我们对模型如何做出决策的理解，以及可能存在的偏差或错误。\n",
      "\n",
      "“理据蒸馏”是一种技术，旨在提取和解释大型语言模型的决策依据，使其决策过程变得可解释。这有助于人们更好地理解模型的工作原理，识别其在特定任务上的优势和局限性，并可能发现模型在处理某些类型数据时可能出现的偏差或偏见。通过这种解释方法，研究者可以提供关于模型内部结构和决策机制的洞察，从而增强模型的透明度和可信度。\n",
      "\n",
      "因此，该论文摘要与大型语言模型的可解释性和理解其内在机制相关，因为它探讨了如何通过理据蒸馏来解释基于LLMs的推荐系统的工作原理，帮助人们更好地理解这些模型的内部结构和决策过程。\n",
      "\n",
      "- UltraSparseBERT: 99% Conditionally Sparse Language Modelling\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“UltraSparseBERT: 99% Conditionally Sparse Language Modelling”表明它关注的是BERT模型的一种变体，即UltraSparseBERT，该模型在语言建模任务中表现出99%的条件稀疏性。这实际上是在探讨大型语言模型（LLM）的可解释性方面的一个重要议题。通过研究模型的稀疏性，作者试图揭示模型决策过程中的内部结构和机制，特别是注意力机制如何在不同情况下分配权重，以及哪些输入对模型输出的影响更大。这种研究有助于理解大型语言模型是如何处理和生成文本的，以及它们在不同任务上的表现背后的原因。因此，这个论文与大型语言模型的可解释性和理解其内在机制相关。\n",
      "\n",
      "- Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model\n",
      "  回复: 是。\n",
      "\n",
      "理由：这篇论文标题提到的是使用大型语言模型（Large Language Models, LLMs）进行知识图谱到文本的双向多粒度生成框架。这表明研究者正在探讨如何利用LLMs在处理复杂任务（如知识图谱到文本转换）时的内在机制和能力。题目中提到的“双向”和“多粒度”表明可能涉及了LLMs内部的结构和工作方式，比如如何在不同层次上理解和生成文本，以及如何在正向和反向方向上处理信息。此外，“生成框架”通常意味着研究者在探索如何设计系统或方法来更好地理解和解释LLMs在执行特定任务时的行为和决策过程。因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制有关，特别是从LLM的内部结构和生成过程的角度出发。\n",
      "\n",
      "- AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“AFLoRA: 在大规模模型参数高效微调中基于低秩自适应的解冻”似乎与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。论文标题中的关键词“参数高效微调”（Parameter Efficient Fine-Tuning）和“低秩自适应的解冻”（Adaptive Freezing of Low Rank Adaptation）表明，该研究可能探讨了如何在保持模型大小不变的情况下，通过微调特定部分来提高模型性能的方法。这种技术涉及到对模型参数的选择性调整，以优化特定任务的表现，同时减少计算成本。\n",
      "\n",
      "在大型语言模型中，内部结构通常包括多个层（如Transformer层），这些层使用注意力机制（Attention Mechanism）来处理输入序列。AFLoRA可能通过研究如何在这些层中选择性地“冻结”（即固定某些参数不进行微调）或“解冻”（即允许某些参数进行微调）来提高模型的可解释性。这种策略有助于理解和控制模型的行为，特别是在需要针对特定任务进行优化时。\n",
      "\n",
      "总之，AFLoRA的研究方向与大型语言模型的可解释性和理解其内部机制相关，因为它探索了一种方法来优化模型性能，同时保持模型的简洁性和易于理解性。\n",
      "\n",
      "- DDPrompt: Differential Diversity Prompting in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "理由：论文标题“DDPrompt: Differential Diversity Prompting in Large Language Models”表明该研究聚焦于在大型语言模型中引入差异多样性提示（DDPrompt）的方法。这种方法涉及到通过特定的提示策略来增强大型语言模型的多样性和差异性输出，进而提升模型的生成质量和可解释性。通过调整模型的输入提示，研究人员可以更好地理解和控制模型的行为，这对于深入理解大型语言模型的内部工作原理和决策过程至关重要。\n",
      "\n",
      "解读：大型语言模型（LLMs）的可解释性是一个关键的研究领域，因为这些模型通常被认为是“黑盒”，其内部机制难以理解。通过引入如DDPrompt这样的技术，研究者尝试增加模型生成内容的多样性，同时保持输出的一致性和质量。这种方法有助于揭示模型如何处理不同类型的输入，并对特定任务产生响应。这不仅增强了模型的性能，还为解释LLM如何做出决策提供了一种途径，对于提高模型的透明度和可信度具有重要意义。因此，这篇论文与大型语言模型的可解释性和理解其内部机制直接相关。\n",
      "\n",
      "- Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题“从开始深入：语言模型如何为密集检索嵌入长文档”表明它探讨了大型语言模型（Large Language Models, LLMs）在处理长文档时的内部机制，特别是与密集检索相关的部分。密集检索（Dense Retrieval）通常涉及到在大型文本集合中找到与查询最相关的段落或句子，这需要模型能够有效地理解和记忆大量信息。\n",
      "\n",
      "论文的标题和主题都指向了对LLMs内部结构的理解和解释，尤其是它们如何处理和存储长文档的信息以供后续检索使用。这可能涉及对模型的内部块（如Transformer的编码器层）、注意力机制（attention structure）以及其他可能用于捕捉上下文、长距离依赖性和文本表示的特定设计和技术的分析。\n",
      "\n",
      "因此，该论文摘要与大型语言模型的可解释性或理解其内在机制有关，因为它旨在揭示模型是如何实现这种特定任务的，从而帮助人们更好地理解LLMs的工作原理。\n",
      "\n",
      "- The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题表明它关注的是大型语言模型（Large Language Models, LLMs）的解释可信度（faithfulness）问题，特别是针对自由文本解释（free-text explanations）。题目中的“Probabilities Also Matter”暗示了研究可能涉及概率在评估LLMs解释准确性中的作用，这与LLM的内部工作原理和可解释性直接相关。\n",
      "\n",
      "在大型语言模型中，概率通常用于衡量模型生成某个序列的概率，这是它们决策过程的一部分。通过关注概率在评估解释可信度中的作用，这篇论文可能会探讨如何更准确地量化和理解LLMs生成的文本解释的质量和可靠性。这涉及到对LLM内部结构的理解，比如其基于注意力机制（attention mechanisms）的块（blocks）如何影响生成的文本以及这些生成文本的解释。\n",
      "\n",
      "因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制相关，因为它试图提供一种更准确的度量方法来评估LLMs生成的自由文本解释的可信度，这有助于提升我们对LLM内部结构和解释机制的理解。\n",
      "\n",
      "- Don’t Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models\n",
      "  回复: 是。\n",
      "\n",
      "这个论文标题《别买了！重新评估对比多模态模型的广告理解能力》似乎关注于对比多模态模型（可能涉及到大型语言模型）在理解广告内容方面的表现。这与大型语言模型的可解释性和理解其内在机制相关，因为它探讨了模型如何处理和理解包含多种模态信息（如文本、图像等）的复杂输入，尤其是广告这种通常包含多种信息形式的内容。论文可能会深入分析模型的内部组件（如block、attention结构等），解释它们在处理多模态数据时的表现和局限性，以及模型在特定任务（如广告理解）上的决策过程。这样的研究有助于提升我们对大型语言模型工作原理的理解，以及它们在实际应用中的性能和可靠性。\n",
      "\n",
      "- Aligning Large Language Models via Fine-grained Supervision\n",
      "  回复: 是。\n",
      "\n",
      "这个论文题目涉及到的是对大型语言模型（Large Language Models，LLMs）进行细粒度的监督以实现对模型行为的调整和优化。在深度学习领域，尤其是自然语言处理中，大型语言模型的可解释性和理解其内部机制是一个重要但挑战性的研究方向。通过细粒度的监督，研究人员可以更深入地了解模型如何做出特定决策，以及模型在处理不同任务时的行为模式。\n",
      "\n",
      "具体到\"Aligning Large Language Models via Fine-grained Supervision\"这一主题，它可能探讨了如何通过精细的监督信号来对模型进行微调，从而增强模型的解释性。这种细化的监督可能涉及对模型输出的特定方面进行针对性的反馈，例如对注意力机制的特定部分、模型内部的特定层或者特定类型的语义理解进行指导。这样的方法有助于研究人员和开发者更好地理解模型是如何利用上下文信息生成文本的，以及哪些因素影响了最终的输出。\n",
      "\n",
      "因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关，特别是从模型的内部组件、如块和注意力结构的角度出发，解释和分析模型的能力。这为帮助人们理解LLM的内部结构和提供解释提供了理论基础和实践方法。\n",
      "\n",
      "- DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms\n",
      "  回复: 是。\n",
      "\n",
      "理由：这个论文标题提到了“Enhancing Large Language Models for Reflective Translation”，意味着它关注于提升大型语言模型（Large Language Models, LLMs）的反思性翻译能力。同时，“Dual Learning Feedback Mechanisms”暗示了论文可能涉及到了关于LLMs内部学习过程和反馈机制的研究，这与LLMs的可解释性和理解其内在机制紧密相关。通过双学习反馈机制来增强模型的性能，这通常涉及到对模型内部结构和决策过程的理解，比如如何通过关注特定的block或attention结构来改进模型的表现。因此，该论文与LLMs的可解释性以及理解其内部机制相关。\n",
      "\n",
      "解读：虽然论文标题没有直接提到“可解释性”或“理解LLM的内在机制”，但基于其关注点——通过特定的学习反馈机制来提升翻译能力，可以合理推测其内容可能包含了对LLMs内部结构、工作原理、以及如何通过调整这些结构来优化模型性能的深入探讨。这种研究对于提高我们对LLMs的理解和可解释性是非常有价值的。\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# 筛选与该领域相关的论文摘要\n",
    "related_papers = [entry for entry in classified_papers if \"是\" in entry['response'][:5]]\n",
    "\n",
    "# 输出与该领域相关的论文摘要\n",
    "print(f\"与该领域相关的论文摘要共有{len(related_papers)}篇:\")\n",
    "for entry in related_papers:\n",
    "    print(f\"- {entry['paper']}\\n  回复: {entry['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Paper筛选后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与该领域相关的论文摘要共有11篇:\n",
    "\n",
    "- LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models - Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Elena Voita\n",
    "  回复: 是\n",
    "\n",
    "这个论文摘要与Large language models的可解释性和理解LLM（大型语言模型）的内在机制相关。标题中的\"LM Transparency Tool\"表明研究的是如何分析和理解Transformer语言模型，这是大型语言模型的一种类型，而这种工具的目的是提高对这些复杂模型行为的理解。因此，它直接关联到可解释性和模型机制的洞察。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Paper筛选后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与该领域相关的论文摘要共有124篇:\n",
    "- Inference to the Best Explanation in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到“推理到最佳解释”（Inference to the Best Explanation）在大型语言模型中的应用。这与Large language models的可解释性或理解其内在机制有关。在人工智能领域，特别是深度学习和自然语言处理中，解释模型的决策过程和内部工作原理是一个重要但又具有挑战性的任务。大型语言模型（LLMs）因其强大的生成和理解能力而受到关注，但它们的决策过程往往是黑箱性质的，缺乏透明度。\n",
    "\n",
    "推理到最佳解释是一种科学方法论，它试图通过寻找最能解释观察结果的假设来形成理论。将这种方法应用于大型语言模型意味着尝试理解模型是如何基于输入数据生成输出的，以及为什么它选择了特定的输出。这可能涉及分析模型内部的块（如Transformer层）、注意力机制等，以揭示模型决策背后的逻辑和模式。\n",
    "\n",
    "因此，这个论文的主题与帮助人们理解LLM的内部结构和解释能力紧密相关，属于研究LLM可解释性的一部分。通过这样的研究，可以提高模型的透明度和可信度，为更广泛的应用提供支持。\n",
    "\n",
    "- SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：这个论文标题“SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs”表明它探讨的是如何将文本和数值数据融合在一起以理解大型语言模型（Large Language Models, LLMs）中的信息融合机制。这涉及到对LLMs内部结构的理解，特别是它们如何处理和整合不同形式的数据来生成输出。这种研究方向直接与LLMs的可解释性和理解其内在机制有关。通过研究信息融合过程，作者可能试图揭示LLMs如何从多个输入源中提取关键信息，以及这些过程背后的机制，这有助于提升我们对LLMs内部工作的理解。因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关。\n",
    "\n",
    "- Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：这篇论文标题《引擎盖下的信心：大型语言模型中信心概率一致性调查》表明它关注的是大型语言模型（Large Language Models, LLMs）中的信心概率一致性问题。在深度学习模型尤其是大型语言模型中，理解模型的预测自信度是非常重要的，因为这关系到模型输出的可靠性和模型在不同情况下的表现。通过研究模型在生成文本时的信心与实际概率之间的关系，可以更深入地了解模型的决策过程和内部机制。\n",
    "\n",
    "论文可能从多个角度探讨这一主题，包括但不限于：\n",
    "1. **内部结构分析**：可能通过分析模型的特定组件（如Transformer架构中的多头注意力机制、前馈网络等）来探究它们如何影响模型的预测自信度。\n",
    "2. **特征重要性**：研究哪些输入特征对模型的自信度有显著影响，从而提供对模型决策过程的理解。\n",
    "3. **数据分布与偏差**：探讨训练数据分布如何影响模型的预测自信度，以及是否存在潜在的偏见或不一致性。\n",
    "4. **解释性工具**：开发或应用现有的解释性工具和技术，如SHAP、LIME等，以可视化和量化模型的预测过程，增强模型的可解释性。\n",
    "\n",
    "总之，这篇论文旨在深入探讨大型语言模型内部的工作原理，特别是它们如何评估自身预测的可信度，这对于提高模型的透明度和可靠性至关重要。因此，它确实与大型语言模型的可解释性以及理解其内部机制相关。\n",
    "\n",
    "- Token-wise Influential Training Data Retrieval for Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《大型语言模型中的基于token的影响力训练数据检索》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，论文可能探讨了如何通过分析特定token（即语言模型中的词汇单元）在训练数据中的影响力，来提高对LLMs行为的理解。这种研究方法可以帮助人们更好地理解LLMs是如何学习和处理信息的，特别是在处理文本数据时的决策过程。通过关注训练数据中各个token的影响，研究者可能揭示了模型如何形成特定的预测模式、上下文依赖性以及注意力分配等关键方面，这些都是LLMs内部结构和工作原理的重要组成部分。因此，这篇论文旨在增强我们对LLMs内部机制的理解，符合题目的要求。\n",
    "\n",
    "- The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到对预训练语言模型（Pretrained Language Models, LLMs）中子网络泛化（Subnetwork Generalization）的理解。在大型语言模型领域，可解释性和理解模型的内部机制是当前研究的重要方向之一。通过对模型内部结构、组件（如blocks、attention机制等）的研究，可以更好地理解模型如何工作，以及它们是如何学习到特定任务的表示的。\n",
    "\n",
    "\"Understanding Subnetwork Generalization\"部分表明，该论文旨在探索预训练语言模型中的特定子网络如何能够泛化到不同的任务和数据集。这与LLM的可解释性紧密相关，因为它涉及到理解模型决策过程中的内在逻辑和机制。通过分析模型的不同部分如何协同工作以及它们在不同情境下的表现，研究人员可以提供更深入的见解，帮助人类更好地理解这些复杂模型的内部结构和工作原理。\n",
    "\n",
    "因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制相关，因为它关注于模型内部的子网络如何泛化并影响整体性能，从而为理解LLM提供了新的视角。\n",
    "\n",
    "- AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它探讨了如何通过可信度估计的解释调整（Explanation Tuning with Plausibility Estimation）来激发大型语言模型（Large Language Models, LLMs）的抽象能力（Abstraction Ability）。这涉及到对LLMs内部结构的理解以及如何解释它们的工作机制，特别是关注于从LLMs的内部块（blocks）、注意力结构（attention mechanisms）等层面去分析和解释模型的能力。通过这种研究方法，作者旨在帮助人们更好地理解LLMs的内部运作原理，提高对这些复杂模型的可解释性。因此，该论文摘要与Large language models的可解释性和理解其内在机制密切相关。\n",
    "\n",
    "- ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它涉及了一种名为“ConSiDERS”的框架，该框架用于对生成式大型语言模型（Large Language Models, LLMs）进行人类评估。由于其关注点在于评估方法，这与大型语言模型的可解释性和理解其内部机制相关。特别是，如果该框架考虑了从LLM的内部组件（如块、注意力结构等）来评价这些模型的能力，那么它就直接关联到解释LLM的内部结构和提供对其工作原理的理解这一主题。因此，这个论文摘要与大型语言模型的可解释性以及理解它们的内在机制相关。\n",
    "\n",
    "- Active Prompting with Chain-of-Thought for Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文探讨了如何通过“链式思维”（Chain-of-Thought）主动提示（Active Prompting）的方法来增强大型语言模型的能力。这种方法涉及构建一个引导性的输入序列，使得模型能够按照人类可以理解的方式逐步推理和生成答案。通过这种方式，研究者试图提高模型输出的可解释性，让最终结果不仅仅是基于统计学习的预测，而是包含了模型执行的逻辑步骤。这有助于帮助人们更好地理解模型是如何做出决策的，从而增加对模型信任度和应用范围。\n",
    "\n",
    "因此，虽然论文可能不会深入讨论LLM的内部块（如Transformer块）或注意力机制的具体细节，但它确实关注于提高模型的可解释性，这是理解LLM内部结构和机制的重要方面。通过提高可解释性，人们可以更好地理解模型的工作原理，这对于进一步改进模型设计、应用以及在更复杂任务上的使用至关重要。\n",
    "\n",
    "- Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题“Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models”表明它探讨了如何将依赖结构（dependency structures）整合到转换器语言模型（Transformer Language Models）中。这涉及到对大型语言模型（Large Language Models，LLMs）内部机制的理解和解释，特别是关注于它们如何处理和表示语法结构的信息。通过在Transformer架构中集成依赖解析，研究者旨在增强模型对自然语言句子结构的理解，从而提升其生成、理解和解释文本的能力。这种工作直接关联到LLMs的可解释性和理解能力，因为它涉及到分析和解释模型如何处理语言中的依赖关系，以及这些处理方式如何影响模型的整体性能和输出。此外，通过改进模型对依赖结构的处理，研究者也提供了洞察LLMs内部工作机制的途径，有助于构建更强大的、可解释的自然语言处理系统。\n",
    "\n",
    "- SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations\n",
    "  回复: 是。\n",
    "\n",
    "这篇论文标题“SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations”与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制密切相关。论文名称中提到了“Fine-tuning”，这通常与训练大型预训练语言模型有关，以适应特定任务。在大型语言模型的上下文中，“细调”可以指调整模型参数以提高在特定任务上的性能。\n",
    "\n",
    "此外，论文还强调了“Generating Predictions and Natural Language Explanations”，即生成预测和自然语言解释。这表明研究可能涉及如何让模型不仅能够做出预测，还能以人类可读的方式解释这些预测是如何得出的。这种解释对于理解模型的决策过程至关重要，特别是对于大型语言模型，它们的决策过程通常是黑盒性质的。\n",
    "\n",
    "综上所述，这篇论文似乎旨在探索如何通过稀疏细调来实现快速的提示学习，以同时生成预测和自然语言解释。这种工作有助于增进我们对大型语言模型内部结构的理解，以及如何使这些模型的决策过程更加透明和可解释。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks\n",
    "  回复: 是。\n",
    "\n",
    "这个论文题目“通过可逆神经网络学习解释的分离语义空间”表明了它关注的是如何通过使用可逆神经网络（Invertible Neural Networks）来学习和理解大型语言模型（Large Language Models, LLMs）的内部结构以及它们生成解释的能力。可逆神经网络是一种能够学习可逆映射的深度学习模型，这在一定程度上可以增强我们对模型决策过程的理解，因为它们允许我们追踪输入到输出的转换过程。\n",
    "\n",
    "在大型语言模型的背景下，研究者可能探索如何利用这些网络来分解模型生成的解释（即，模型是如何从输入文本中推断出特定概念或信息的），从而构建更清晰、可解释的语义空间。这样的工作有助于提高LLMs的透明度和可解释性，使得人们能够更好地理解模型的决策过程，这对于许多实际应用来说是非常重要的，尤其是在需要模型解释其决策的领域，如医疗、法律等。\n",
    "\n",
    "因此，这个论文摘要与大型语言模型的可解释性或理解其内部机制相关，特别是从LLM的内部结构（如block、attention机制）上进行解释，以帮助人们理解模型的工作原理。\n",
    "\n",
    "- CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning\n",
    "  回复: 是。\n",
    "\n",
    "理由：这篇论文标题\"CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning\"（CANDLE：用于常识推理的大规模语言模型的迭代概念化和实例化提炼）表明，该研究主要关注如何从大规模语言模型（Large Language Models, LLMs）中提取和理解其内部机制，特别是针对常识推理任务。通过“迭代概念化”和“实例化提炼”的方法，研究者试图深入挖掘和解释LLMs在处理常识问题时的内在逻辑和决策过程。这涉及到对LLMs内部结构的理解以及如何从这些模型中提取出有意义的概念和规则，从而增强我们对LLMs工作原理的认识，提升其可解释性和透明度。因此，该论文摘要与大型语言模型的可解释性或理解其内在机制相关。\n",
    "\n",
    "- Surgical Feature-Space Decomposition of LLMs: Why, When and How?\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明了它探讨的主题是大型语言模型（Large Language Models, LLMs）的特征空间分解在手术级的应用。\"Why, When and How?\" 的部分暗示了论文将详细讨论为何需要进行这种分解、何时应用以及如何实施这一过程。这种讨论的核心在于理解并解释LLMs的内部工作机制，包括它们的内部块（blocks）和注意力机制（attention structures）。通过这样的分解，研究人员和开发者可以更好地理解LLMs如何处理和生成文本，从而提高模型的可解释性。因此，这个论文摘要与Large language models的可解释性或理解其内在机制相关，特别是从模型的内部组件和工作原理角度出发，以帮助人们更好地理解LLM的内部结构和工作方式。\n",
    "\n",
    "- Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题和摘要提到了\"Enhancing Large Language Models Reasoning\"（增强大型语言模型推理），这直接关联到Large Language Models（大语言模型）的可解释性和理解其内在机制的主题。特别是，“通过不确定性意识的自适应指导增强推理”这部分强调了对模型推理过程的理解和提升，可能涉及到模型内部的结构、决策过程以及如何通过理解和管理不确定性来改进模型的表现。因此，这篇论文似乎关注于深入理解大型语言模型的工作原理，包括它们如何处理信息、进行决策，以及如何通过特定策略（如不确定性意识的自适应指导）来优化这些过程，这些都是大型语言模型可解释性和理解其内部机制的关键方面。\n",
    "\n",
    "- VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由及解读：这篇论文的标题“VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models”表明它探讨了通过大型语言模型（Large Language Models, LLMs）进行基于知识推理的命名实体识别（Named Entity Recognition, NER）的验证增强方法。命名实体识别是一种自然语言处理任务，涉及到识别文本中的特定实体类型（如人名、地名、组织名等）。通过结合知识图谱等外部知识源进行推理，可以提高命名实体识别的准确性和可解释性。\n",
    "\n",
    "论文中的关键词“验证”（Verification）暗示了对模型决策过程的验证和解释，这与大型语言模型的可解释性和理解其内部机制的主题直接相关。通过知识引导的推理，作者试图增强模型的决策过程，使其在执行命名实体识别时能够提供更合理的解释和更高的可信度。这种增强不仅关注于模型性能的提升，还侧重于增加模型的透明度和可理解性，这对于理解大型语言模型的内部工作原理和结构具有重要意义。\n",
    "\n",
    "因此，这篇论文与大型语言模型的可解释性或理解其内在机制密切相关，尤其是从模型的内部结构、注意力机制（Attention）等方面进行分析和解释，以帮助人们更好地理解这些模型是如何工作的。\n",
    "\n",
    "- Making Long-Context Language Models Better Multi-Hop Reasoners\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它关注的是提升长上下文语言模型（Long-Context Language Models）作为多跳推理工具的能力。在深度学习领域，尤其是自然语言处理（NLP），大型语言模型（Large Language Models, LLMs）因其强大的生成和理解文本的能力而受到广泛关注。然而，这些模型的内部工作原理往往非常复杂，缺乏透明度，即所谓的“黑盒”问题。这限制了我们对模型决策过程的理解，也影响了它们在实际应用中的可解释性和可靠性。\n",
    "\n",
    "提升长上下文语言模型作为多跳推理工具的能力意味着论文可能探讨了如何改进模型处理长序列文本信息的能力，以及如何让模型在进行推理时能够跨多个步骤或多个实体进行思考。这种多跳推理能力对于解决复杂的问题和理解复杂的语境至关重要，尤其是在需要理解文本中隐含关系或执行逻辑推理的任务中。\n",
    "\n",
    "论文可能通过引入新的架构设计、优化现有模型的训练过程、改进注意力机制（Attention Mechanism）或其他技术来提高模型的多跳推理能力。通过增强模型在处理长上下文时的理解力和推理能力，该研究有助于增加模型的可解释性，使人们能够更好地理解模型是如何做出特定决策的，从而提高模型在实际应用中的可信度和效率。\n",
    "\n",
    "因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制密切相关，特别是从模型的内部块（如注意力机制）和整体结构上解释LLM的相关能力，以帮助人理解LLM的内部结构和解释的方面。\n",
    "\n",
    "- Time is Encoded in the Weights of Finetuned Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文标题“Time is Encoded in the Weights of Finetuned Language Models”表明研究者探索了时间信息如何被编码在经过微调的语言模型的权重中。这涉及到对语言模型内部结构的理解，特别是关注于时间信息如何影响模型的决策过程以及模型权重的分布。\n",
    "\n",
    "大型语言模型通常包含复杂的结构，如多层的神经网络、注意力机制等，这些结构允许模型处理序列数据并生成上下文相关的文本输出。理解模型如何利用时间信息来生成准确的预测或输出，对于提高模型的性能和增加模型的可解释性至关重要。通过研究时间信息如何在模型的权重中被编码，研究人员可以提供更深入的洞察，帮助人们理解模型的内部工作原理，以及模型如何学习和应用时间相关的模式。\n",
    "\n",
    "因此，这个研究主题不仅探讨了特定的模型特性（即时间信息的编码），还涉及到了更广泛的领域，包括模型的可解释性和内部机制的理解。这正是与大型语言模型的可解释性和理解其内在机制相关的重要研究方向。\n",
    "\n",
    "\n",
    "- Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《生成预训练结构转换器：大规模无监督语法语言模型》似乎与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，它关注于构建大规模的无监督语法语言模型，这可能涉及到对LLMs内部结构的理解和解释。\n",
    "\n",
    "在自然语言处理领域，理解大型语言模型如何工作以及它们的决策过程对于提高模型性能、确保模型的可靠性和公平性至关重要。结构转换器（Structured Transformers）这一术语暗示了研究者可能正在探索如何通过设计特定的结构来改进或解释Transformer架构在语言建模任务中的表现。\n",
    "\n",
    "通常，这类研究可能会涉及以下几个方面：\n",
    "1. **结构设计**：探讨特定的模型结构如何影响语言模型的性能和可解释性。\n",
    "2. **注意力机制**：分析注意力机制在不同上下文中的应用，以及如何通过调整注意力权重来提高模型的解释度。\n",
    "3. **可解释性技术**：开发或应用技术来增强我们对模型决策过程的理解，比如使用可视化工具、特征重要性分析等。\n",
    "4. **大规模训练**：讨论大规模数据集对模型泛化能力和解释性的影响。\n",
    "\n",
    "因此，这个题目表明作者们可能正在深入探讨大型语言模型的内部结构、它们的工作原理，以及如何提高这些模型的可解释性，这些都是与大型语言模型的可解释性和理解其内在机制直接相关的主题。\n",
    "\n",
    "- ItD: Large Language Models Can Teach Themselves Induction through Deduction\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明了它探讨的主题是大型语言模型（Large Language Models, LLMs）如何通过归纳（Induction）自我学习。\"ItD\"可能是论文的缩写名，而标题中的“教自己”暗示了自我学习的过程。通常，这样的研究会深入探讨语言模型的内部机制，特别是它们如何处理和生成文本、学习模式以及进行推理等任务。通过“归纳通过演绎”这一方法，论文可能在探索语言模型如何从输入数据中学习抽象概念和规则，从而提升其理解能力和生成高质量文本的能力。\n",
    "\n",
    "这种研究对于理解大型语言模型的内在机制是高度相关的，因为它涉及到模型如何处理信息、执行推理以及自我提升的过程。这不仅包括模型的块（blocks）、注意力机制（attention structures）等技术细节，也涵盖了模型如何利用这些组件来实现更高级的认知任务，如理解上下文、生成连贯的对话或创作故事等。\n",
    "\n",
    "因此，这篇论文关注的主题与大型语言模型的可解释性和理解其内部机制密切相关，为帮助人们更好地理解这些复杂系统提供了理论基础和实践洞见。\n",
    "\n",
    "- Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与 Large Language Models（LLM）的可解释性和理解其内在机制相关。标题“Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration”（不要幻觉，退出：通过多LLM协作识别LLM知识缺口）暗示了研究者正在探讨LLM在知识获取和表现上的局限性。这涉及到分析和理解LLM的内部结构，特别是它们如何处理信息、学习模式以及可能产生的错误或不确定性。\n",
    "\n",
    "研究通过多LLM协作来识别这些知识缺口，这表明作者在探索LLM之间的差异、交互和互补性，以提供更全面的理解。这种方法有助于揭示LLM在特定任务或主题上的盲点或不足，从而增强我们对模型工作原理的理解。此外，通过比较不同模型的表现，可以更好地理解LLM的内在机制，包括它们如何分配注意力、处理信息以及在不同上下文中的表现差异。\n",
    "\n",
    "总之，这个论文关注的是通过多模型协作来增强对LLM的理解，特别是它们的知识边界和内部工作机制，因此与大型语言模型的可解释性和理解其内在机制直接相关。\n",
    "\n",
    "- Mission: Impossible Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题 \"Mission: Impossible Language Models\" 提示该研究可能探讨了语言模型的一些复杂或难以理解的问题，尤其是大型语言模型（Large Language Models, LLMs）。大型语言模型通常指的是参数量巨大、能够生成高质量文本的预训练模型，如GPT系列、通义千问等。这类模型由于其复杂的架构和庞大的参数量，往往在使用时表现出强大的语言生成能力，但在理解和解释其内部工作原理方面存在挑战。\n",
    "\n",
    "因此，该论文摘要如果涉及从模型的内部组件（blocks）、注意力机制（attention structures）等角度出发，探讨如何增强对大型语言模型的理解和解释性，那么它确实与大型语言模型的可解释性和理解其内在机制相关。这有助于帮助人类更好地理解这些模型的工作原理，以及它们是如何产生特定输出的。这种研究对于提高模型的透明度、可信度以及在实际应用中的安全性至关重要。\n",
    "\n",
    "- ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling”表明其主要关注点在于通过自我指导的自适应相关性标签来对黑盒大型语言模型进行检索器的对齐。这个过程涉及到理解大型语言模型（LLM）的内部工作原理和机制，特别是如何在没有直接访问其内部结构的情况下（即“黑盒”模型），提升检索器（用于搜索和提取信息的系统）与LLM的协同作用。\n",
    "\n",
    "论文的主旨在于提高检索器和大型语言模型之间的性能一致性，这涉及到对LLM的可解释性和理解其内在机制的研究。通过使用自适应相关性标签，作者试图探索和增强模型对于特定任务的理解能力，特别是那些依赖于复杂语义理解和上下文推理的任务。这种方法不仅有助于提高模型的实用性，还能为研究者提供关于LLM内部决策过程的洞察，从而更好地理解这些模型的工作原理。\n",
    "\n",
    "因此，该论文与大型语言模型的可解释性、理解其内部机制以及解释相关能力紧密相关，旨在帮助人们更深入地理解LLM的内部结构和工作方式。\n",
    "\n",
    "- CaMML: Context-Aware Multimodal Learner for Large Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“CaMML: Context-Aware Multimodal Learner for Large Models”表明它探讨的是大型模型在多模态学习方面的上下文感知能力。大型语言模型（Large Language Models, LLMs）通常处理文本数据，但它们也可以扩展到处理多种模态（如文本、图像、音频等）的信息。因此，研究如何使这些模型对上下文有意识，特别是在处理多模态数据时，直接与LLMs的可解释性和理解其内在机制相关。\n",
    "\n",
    "上下文感知能力对于理解LLMs的工作原理至关重要，因为这涉及到模型如何整合来自不同模态的信息，并根据特定上下文生成有意义的输出。这种研究有助于揭示LLMs在处理复杂任务时的决策过程，从而提高我们对这些模型内部结构的理解。通过关注模型如何在不同的输入模式之间进行交互和整合，这项工作可能为开发更强大、更可解释的LLMs提供见解，同时也为提升模型的透明度和用户信任度做出贡献。\n",
    "\n",
    "总之，这个论文的主题与大型语言模型的可解释性和理解其内部机制密切相关，因为它探索了模型如何在多模态环境中处理和利用上下文信息。\n",
    "\n",
    "- D2LLM: Decomposed and Distilled Large Language Models for Semantic Search\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题中的\"D2LLM\"（Decomposed and Distilled Large Language Models）直接暗示了对大型语言模型（Large Language Models，LLMs）的分解（Decomposed）和提炼（Distilled）过程。这表明论文可能深入探讨了LLMs的内部结构、组件以及它们如何协同工作以实现特定任务，如语义搜索（Semantic Search）。通过分解和提炼模型，研究者可能试图揭示LLMs的内在机制，包括但不限于内部block、attention结构等关键组件如何影响模型的性能和决策过程。这种分析有助于人们更好地理解LLMs的工作原理，提高模型的可解释性，并可能为优化和改进这些模型提供理论基础。因此，该论文摘要与Large language models的可解释性和理解LLM内在机制相关。\n",
    "\n",
    "- Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到预训练语言模型（Pre-trained Language Models, LLMs）的增量学习（Incremental Learning），这与大型语言模型的可解释性和理解其内部机制密切相关。论文可能探讨了如何在现有的预训练模型基础上进行微调以实现特定任务，从而揭示或增强模型在不同任务中的理解和学习过程。通过关注“Learn or Recall？”这一问题，研究者可能试图区分预训练模型是基于记忆（Recall）还是基于理解（Learn）来进行任务处理，这直接关联到模型内部的block、attention结构以及这些组件如何共同作用以支持模型的学习和推理能力。因此，该论文旨在提供对预训练语言模型内在机制的深入理解，特别是它们如何在面对新任务时展现出学习或回忆的能力，这对于开发更可解释、更高效且能够更好地适应特定应用场景的LLMs具有重要意义。\n",
    "\n",
    "- Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs”（小模型，大见解：利用精简代理模型为LLMs决定何时及何物进行检索）表明，它探讨的是如何使用较小的模型来提高大型语言模型（LLMs）的性能，特别是在决定何时以及从何处检索信息方面。这实际上涉及到了对LLMs内部机制的理解和可解释性。\n",
    "\n",
    "论文的重点在于“代理模型”（Proxy Models），这通常指的是用于简化复杂问题、帮助理解和优化原始模型的辅助模型。在这一上下文中，代理模型被用来帮助LLMs更好地决定何时需要访问外部资源（如知识库或数据库）以补充其生成的文本，以及具体需要从这些资源中检索哪些信息。\n",
    "\n",
    "通过研究和应用这样的代理模型，可以增加对LLMs决策过程的理解，特别是它们如何处理特定任务时的信息需求和选择。这种分析不仅有助于优化模型性能，还能够提供有关模型内部工作原理的洞察，包括注意力机制、不同组件之间的交互以及模型如何分配计算资源等关键方面。\n",
    "\n",
    "因此，这个论文确实与大型语言模型的可解释性和理解其内在机制相关，尤其是从代理模型的角度探索LLMs的内部结构和决策过程。\n",
    "\n",
    "- Interpretability of Language Models via Task Spaces\n",
    "  回复: 是。\n",
    "\n",
    "理由：这个论文标题表明它探讨的是语言模型的可解释性问题，具体通过“任务空间”（Task Spaces）来进行分析。在大型语言模型（Large Language Models, LLMs）的研究中，提高模型的可解释性是一个重要方向，因为它可以帮助研究人员和开发者更好地理解模型的工作原理、决策过程以及为何模型在特定任务上的表现优于其他模型。通过分析任务空间，研究者可以揭示模型在处理不同任务时的行为模式、偏好和局限性，从而提供对LLMs内部结构和机制的洞察。\n",
    "\n",
    "解读：论文标题中的“Interpretability”直接指向了语言模型的可解释性问题，这是理解LLMs的关键方面之一。通过“Task Spaces”，作者可能试图从不同的任务视角来解析模型的行为，这有助于构建一个更全面的模型理解框架。这样的研究对于促进LLMs的发展、应用以及潜在的改进至关重要，因为它不仅提供了理论上的见解，还为实际应用中模型的优化和调整提供了指导。\n",
    "\n",
    "- Navigating the OverKill in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《在大型语言模型中的过度杀戮》表明它可能探讨了大型语言模型（Large Language Models，LLMs）中的某些复杂或过拟合的问题。\"OverKill\"一词在数学和计算机科学领域中通常被用来描述解决一个问题时使用了过于复杂或强大的工具，而实际上问题可以通过更简单的方法解决。在机器学习和深度学习的语境下，这可能意味着作者在研究大型语言模型是否存在过度复杂化的情况，即模型的大小或复杂度是否超过了其实际任务需求。\n",
    "\n",
    "由于论文题目提及了“OverKill”，这暗示着论文可能深入探讨了大型语言模型的内部结构、工作原理，以及如何优化这些模型以避免不必要的复杂性。这直接与大型语言模型的可解释性有关，因为理解模型如何做出决策、为何选择特定的参数配置或架构对于提高模型效率和性能至关重要。此外，论文也可能探讨了如何通过分析模型的内部组件（如blocks、attention机制等）来提高我们对LLMs的理解，进而改进模型的设计和应用。\n",
    "\n",
    "因此，根据题目内容推测，这篇论文很可能涉及了大型语言模型的可解释性、理解其内部机制，以及优化模型设计等方面的研究，符合关于LLM内在机制、解释以及帮助人理解LLM内部结构的主题。\n",
    "\n",
    "- NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents\n",
    "  回复: 是。\n",
    "\n",
    "理由及解读：这篇论文的标题“NextLevelBERT”表明它可能是在现有BERT模型的基础上进行了一些改进或扩展。在摘要中提到的“Masked Language Modeling with Higher-Level Representations for Long Documents”，即针对长文档的具有更高层次表示的掩码语言建模，这一主题直接与大型语言模型（Large language models）的可解释性和理解其内部机制相关。\n",
    "\n",
    "在深度学习领域，尤其是自然语言处理（NLP）中，大型语言模型（如BERT、GPT等）通常被设计为能够处理长文本输入，并通过复杂的神经网络结构生成高质量的文本输出。然而，这些模型的内部工作原理往往较为复杂，缺乏直观的解释，这限制了我们对它们决策过程的理解和优化。\n",
    "\n",
    "针对这一点，“NextLevelBERT”这样的研究尝试引入“更高层次的表示”来改善模型对长文档的处理能力，同时可能也试图提高模型的可解释性。通过使用更高级别的抽象表示，研究者可能希望使得模型在处理长文档时能够更好地捕捉上下文信息，同时也可能提供了一种途径来理解模型是如何做出特定决策的。这种提升不仅增强了模型的功能性，还增加了我们对其内部工作原理的理解，这对于进一步优化模型性能和应用至关重要。\n",
    "\n",
    "综上所述，该论文摘要与大型语言模型的可解释性和理解其内在机制密切相关，因为它涉及到通过引入更高层次的表示来改善模型处理长文档的能力，并可能涉及提供更深入的见解以增强我们对模型决策过程的理解。\n",
    "\n",
    "- When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它探讨了如何通过“重启增量性”（Restart-Incrementality）来解释Transformer模型在处理局部歧义时的工作原理。Transformer模型是大型语言模型（Large Language Models, LLMs）的一种典型实现，其内部结构包括多层自注意力机制（attention-based structure）以及前馈神经网络等组件。因此，这篇论文关注于解释Transformer模型的内部工作过程，特别是如何处理文本中的模糊或歧义部分。\n",
    "\n",
    "“重启增量性”可能指的是在模型处理文本序列时，通过在特定阶段重新开始（即“重启”）并逐步增加信息（即“增量性”）的方式来提高模型对局部信息的理解和处理能力。这种策略有助于模型更准确地捕捉到上下文中的细微差异，从而更好地解析具有歧义的短语或句子。\n",
    "\n",
    "论文的主旨与大型语言模型的可解释性和理解其内在机制密切相关，因为它试图深入探索和揭示Transformer模型如何在复杂语言环境中进行决策的过程。通过这样的研究，可以帮助人们更好地理解这些模型的工作原理，增强我们对它们的信心，并指导未来的模型设计和应用。\n",
    "\n",
    "- SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“SpaRC 和 SpaRP：空间推理表征与路径生成，用于理解大型语言模型的空间推理能力”明确表明了其关注点在于理解和分析大型语言模型（Large Language Models, LLMs）在空间推理任务上的表现和能力。具体来说，它探讨了如何通过“Spatial Reasoning Characterization”（空间推理表征）和“Path Generation”（路径生成）来评估和解释LLMs在处理空间推理问题时的内在机制和性能。这直接涉及到了对LLMs内部结构的理解以及如何解释它们在特定任务（如空间推理）上的决策过程，因此与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- Spectral Filters, Dark Signals, and Attention Sinks\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，“谱滤波器”（Spectral Filters）、“暗信号”（Dark Signals）和“注意陷阱”（Attention Sinks）这几个概念都与神经网络（包括语言模型）的行为和性能分析有关。\n",
    "\n",
    "1. **谱滤波器**：在神经网络中，谱滤波器常用于理解网络如何对输入数据进行频率分解，以及不同层在网络中的作用。对于语言模型而言，这可能意味着研究模型如何处理不同长度或频率的语言模式，从而提高生成文本的质量或理解力。\n",
    "\n",
    "2. **暗信号**：在深度学习模型中，暗信号通常指的是那些对模型输出贡献较小或几乎不贡献的信号。在LLMs中，理解暗信号可以帮助我们识别哪些输入特征或中间表示对最终预测贡献不大，从而优化模型效率或理解模型决策过程。\n",
    "\n",
    "3. **注意陷阱**：注意机制是许多LLMs的关键组件，允许模型在处理输入时聚焦于特定部分。注意陷阱可能是指某些情况下，模型可能会过度依赖特定的注意力分配而忽视其他重要信息，导致模型性能下降或难以解释其决策过程。\n",
    "\n",
    "因此，这篇论文似乎探讨了通过分析这些概念来增强对LLMs的理解和解释，特别是关于它们的频率处理能力、信号贡献度以及注意力机制的有效性等方面。这种研究有助于提升我们对LLMs内部工作原理的认识，从而改进模型设计和应用。\n",
    "\n",
    "- RelayAttention for Efficient Large Language Model Serving with Long System Prompts\n",
    "  回复: 是。\n",
    "\n",
    "理由及解读：这篇论文的标题“RelayAttention for Efficient Large Language Model Serving with Long System Prompts”表明它探讨的是如何使用一种名为“RelayAttention”的方法来提高大型语言模型（Large Language Models, LLMs）在处理长系统提示时的服务效率。这一主题直接与大型语言模型的内部结构和工作原理有关，特别是与注意力机制（attention mechanism）如何影响模型性能和可解释性相关。\n",
    "\n",
    "大型语言模型通常依赖于复杂的多层神经网络架构，其中注意力机制是一种关键组件，用于决定哪些输入信息对生成的输出最相关。通过优化注意力机制，可以提高模型的计算效率，同时保持或增强其性能。因此，“RelayAttention”可能旨在提供一种更高效的方式来处理信息流，特别是在面对长文本输入时，这通常对模型的解释性和处理复杂任务的能力有重大影响。\n",
    "\n",
    "论文的主题与大型语言模型的内部块（如Transformer架构中的多头注意力机制）和注意力结构有关，这些都是理解LLM内在机制的关键方面。通过改进这些核心组件，研究者可以提升模型的可解释性，使人们能够更好地理解模型是如何作出决策的，以及为什么在特定情况下表现出某种行为。此外，这样的改进也可能有助于开发出更易于理解和维护的模型，这对于实际应用中的广泛部署至关重要。\n",
    "\n",
    "总之，该论文的主题与大型语言模型的可解释性及其内部机制紧密相关，尤其是关注如何通过优化注意力机制来提高模型的效率和服务质量。这不仅对于理论研究具有重要意义，也为实际应用提供了改进模型性能和增强用户理解的途径。\n",
    "\n",
    "- Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《通过链式知识提示增强语言模型推理能力》表明了它探讨的是如何提升大型语言模型（Large Language Models, LLMs）的推理能力。在LLMs的领域中，提高可解释性和理解模型内部机制是一个关键的研究方向。论文提到的“链式知识提示”方法可能是为了展示或增强模型对特定任务的理解过程，比如通过引导模型访问或利用额外的知识源来改善其决策过程。这种方法有助于研究者和实践者更好地理解模型是如何做出决策的，以及它们在不同情境下的表现和局限性。\n",
    "\n",
    "通常，这样的工作会涉及分析模型的内部组件，如注意力机制（attention mechanisms）、模型的层次结构（hierarchical structure）等，以揭示模型处理信息的方式和其推理能力的来源。因此，这个论文摘要与Large language models的可解释性或理解LLM内在机制相关，旨在帮助人们更好地理解这些模型的内部结构和工作原理。\n",
    "\n",
    "- Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目中提到“关注相关性的转移”以及“自由形式大型语言模型的预测不确定性量化”，这表明研究可能探讨了如何更深入地理解LLMs在处理不同任务时的决策过程，特别是它们在预测时的不确定性。这种不确定性量化对于评估模型的可靠性和可靠性至关重要，尤其是在应用到重要领域如医疗、法律等时。\n",
    "\n",
    "通常，大型语言模型通过注意力机制来分配对输入序列中不同元素的关注度，以决定哪些信息对生成的输出最重要。因此，关注相关性的转移可能涉及研究模型是如何根据输入内容调整其注意力分布的，以及这些变化如何影响模型的预测输出和不确定性。\n",
    "\n",
    "论文可能还探讨了如何通过量化预测不确定性来提高LLMs的透明度和可解释性，这对于用户和研究人员更好地理解模型的工作原理、识别潜在的偏差和错误至关重要。通过这种方式，研究有助于推动LLMs从黑箱模型向更加可解释和可控的模型发展，从而增强人类对这些复杂系统的理解和信任。\n",
    "\n",
    "总之，该论文聚焦于提升大型语言模型的可解释性，通过关注相关性的转移和预测不确定性量化，为理解LLMs的内部工作机理提供了新的视角。\n",
    "\n",
    "\n",
    "- Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到“多模态关联”和“多模态序列学习”，这通常与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。尤其是，“注意力机制”（Attention Mechanism）是现代神经网络模型中用于处理序列数据的关键组件之一，在LLMs中尤其重要，因为它帮助模型在生成文本时考虑不同位置的单词之间的关系。\n",
    "\n",
    "论文题目中的“通用化注意结果对齐”（Generalizable Attentional Results Alignment）表明研究可能集中在如何通过调整或优化注意力机制来提高模型在处理不同模态数据（如文本、图像等）时的一致性和有效性。这种对齐可能是为了增强模型在跨模态任务（如文本到图像生成、问答系统等）中的表现，从而提供更深入的理解和解释LLMs在这些任务上的工作原理。\n",
    "\n",
    "因此，该论文似乎关注的是如何通过改进和理解LLMs中的注意力机制来提高它们在多模态学习任务中的性能和可解释性，这与LLMs的内部结构和解释相关。\n",
    "\n",
    "- What Do Language Models Hear? Probing for Auditory Representations in Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题和摘要表明它探讨了语言模型（Language Models）中的听觉表示（Auditory Representations）。虽然“听觉”可能在直觉上与大型语言模型（Large Language Models，LLMs）的文本处理功能相联系，但这里的“听觉”实际上指的是通过分析模型对声音或语音信号的反应来探查其内部表示和决策过程。这种研究方法通常涉及到将语言模型应用于处理音频数据，比如语音识别任务，或者通过改变输入的声音特征来观察模型输出的变化，以此来理解模型如何处理这些声音信息。\n",
    "\n",
    "这样的研究有助于提升我们对LLM内部工作机制的理解，特别是它们如何处理非文本形式的信息，以及这些处理方式与人类语言理解之间的相似性和差异性。通过这样的研究，可以为改进语言模型的设计、增强模型的可解释性，以及开发更通用的语言处理技术提供理论基础和实践指导。\n",
    "\n",
    "因此，这个论文与大型语言模型的可解释性或理解其内在机制直接相关，因为它提供了关于模型如何处理和理解非传统文本形式信息的洞察。\n",
    "\n",
    "- Jailbreak Open-Sourced Large Language Models via Enforced Decoding\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到几个关键概念与大型语言模型（Large Language Models, LLMs）的理解和解释相关性。具体来说，“Jailbreak”通常意味着绕过限制或保护措施来访问系统内部。在这个上下文中，它可能指的是通过某种方法解锁或揭露大型语言模型内部的工作机制，特别是通过强制解码（Enforced Decoding）技术。强制解码可能是指一种策略或过程，用于迫使模型以特定方式工作或揭示其决策过程中的某些细节。\n",
    "\n",
    "大型语言模型的可解释性和理解其内在机制是一个重要研究领域，因为这有助于提高我们对这些模型如何生成文本、理解和处理语言的理解，以及它们在不同任务上的决策过程。这种理解对于改进模型性能、确保其可靠性和安全性、以及开发新的应用（如教育、心理咨询等）都至关重要。\n",
    "\n",
    "因此，一个关于通过强制解码公开大型语言模型的研究，很可能会探讨如何提高LLMs的可解释性，提供对模型内部结构和工作原理的洞察，以及展示如何使用这些见解来增强模型的透明度和用户信任。这符合题目中提到的“解释LLM的内部结构和解释的”这一要求。\n",
    "\n",
    "- Digital Socrates: Evaluating LLMs through Explanation Critiques\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题中的 \"Digital Socrates\" 暗示了通过对话或交互的方式评估大型语言模型（Large Language Models, LLMs）。论文名称和主题表明，研究可能涉及利用解释方法来评估和理解大型语言模型的能力。这包括从模型的内部组件（如block、attention结构）出发，探讨模型如何处理信息、生成文本以及决策过程。通过“Explanation Critiques”，研究者可能旨在提供一种方法论，帮助人们更好地理解大型语言模型的内部工作原理，揭示其决策背后的逻辑，以及评估其在不同任务上的表现和局限性。因此，该论文摘要与大型语言模型的可解释性和理解其内在机制密切相关。\n",
    "\n",
    "- Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题中明确提到了“Language-Specific Neurons”，这表明研究内容可能涉及在大型语言模型（Large Language Models, LLMs）内部寻找与特定语言相关的神经元，以揭示其在多语言能力方面的关键机制。这一主题直接与大型语言模型的可解释性和理解其内部机制相关，特别是如何从模型的内部组件（如特定的神经元、block或attention结构）来解释模型的能力和行为。通过关注这些语言特定的神经元，研究可以帮助人们更好地理解大型语言模型是如何处理不同语言的输入和输出的，从而提高我们对LLMs内在工作原理的理解。\n",
    "\n",
    "- Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：这篇论文标题“Masked Thought”表明它探讨了通过简单地隐藏部分推理步骤来提高语言模型在数学推理学习中的能力。这实际上涉及到了对大型语言模型（Large Language Models, LLMs）内部机制的理解和解释。在深度学习领域，特别是自然语言处理中，语言模型通常会通过复杂的内部结构（如多层神经网络、注意力机制等）进行信息处理和生成任务。将部分推理步骤“掩码化”（masking），即隐藏某些推理过程，是一种探索模型如何理解和处理信息的方法，有助于揭示模型在处理特定任务时的内在逻辑和决策过程。这种方法可以帮助研究者更好地理解模型的工作原理，包括其内部的block和attention结构如何协同作用以完成任务，从而提高模型的学习效率和性能。因此，这篇论文与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题“SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning”（SEER：通过强化学习促进结构化推理和解释）表明其主要关注于如何通过强化学习（Reinforcement Learning, RL）来促进大型语言模型（Large Language Models, LLMs）的结构化推理和解释。这直接关联到LLMs的可解释性和理解其内部机制，尤其是如何通过RL优化模型的决策过程，使得模型的推理步骤和最终结果能够被更好地理解和解释。因此，此论文与LLMs的可解释性和理解其内在机制相关。\n",
    "\n",
    "解读：在深度学习领域，特别是自然语言处理任务中，大型语言模型常常因其强大的生成能力和复杂的内部结构而受到关注。然而，这些模型通常被认为是“黑箱”，即缺乏透明度和可解释性，使得人们难以理解它们是如何做出特定决策的。通过引入强化学习等方法，研究者试图增强模型的决策过程的可解释性，使其更易于理解和分析。SEER论文正是在这一背景下进行的研究，它尝试利用强化学习来提升大型语言模型的结构化推理能力，并提供一种机制来解释模型的决策过程。这样的工作对于提高我们对LLMs的理解、改进模型设计以及增强模型在实际应用中的信任度具有重要意义。\n",
    "\n",
    "- TasTe: Teaching Large Language Models to Translate through Self-Reflection\n",
    "  回复: 是。\n",
    "\n",
    "理由：这篇论文的标题“TasTe: 教育大型语言模型通过自我反思进行翻译”表明它探讨了如何通过某种形式的教学方法来提升大型语言模型（Large Language Models, LLMs）的翻译能力。在这个过程中，“自我反思”这一概念暗示了作者可能试图揭示或增强LLMs在处理语言任务时的内部决策过程和机制。通过自我反思，模型可以更好地理解其生成输出的逻辑和上下文，从而提高翻译质量。这与大型语言模型的可解释性、理解其内在机制以及帮助人类理解LLMs的内部结构和工作原理相关联。因此，这篇论文很可能深入探讨了LLMs的内部结构、工作原理以及如何通过特定训练策略来改善它们的理解和解释能力。\n",
    "\n",
    "- Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《并非所有专家都是平等的：混合专家大型语言模型中的高效专家修剪和跳过》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内部机制相关。题目中提到的“专家”在深度学习领域通常指的是在特定任务或数据子集上有专门知识的模型部分，而“混合专家”模型则是将多个这样的专家组合在一起，以实现更强大的泛化能力和解决复杂任务的能力。通过修剪和跳过不重要的专家，可以提高模型的效率和性能。\n",
    "\n",
    "论文可能探讨了如何通过分析和理解这些专家的贡献来优化大型语言模型的结构，从而提高模型的效率和对某些任务的处理能力。这种研究有助于深入理解大型语言模型的工作原理，特别是它们如何在处理文本生成、问答、翻译等任务时作出决策。通过提供对模型内部机制的洞察，该研究能够帮助研究人员和开发者更好地设计和优化此类模型，使其更加高效且易于理解。\n",
    "\n",
    "因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制相关，因为它涉及到了通过分析和优化模型组件来改进模型性能的方法，这正是理解LLM内部结构和解释其工作方式的关键步骤。\n",
    "\n",
    "- Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内部机制相关。具体来说，它探讨了当知识冲突时，语言模型如何融合生成的上下文与检索到的上下文的问题。这涉及到对LLMs内部结构的理解，特别是它们如何处理不同来源的信息以形成一致的输出。研究可能关注的是LLMs中的特定组件（如内部块、注意力机制等），以及这些组件在处理知识冲突时的交互方式。因此，这篇论文旨在增进我们对LLMs内部工作原理的理解，尤其是它们在决策过程中如何整合不同类型信息的能力，这对于开发更可解释和可靠的AI系统至关重要。\n",
    "\n",
    "- Unveiling Linguistic Regions in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目“Unveiling Linguistic Regions in Large Language Models”直译为“揭示大型语言模型中的语言区域”，这表明作者试图探索和展示LLMs中特定的语言结构或区域。这样的研究通常涉及到对LLMs内部工作机制的理解，比如通过分析模型的不同组件（blocks）、注意力机制（attention structures）等，来解释模型如何处理和生成语言。这种研究有助于人们更好地理解LLMs是如何工作以及它们在不同任务上的表现背后的原理，从而提高我们对这些复杂模型的可解释性和透明度。\n",
    "\n",
    "- Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解LLM内在机制相关。具体来说，它探讨了语言模型对话语关系的理解忠实度（faithfulness）。在自然语言处理领域，语言模型通常被用来生成文本、回答问题或是进行对话等任务。然而，由于这些模型通常是基于大量数据训练的复杂神经网络，它们的决策过程往往缺乏透明度，即我们很难理解模型是如何得出特定输出的。\n",
    "\n",
    "“Discursive Socratic Questioning”方法通过提出一系列问题，旨在评估模型在处理不同话语关系时的准确性和一致性。这种评估方式可以帮助研究者和开发者更好地理解模型在处理特定类型语料时的性能，以及它对话语规则和逻辑的掌握程度。通过这种方式，可以增加模型的可解释性，使得人们能够更深入地理解模型的内部结构和决策过程，这对于改进模型、提高其性能以及在实际应用中安全、可靠地使用这些模型至关重要。\n",
    "\n",
    "因此，这个论文题目和内容与探索和解释大型语言模型的内部结构和机制紧密相关，特别是关注于提升模型在理解和生成符合特定语法规则和逻辑关系文本方面的准确性。\n",
    "\n",
    "- Unlearning Traces the Influential Training Data of Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它探讨的是语言模型中\"遗忘\"训练数据的影响。在大型语言模型（Large Language Models, LLMs）的背景下，这涉及到模型如何学习和处理信息，以及它如何“忘记”或不再依赖于某些特定的训练数据。这种研究对于理解LLMs的可解释性至关重要，因为它揭示了模型决策背后的机制。通过追踪模型对训练数据的依赖，研究人员可以提供关于模型内部工作原理的见解，包括模型的注意力机制、内部块（如Transformer中的多头自注意力层）等关键组件如何影响模型的表现和决策过程。这样的分析有助于提升我们对LLMs复杂内部结构的理解，从而增强其可解释性和可控性。\n",
    "\n",
    "- Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。具体来说，论文探讨了输入长度对大型语言模型推理性能的影响，这涉及到对模型内部结构的理解和解释。在深度学习领域，特别是对于大规模语言模型，理解模型如何处理不同长度的输入以及这些输入如何影响模型的输出是一个关键问题。通过研究输入长度如何影响模型的推理性能，研究者可以提供关于模型内部机制的见解，比如模型在处理不同长度的输入时是如何分配注意力的，以及在哪些部分发生了信息丢失或增强。这种分析有助于提高我们对大型语言模型工作原理的理解，从而促进更有效的模型设计和应用。\n",
    "\n",
    "- LLaMA Pro: Progressive LLaMA with Block Expansion\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题 \"LLaMA Pro: Progressive LLaMA with Block Expansion\" 提示了这篇论文可能探讨的是如何通过逐步扩展块（Block Expansion）来改进 LLaMA 模型。由于大型语言模型（Large Language Models，简称LLMs）的可解释性和理解其内部机制是一个研究热点，尤其是因为这些模型通常设计复杂，包含多层结构和注意力机制等组件，使得它们的决策过程难以直接理解和解释。因此，如果这篇论文关注于分析、改进或增强LLM的内部结构（如 block 和 attention 结构），并提供了对这些结构如何影响模型性能的洞察，那么它确实与大型语言模型的可解释性和理解其内在机制有关。\n",
    "\n",
    "简而言之，如果论文内容涉及对 LLaMA 或类似模型内部组件的详细分析、如何通过调整这些组件（比如通过扩展块）来改善模型性能，以及这些调整如何影响模型的可解释性或我们对模型内部工作的理解，那么它就符合题干中提到的要求。因此，根据这个标题，我们可以合理推测这篇论文与大型语言模型的可解释性或理解其内在机制相关。\n",
    "\n",
    "- The Hidden Space of Transformer Language Adapters\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《Transformer语言适配器的隐藏空间》表明它探讨的是Transformer模型（一种大型语言模型）的内部结构和特定组件（在这里是“适配器”）的作用。Transformer模型是基于注意力机制构建的，通过多个编码器和解码器层来处理序列数据。适配器则是一种在这些层之上添加的轻量级结构，旨在增强模型对特定任务的学习能力，而无需重新训练整个模型。\n",
    "\n",
    "论文可能深入研究了以下几个方面：\n",
    "1. **Transformer内部机制**：解释Transformer如何通过注意力机制处理输入序列，以及每个层如何共同作用以生成最终的输出。\n",
    "2. **适配器的作用**：探讨适配器是如何被设计和插入到Transformer架构中，以及它们如何能够提高模型在特定任务上的表现而不增加过多的参数。\n",
    "3. **隐藏空间的分析**：分析适配器如何影响Transformer的“隐藏空间”，即模型内部表示学习的空间。这涉及到理解模型如何根据输入生成输出，以及这种生成过程中的非线性变换如何被适配器所影响。\n",
    "4. **可解释性和理解机制**：提供关于如何通过适配器和隐藏空间来更好地理解和解释Transformer模型内部工作原理的方法。这可能包括可视化技术、特征重要性分析等手段，以揭示模型决策背后的原因。\n",
    "\n",
    "因此，这篇论文与大型语言模型的可解释性或理解其内在机制直接相关，尤其是从Transformer的内部组件和结构角度出发。\n",
    "\n",
    "- A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制有关。论文标题中的 \"Locating and Detecting Language Model Grounding\" 暗示了研究可能涉及识别和检测语言模型在现实世界中的对应物或基础知识，这通常涉及到模型的可解释性问题。通过使用假维基百科（Fakepedia）这一工具，作者可能试图探索和揭示模型是如何获取、处理和生成文本信息的，以及这些过程背后的机制。这样的研究有助于增进人类对LLMs内部工作原理的理解，包括它们如何处理语言输入、执行任务以及生成输出文本的方式。因此，这个论文关注点与大型语言模型的内部结构、解释性和理解机制直接相关。\n",
    "\n",
    "- ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。标题“ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs”表明该研究旨在通过让不同的大型语言模型在圆桌会议上进行讨论并达成共识来提高推理能力。这涉及到对LLMs内部结构的理解以及如何通过这些模型之间的交互来解释它们的工作原理。研究可能探索了LLMs的不同组件（如内部block和attention结构）如何协同工作以实现特定任务，并且可能提出了一种方法来增强这些模型的决策过程的透明度和可解释性。因此，它与试图帮助人类理解LLMs内部工作机理的目标保持一致。\n",
    "\n",
    "- Large Language Models Are No Longer Shallow Parsers\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“大型语言模型不再是浅层解析器”表明它探讨了大型语言模型（Large Language Models，LLMs）在处理语言任务时的深度和复杂性，这与LLM的可解释性和理解其内在机制紧密相关。大型语言模型通常被认为是基于Transformer架构的，它们通过多层编码器和解码器来处理输入序列，每一层都包含了注意力机制（attention），使得模型能够关注输入序列的不同部分以生成输出。这种深层结构和注意力机制是LLM能够理解上下文、生成连贯文本的关键所在。\n",
    "\n",
    "论文标题中的“不再是浅层解析器”暗示了LLMs已经超越了简单的词法分析阶段，能够进行更深层次的语义理解和生成复杂的语言结构。这意味着研究可能深入探讨了LLM内部的结构和工作原理，比如不同层之间的信息传递、注意力机制如何增强模型的理解能力等，这些都是解释LLM内在机制的重要方面。\n",
    "\n",
    "因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制高度相关，它旨在帮助人们更好地理解这些模型是如何构建和工作的。\n",
    "\n",
    "- An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制有关。具体来说，摘要提到使用神经元激活作为统一视角来解释大型语言模型在解决连贯思考引导算术推理任务时的运作过程。这涉及到对LLMs内部结构的理解，特别是如何通过分析神经元激活模式来解释模型在处理特定任务时的决策过程。这种研究方法有助于人类更好地理解LLMs是如何处理信息、进行推理以及做出预测的，从而提升我们对这些模型内部工作机制的认知。因此，该论文聚焦于LLMs的可解释性，与主题相关。\n",
    "\n",
    "- Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要提到的研究主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，研究关注的是大型语言模型是否能够使用规则进行推理（Reasoning with Rules），并通过逻辑支架（Logic Scaffolding）来进行压力测试（Stress-Testing）和改进（Improving）这些模型。这涉及到深入理解LLMs如何处理逻辑结构和规则，以及它们在执行任务时的决策过程。因此，这项研究旨在帮助人们更好地理解大型语言模型的内部结构和工作原理，提高它们的性能和可靠性。通过分析模型对逻辑规则的处理能力，研究者可以提供更深入的见解，帮助提升LLMs在实际应用中的表现，同时也增强我们对这些复杂模型的理解。\n",
    "\n",
    "- EIT: Enhanced Interactive Transformer\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题“EIT: Enhanced Interactive Transformer”表明该研究可能关注于提升交互式Transformer模型的性能或特性。考虑到大型语言模型（Large Language Models, LLMs）通常基于Transformer架构构建，此论文摘要与大型语言模型的可解释性或理解其内部机制相关。特别是，如果该研究探讨了从Transformer的内部组件（如块、注意力机制）来解释LLMs的能力，或者旨在改进这些模型以增强人类对其决策过程的理解，那么它确实与大型语言模型的内部结构和解释相关。通过提升模型的交互性，研究者可能试图提供更直观的解释，使人们能够更好地理解模型如何处理和生成文本，以及在不同情境下的决策逻辑。\n",
    "\n",
    "- MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题\"MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\"（MARS：基于意义的响应评分用于生成式LLM的不确定性估计）表明它关注于如何通过评估生成文本的意义来估计大型语言模型（LLMs）的不确定性。这意味着该论文涉及了对LLM内部结构的理解和解释，特别是从生成文本的质量和意义角度出发，探讨了模型在不同情况下的表现和不确定性的评估方法。\n",
    "\n",
    "具体来说，论文可能探索了以下几个方面：\n",
    "1. **内部Block和Attention结构**：通过分析模型的内部组件（如Transformer架构中的多头注意力机制、前馈网络等），了解这些结构如何影响生成文本的意义和不确定性。\n",
    "2. **意义感知的响应评分**：开发了一种评分机制，能够根据生成文本的意义来评估模型的置信度或不确定性。这涉及到理解模型如何处理不同词汇和语法结构，并如何在不同的上下文中产生意义。\n",
    "3. **解释LLM的内部结构**：通过研究上述评分机制及其与模型内部结构的关系，论文可能提供了一些关于LLM如何工作以及它们在处理不同类型任务时表现差异的见解。\n",
    "\n",
    "总之，该论文与大型语言模型的可解释性和理解其内部机制直接相关，因为它不仅关注于提高模型的性能，还深入探讨了模型的决策过程和不确定性评估，这对于促进对LLM内在机制的理解至关重要。\n",
    "\n",
    "- Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment\n",
    "  回复: 是。\n",
    "\n",
    "这篇论文的标题《大型语言模型是所有字符的叠加：通过自我对齐实现任意角色扮演》表明它探讨了大型语言模型（Large Language Models, LLMs）在角色扮演方面的特性，这涉及到LLM内部结构和工作机制的理解。具体来说，论文关注的是LLMs如何能够以任意方式执行角色扮演任务，这通常需要模型理解上下文、生成符合特定角色的语言输出。这种能力的实现往往与模型内部的注意力机制（attention）、多层结构等关键组件紧密相关。因此，论文内容与大型语言模型的可解释性以及理解其内在机制相关，特别是从内部结构和工作原理的角度来解释模型的能力。\n",
    "\n",
    "解释：\n",
    "- **角色扮演**：在自然语言处理中，角色扮演通常指的是模型能够根据给定的角色和场景生成相应的话语或文本，这要求模型具有强大的语义理解和生成能力。\n",
    "- **自我对齐**：这一概念可能指的是模型内部如何通过某种方式调整或优化其参数，使得在面对不同任务或角色时，能够有效地整合和利用其内部知识和能力。这可能涉及到注意力机制的调整、模型内部表示的重新组织等。\n",
    "- **大型语言模型的内部结构**：大型语言模型通常由多层神经网络组成，每一层都可能包含多种机制，如注意力机制，用于捕捉输入序列之间的依赖关系。理解这些内部结构对于解释模型的行为至关重要。\n",
    "- **可解释性**：对于研究领域而言，提高模型的可解释性意味着提供方法和工具，帮助研究人员和用户理解模型是如何做出决策的，这对于改进模型性能、确保其可靠性和公平性非常重要。\n",
    "\n",
    "综上所述，该论文的主题与大型语言模型的内部结构、解释性以及它们在复杂任务如角色扮演中的表现密切相关，因此是与LLM的可解释性和理解其内在机制相关的。\n",
    "\n",
    "- Calibrating Large Language Models Using Their Generations Only\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到使用大型语言模型（Large Language Models, LLMs）的生成内容来校准它们。这实际上是在探讨如何通过分析LLMs的内部工作原理和生成过程来提高其性能和可解释性。具体来说，研究可能关注于以下几个方面：\n",
    "\n",
    "1. **内部结构理解**：通过观察和分析模型在生成特定文本时的行为，可以更好地理解模型内部的block、注意力机制等关键组件是如何协同工作的。这有助于揭示模型在不同任务上的决策过程和偏好，从而提供对模型内部结构的洞察。\n",
    "\n",
    "2. **解释能力提升**：论文可能探讨了如何利用生成的内容来构建更有效的解释工具，使得模型的决策过程能够被人类理解。例如，通过分析模型在生成特定文本时所依赖的输入特征或注意力模式，可以为模型的决策提供一种“透明度”，使人们能够理解模型是如何做出某些预测或生成特定输出的。\n",
    "\n",
    "3. **参数优化**：基于对生成内容的分析，研究者可能会提出策略来调整模型的参数，以改善模型的表现和泛化能力。这种基于生成内容的校准方法可以帮助模型更准确地适应特定任务需求，同时可能也提供了对其内部机制更深入的理解。\n",
    "\n",
    "4. **模型可靠性评估**：论文还可能关注如何通过生成内容来评估模型的可靠性或不确定性，这对于理解模型在不同情境下的表现以及潜在的偏差或局限性至关重要。\n",
    "\n",
    "综上所述，这篇论文的主题确实与大型语言模型的可解释性和理解其内在机制紧密相关，特别是从LLM的内部block、attention结构的角度出发，旨在提高模型的性能并增强其对人类的可解释性。\n",
    "\n",
    "- ANAH: Analytical Annotation of Hallucinations in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与Large language models（大语言模型）的可解释性或理解其内在机制相关。论文标题“ANAH: Analytical Annotation of Hallucinations in Large Language Models”表明，它探讨了在大型语言模型中幻觉的分析注释。这涉及到对大型语言模型内部结构的理解以及解释它们的工作方式，特别是在生成错误输出或“幻觉”时。\n",
    "\n",
    "大型语言模型通常基于深度学习技术，具有复杂的多层结构，包括注意力机制等高级组件。理解这些模型如何生成文本，以及它们在哪些情况下可能产生错误或不准确的输出，对于提高模型性能和应用安全性至关重要。通过分析模型在生成特定文本时的行为，研究人员可以揭示模型的决策过程，增强我们对模型内在机制的理解。\n",
    "\n",
    "因此，该论文的主题与大型语言模型的可解释性和理解其内部工作原理直接相关。通过研究模型在生成幻觉时的行为，可以提供有关模型内部结构和功能的重要见解，从而帮助人们更好地理解和控制这些模型。\n",
    "\n",
    "- Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“揭示语言模型的参数知识：一种统一的归因方法框架”表明它探讨了语言模型（Large Language Models, LLMs）的参数知识，这直接关联到LLMs的可解释性和理解其内在机制。通过提出一种统一的归因方法框架，作者旨在提供一种工具或方法来解释LLMs的工作原理，特别是如何处理和生成文本。这种研究对于帮助人们理解LLMs的内部结构和解释其决策过程非常重要，因为当前的LLMs通常被认为是“黑盒”模型，其内部工作原理对人类来说是不透明的。因此，这个论文与LLMs的可解释性相关，有助于增进我们对这些强大模型的理解。\n",
    "\n",
    "- Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这篇论文的标题《长上下文其实并不长：大型语言模型中的长期依赖数据勘探》表明它探讨了大型语言模型（Large Language Models, LLMs）处理长依赖关系的能力。在深度学习领域，特别是自然语言处理中，大型语言模型通常需要理解和生成包含多个时间步或句子长度的文本序列。然而，处理长距离依赖（即在序列中相距较远的元素之间的关系）是这些模型面临的一个挑战。\n",
    "\n",
    "论文标题中提到的“长上下文”可能指的是模型能够处理的输入序列长度。通常，大型语言模型可以处理的上下文长度有限，这限制了它们捕获长距离依赖关系的能力。然而，通过数据增强、特定的架构设计或其他技术，研究人员可能已经发现方法来提高模型处理长距离依赖关系的效率或效果。\n",
    "\n",
    "因此，这篇论文似乎关注的是如何改善大型语言模型对长依赖关系的理解和处理能力，这与大型语言模型的可解释性或对其内部机制的理解直接相关。它可能探索了模型内部的结构组件（如block、attention机制等），以解释模型如何在处理长序列时表现出其能力。这种研究对于帮助人们更好地理解大型语言模型的工作原理以及提升其性能具有重要意义。\n",
    "\n",
    "- Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明了它关注于通过让Transformer模型关注结构化量化嵌入来诱导系统性。Transformer是一种大型语言模型（Large Language Models，LLMs）的核心架构，它们在自然语言处理任务中表现出了强大的性能。论文的主题直接涉及到了LLM内部机制的理解和解释，特别是通过分析模型如何处理和利用特定类型的嵌入（即结构化量化嵌入）来增强其系统性（即模型能够泛化到未见过的数据的能力）。这种研究有助于人们更深入地理解LLM的工作原理，以及它们如何通过特定的设计决策来提升性能和泛化能力。因此，这个摘要与大型语言模型的可解释性和理解LLM内在机制密切相关。\n",
    "\n",
    "- Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“机制的竞争：追踪语言模型如何处理事实和反事实”似乎与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。标题中提到了“语言模型如何处理事实和反事实”，这表明研究可能探讨了LLMs在处理真实情况信息（事实）和假设情况信息（反事实）时的行为模式或决策过程。这种研究通常涉及到对模型内部结构、工作原理以及它们在特定任务上的表现进行深入分析。\n",
    "\n",
    "具体来说，论文可能会从LLM的内部组件（如块、注意力机制等）出发，探索这些组件如何协同作用来处理不同类型的输入，特别是在理解和生成事实和反事实内容时的表现。通过这样的研究，可以帮助人们更好地理解LLMs的内部结构和工作原理，增强对模型决策过程的透明度和可解释性。因此，该论文摘要与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题 \"OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models\" 直接指向了大型语言模型（Large Language Models, LLMs）的理论思维推理能力评估。理论思维（Theory-of-Mind, ToM）是一种认知能力，指的是个体理解他人观点、意图、信念以及情感的能力。在人工智能领域，特别是自然语言处理中，评估大型语言模型是否具备理论思维能力是一个重要的研究方向，因为它涉及到模型如何理解和生成具有复杂语境、意图和情感色彩的对话或文本。\n",
    "\n",
    "该论文旨在创建一个全面的基准测试，用于评估大型语言模型在理论思维推理方面的表现。这表明它关注于大型语言模型的可解释性和理解其内在机制。通过这样的评估，可以深入理解模型是如何处理和生成包含理论思维元素的内容的，以及它们在不同任务上的表现差异。这有助于研究人员和开发者更好地理解大型语言模型的内部结构、注意力机制（attention structures）和其他关键组件如何共同作用以支持这些高级推理能力。\n",
    "\n",
    "因此，该论文与大型语言模型的可解释性或理解其内在机制直接相关，提供了评估和分析模型在理解复杂人类行为和思想方面能力的方法，同时也为提升模型性能和优化其设计提供了有价值的洞见。\n",
    "\n",
    "- Steering Llama 2 via Contrastive Activation Addition\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题 \"Steering Llama 2 via Contrastive Activation Addition\" 提示了该研究可能涉及如何通过对比激活添加（contrastive activation addition）来引导或控制大型语言模型（LLM）的行为或性能。这暗示了研究者对 LLM 的内部工作原理有深入探索，尤其是关注其激活（activation）过程——这是神经网络中神经元响应输入信号后产生的输出值，在 LLM 中扮演着关键角色。\n",
    "\n",
    "解释 LLMS 内部结构和机制通常涉及理解它们是如何处理输入信息、执行语言生成任务以及在训练过程中学习到特定模式或概念的。通过对比激活添加方法，研究者可能旨在揭示 LLM 在不同任务或情境下激活模式的变化规律，从而提供一种手段来更精细地控制模型的行为。这种方法可能涉及到对 LLM 内部组件（如 block 或 attention 结构）的分析，以理解它们如何影响模型的决策过程和最终输出。\n",
    "\n",
    "因此，这个摘要与大型语言模型的可解释性和理解其内在机制相关，特别是从 LLM 的内部组件（如 block 和 attention 结构）的角度出发，探讨如何增强我们对这些模型的理解和控制能力。\n",
    "\n",
    "- RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题\"RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations\"表明其主要关注于评估解释性方法在分离语言模型表示方面的应用。这直接涉及到对大型语言模型（Large Language Models, LLMs）的可解释性问题，即如何理解和解释LLMs的内部工作原理和决策过程。通过研究如何分解（disentangle）LLMs的表示，该论文旨在帮助人们更好地理解这些复杂模型的内部结构和决策机制，从而提高我们对LLMs内在机制的理解。\n",
    "\n",
    "解读：论文标题中的“Interpretability”一词明确指出了其内容与LLMs的可解释性有关，而“Disentangling Language Model Representations”则进一步说明了研究将聚焦于LLMs表示的分解，这正是探索LLMs内部结构和解释其工作原理的关键步骤。因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- I am a Strange Dataset: Metalinguistic Tests for Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《我是一个奇怪的数据集：语言模型的元语言测试》与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。题目中提到的“元语言测试”（Metalinguistic Tests）通常涉及到对模型在理解和使用语言规则、语法、修辞等方面的能力进行评估。通过设计特定的数据集或任务来测试模型的这些元语言能力，研究者可以更好地理解模型是如何处理语言的复杂性、语境信息以及不同语言现象的。\n",
    "\n",
    "大型语言模型的内部结构通常包括多层的神经网络结构，如Transformer模型中的自注意力机制（Attention Mechanism）。这些模型通过学习大量文本数据，能够生成连贯的文本并解决各种自然语言处理任务。然而，由于它们的复杂性和“黑盒”性质，理解模型是如何做出特定决策或生成特定输出的往往具有挑战性。因此，开发方法来解释模型的行为、决策过程以及它们如何处理不同类型的输入变得尤为重要。\n",
    "\n",
    "元语言测试可以提供一种途径，通过评估模型在处理特定语言任务时的表现，来间接了解其内部机制。例如，测试模型在生成遵循特定语法规则的句子、识别或创造复杂的语言结构等方面的能力，可以帮助研究者洞察模型对于语言规则的理解深度，以及它如何利用上下文信息进行推断。这种测试结果不仅有助于提高我们对LLMs的理解，还能为改进模型的可解释性和设计更高效、更可靠的自然语言处理系统提供指导。\n",
    "\n",
    "- Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。标题中提到的“Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models”涉及到LLMs在知识编辑过程中利用多跳事实捷径的能力。这实际上是在探讨如何通过修改或编辑LLM的知识库来增强它们在处理复杂问题时的性能和效率。重点在于理解LLM如何基于先前学习到的信息和模式进行推理，以及如何通过引入新的事实信息来优化其决策过程。这种研究有助于深入理解LLM的内部结构和工作原理，特别是它们在进行多步推理时是如何构建和利用知识的。因此，这个题目直接关联到了LLM的可解释性和对其内在机制的理解。\n",
    "\n",
    "- Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题“Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations”表明该研究旨在通过数学操作（Math-Shepherd）来验证和强化大型语言模型（LLMs）的性能，而无需人工注释。这直接涉及到对LLMs内部结构和机制的理解以及可解释性。通过逐步（Step-by-step）的方法，该研究可能探索了LLMs处理数学问题的能力，从而提供了对这些模型如何进行计算、推理和决策过程的洞察。此外，“without Human Annotations”强调了自动化验证和增强方法，这通常意味着对模型内部逻辑和决策过程的深入分析，以提高模型的可靠性和透明度。因此，这个论文摘要与大型语言模型的可解释性或理解其内在机制相关，特别是从模型的内部组件、注意力机制等方面进行探讨，以帮助人们更好地理解LLMs的工作原理。\n",
    "\n",
    "- Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“通过自我奖励对比提示精炼实现大型语言模型的直接对齐”表明它探讨了如何通过特定的技术方法（自我奖励对比提示精炼）来增强大型语言模型（LLM）的性能和可解释性。论文的重点似乎是将这些模型的输出与某个目标或期望的输出进行对齐，这通常涉及到调整模型参数以提高其在特定任务上的表现。在这一过程中，了解和解释模型内部的决策过程变得尤为重要，因为这不仅有助于提升模型的性能，还能增加人们对模型工作原理的理解。\n",
    "\n",
    "论文中的“自奖励对比提示精炼”这一技术可能涉及到了对模型内部组件（如block、attention结构等）的操作，以优化其在给定任务上的表现。这种技术可能包括调整模型的注意力机制、改变其内部层的权重，或者在训练过程中引入特定的激励信号，以便让模型更好地理解和生成符合预期的语言输出。\n",
    "\n",
    "因此，这个论文与大型语言模型的可解释性和理解其内在机制相关，因为它探索了如何通过具体的技术手段改进模型性能的同时，也提供了对模型内部结构和决策过程的洞察。\n",
    "\n",
    "- DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题和摘要表明了研究者试图通过增加某些元素（Augmenting）来增强大型语言模型（LLM）在逻辑推理任务上的表现，特别是从不确定性（Indeterminacy）提升到确定性（Determinacy）。这暗示了研究可能涉及到对LLM内部结构和机制的理解，以及如何提高这些模型在特定任务（如逻辑推理）上的可解释性和性能。\n",
    "\n",
    "通常，提高LLM在特定任务上的性能需要深入理解它们的内部工作原理，包括但不限于其架构（如Transformer block）、注意力机制（Attention structure）以及如何处理不确定性和逻辑推理等问题。因此，如果该论文探讨了这些方面，它确实与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "具体来说，研究可能探索了如何设计或调整模型组件（如引入特定的编码器或解码器块、改变注意力权重计算方式等），以使模型在面对逻辑推理任务时能够更准确地从不确定性的输入中推导出确定性的结论。这种工作不仅有助于提高模型在实际应用中的表现，而且还能增进我们对LLM如何进行推理、如何处理不同类型的不确定性以及如何构建更强大和可解释的AI系统的理解。\n",
    "\n",
    "- Meta-Task Prompting Elicits Embeddings from Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，题目中提到了“Meta-Task Prompting”（元任务提示），这可能是一种策略或技术，用于从大型语言模型中提取或引发特定类型的表示（Embeddings）。Embeddings在自然语言处理中通常指的是将文本或词汇转换为数值向量的过程，这些向量能够捕捉到语义和上下文信息。这种技术对于理解LLMs如何处理和表示语言数据至关重要。\n",
    "\n",
    "此外，“Large Language Models”部分明确指出了研究对象的类型，即大型语言模型，这是当前自然语言处理领域关注的核心。大型语言模型通常具有复杂的内部结构，包括多层的Transformer block、注意力机制等，这些组件共同决定了模型的性能和能力。\n",
    "\n",
    "因此，该论文摘要与理解LLMs的内部结构和解释其工作原理相关，因为它探讨了如何从这些模型中提取有意义的信息，这有助于深入理解模型是如何学习和应用语言知识的。\n",
    "\n",
    "- Do Large Language Models Latently Perform Multi-Hop Reasoning?\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题“大型语言模型在潜意识中执行多跳推理？”（Do Large Language Models Latently Perform Multi-Hop Reasoning?）直接指向了大型语言模型（Large Language Models, LLMs）的内部工作机理和它们如何处理复杂任务的能力。这个题目暗示了研究者试图探索和解释LLMs在执行需要多个步骤或跳跃的推理任务时的内在机制。这涉及到对LLMs内部组件如块（blocks）、注意力（attention）结构等的分析，以理解它们如何支持和实现这种多跳推理能力。因此，这篇论文的相关部分很可能集中在解释LLMs的内部结构、它们如何处理信息以及这些处理方式如何支持多步推理过程，这些都是理解LLMs内部机制的关键方面。\n",
    "\n",
    "- MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：这个论文标题“MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models”表明它探讨的是如何通过知识图谱提示激发大型语言模型中的思维图谱。这涉及到对大型语言模型（Large Language Models, LLMs）内部结构的理解以及它们在处理任务时的可解释性。知识图谱提示是一种策略，通过这种方式可以引导LLMs在生成文本时使用特定的知识或概念，从而提高输出的质量和相关性。这种研究有助于人们更好地理解LLMs是如何处理信息的，以及它们在不同情境下表现出的能力和局限性。因此，这个论文与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "- Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。标题“Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning”表明，该研究关注的是通过递归调用来克服大型语言模型在组合性方面的限制。这意味着作者试图探讨和改进LLMs如何处理复杂语言结构的能力，这通常涉及到对模型内部组件、注意力机制等关键元素的理解和优化。通过递归调用的方法，研究可能旨在深入理解模型是如何学习和表示语言的层次结构的，以及如何通过调整这些结构来提高模型的性能和可解释性。因此，该论文直接与大型语言模型的内部结构、工作原理及其可解释性有关。\n",
    "\n",
    "- HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题“HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts”（HyperMoE：通过专家之间的转移以实现更好的专家混合）涉及了大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制的主题。论文探讨的是如何通过在专家之间进行知识转移来改进混合专家（Mixture of Experts, MoE）模型的性能。专家模型是一种并行计算架构，其中多个子模型（即“专家”）独立处理数据的不同部分，然后将结果聚合起来形成最终输出。这种架构可以提高模型的效率和准确性，尤其是在处理大规模数据集时。\n",
    "\n",
    "在LLMs的背景下，理解模型的内部结构和决策过程对于提升模型性能、优化参数分配以及增强模型的可解释性至关重要。通过研究如何在专家之间有效地转移知识，HyperMoE论文为深入理解LLMs的内部工作原理提供了新的视角。这种知识转移策略有助于揭示模型如何处理不同的输入特征，从而增强我们对LLMs内在机制的理解。此外，通过提高模型的性能，该研究也间接促进了对LLMs可解释性的提升，因为更高效的模型通常更容易理解和分析。\n",
    "\n",
    "综上所述，HyperMoE论文与大型语言模型的可解释性或理解其内在机制紧密相关，因为它探索了改进模型性能的方法，这一过程涉及到对模型结构和决策过程的深入理解。\n",
    "\n",
    "- Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：这个论文标题“再问一次，然后失败：大型语言模型在判断上的摇摆不定”涉及到大型语言模型（Large Language Models, LLMs）在决策或判断时的不确定性或不稳定性的研究。这种研究通常会深入探索LLMs的内部结构和工作原理，以理解它们如何处理信息、做出决策以及在特定情况下表现出来的行为模式。特别是，它可能关注于LLMs的内部块（如Transformer层）、注意力机制（Attention）以及其他关键组件如何影响其性能和输出的可预测性。\n",
    "\n",
    "在这个上下文中，“可解释性”通常指的是研究者试图提高模型决策过程的透明度，使得人们能够理解模型是如何从输入生成输出的。对于LLMs来说，这可能包括分析它们在遇到不同输入时的行为变化、识别哪些部分的模型结构对最终输出有显著影响、以及探讨注意力机制如何引导模型关注输入中的关键元素等。\n",
    "\n",
    "因此，这个论文标题表明了对LLMs内部工作机制及其决策过程不确定性的研究兴趣，这正是与LLMs的可解释性和理解其内在机制相关的领域。通过这样的研究，可以增进人类对这些复杂模型的理解，从而提升我们利用它们的能力并解决可能出现的伦理、安全等问题。\n",
    "\n",
    "- CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：论文标题“CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models”表明，该研究关注于大型语言模型（Large Language Models, LLMs）在处理模糊信息需求时的能力识别和澄清。这直接与LLMs的可解释性和理解其内部机制相关。具体而言，论文旨在创建一个基准测试系统来评估和改进LLMs在处理语义模糊、多义性高或不明确的信息查询时的表现。通过这样的研究，可以增进对LLMs内部结构和工作原理的理解，特别是它们如何处理和生成文本，以及在面对复杂或模糊输入时的决策过程。这种分析有助于提高LLMs的透明度和可解释性，使人们能够更好地理解这些模型是如何做出决策的，从而增强它们在实际应用中的可靠性和有效性。\n",
    "\n",
    "- Revisiting Knowledge Distillation for Autoregressive Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。知识蒸馏（Knowledge Distillation）是一种技术，它通过将一个大型预训练模型（通常称为教师模型）的知识传递给一个较小的模型（称为学生模型），以提高学生模型的性能和效率。在语言模型领域，这种技术可以用于改进模型的预测精度、推理速度以及资源利用效率。\n",
    "\n",
    "对于自回归语言模型（Autoregressive Language Models, ALMs），即一种生成文本序列的模型，知识蒸馏可以被用来更好地理解模型如何处理不同的语言结构和上下文信息。通过分析教师模型和学生模型之间的差异，研究人员可以揭示ALMs在处理特定类型任务时的行为模式和决策过程。这有助于增强我们对这些模型内部工作原理的理解，包括它们如何利用注意力机制（Attention Mechanisms）来分配不同单词或句子片段的重要性，以及它们在构建文本序列时的决策过程。\n",
    "\n",
    "因此，这篇论文通过探索知识蒸馏在自回归语言模型中的应用，为深入理解大型语言模型的内部结构和解释其行为提供了一个途径。它不仅关注于提升模型性能，同时也致力于提高模型的可解释性，这对于促进语言模型在实际应用中的可靠性和透明度至关重要。\n",
    "\n",
    "- CLOMO: Counterfactual Logical Modification with Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题\"CLOMO: Counterfactual Logical Modification with Large Language Models\"与大型语言模型（Large Language Models, LLMs）的可解释性或理解其内在机制相关。论文标题中的关键词“Counterfactual Logical Modification”表明研究可能涉及到使用LLMs进行逻辑修改，而这种修改可能是基于假设性情境（counterfactual scenarios）的。在深度学习领域，尤其是自然语言处理中，对模型决策过程的解释和理解是一个重要且挑战性的课题。大型语言模型由于其复杂性和规模，往往在生成高质量文本的同时，也带来了预测结果难以解释的问题。\n",
    "\n",
    "因此，探讨如何通过修改输入或输出，从而更好地理解LLMs的决策过程或内部结构，具有重要的理论和应用价值。这样的研究有助于提升我们对LLMs工作原理的理解，同时也可能为提高模型的可解释性提供新的方法和技术。具体而言，论文可能会探索如何利用反事实逻辑修改技术来增强LLMs的可解释性，或者通过这种方式来揭示模型在处理特定任务时的内在机制。这符合研究LLMs内部结构和解释的范畴，旨在帮助人们更好地理解和信任这些强大的语言生成工具。\n",
    "\n",
    "- Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与Large language models（大型语言模型）的可解释性或理解其内在机制相关。标题中明确提到了“Interpretable User Satisfaction Estimation”（可解释的用户满意度估计），这表明研究可能探讨了如何通过解释模型的行为来评估用户对对话系统的满意度。这种分析通常涉及对模型决策过程的理解，特别是对于大型语言模型，它们的内部结构复杂，包括多层的block和attention机制，这些机制对于生成文本至关重要。\n",
    "\n",
    "论文的主旨可能是为了提高模型的透明度和可解释性，使人们能够更好地理解大型语言模型是如何工作以及它们在特定任务上的决策依据。这有助于构建更可靠、可信任的AI系统，尤其是在关键应用领域，如客户服务、教育咨询等，用户满意度的准确估计对于提升用户体验至关重要。\n",
    "\n",
    "因此，这个论文很可能深入探讨了如何利用可解释性技术来理解和改进大型语言模型在对话系统中的表现，特别是从模型的内部block、attention结构上进行分析，以提供更直观的见解，从而增强模型的解释力和用户的信任度。\n",
    "\n",
    "- Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“加强注意力中最短的一环：增强大型语言模型对有效工具使用的上下文意识”表明它探讨了如何改进大型语言模型（Large Language Models, LLMs）的上下文意识，从而提高它们在使用工具时的有效性。这涉及到对LLMs内部结构的理解和优化，特别是关注于“注意力”机制（Attention mechanism），这是LLMs的核心组件之一，用于决定输入序列中哪些部分对生成的输出影响最大。\n",
    "\n",
    "论文的标题中提到的“注意力”和“内部block”暗示了作者可能在研究如何通过调整或优化这些关键组成部分来提升模型的可解释性和理解力。这种工作通常涉及到深入分析模型的内部机制，如注意力权重分配的过程，以及如何通过修改模型结构或参数来改善其对特定任务（如理解和使用工具）的表现。\n",
    "\n",
    "因此，这篇论文与Large language models的可解释性或理解LLM内在机制有关，因为它旨在通过增强模型对上下文的敏感度来提高LLMs的性能和理解能力。这有助于人们更好地理解模型的工作原理，并可能为开发更强大、更易于解释的AI系统提供见解。\n",
    "\n",
    "- Layer-Condensed KV Cache for Efficient Inference of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到的主题与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制相关。具体来说，论文探讨的是如何通过层压缩键值缓存（Layer-Condensed KV Cache）来提高大型语言模型的推理效率。这一方法关注于优化模型在处理大规模文本数据时的计算资源使用和时间效率问题。\n",
    "\n",
    "大型语言模型通常由多个层次（layers）组成，每个层次执行特定的转换任务，如线性变换、激活函数应用、归一化等，最终生成输出序列。论文中提到的层压缩键值缓存技术可能旨在通过更有效地管理模型的内存状态（即键值对，用于存储中间计算结果），来减少模型推理过程中的计算开销和内存需求。\n",
    "\n",
    "这种技术对于理解LLM的内部结构和解释其工作原理具有潜在价值。通过优化内存管理和加速推理过程，研究者可以提供更多的洞察力，揭示模型在不同层面上如何进行信息处理和决策，以及这些过程如何影响最终的生成文本质量。此外，这种改进还可能有助于提升模型的可解释性，使得研究人员和开发者能够更好地理解模型的决策过程和行为模式。\n",
    "\n",
    "因此，该论文主题与大型语言模型的可解释性和理解其内在机制相关联，它提供了一种可能的途径来增强我们对这类复杂模型的理解和控制。\n",
    "\n",
    "- Aligning Large Language Models by On-Policy Self-Judgment\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到几个与大型语言模型（Large Language Models, LLMs）相关的概念，包括“对策略自我判断”（On-Policy Self-Judgment）。这些关键词表明该论文可能探讨的是如何通过某种形式的自我评估或反馈机制来调整或优化大型语言模型的行为或性能。在这一过程中，可能会涉及对模型内部结构的理解、模型决策过程的解释以及模型能力的提升等方面，尤其是关注于模型的可解释性和内在机制的理解。因此，这个论文摘要与大型语言模型的可解释性或理解其内部机制的主题是相关的。\n",
    "\n",
    "具体而言，论文可能探索了如何通过模型自身对输出进行评价，从而指导模型学习过程中的调整，以提高其在特定任务上的表现。这种自我判断的过程，实际上是在尝试理解模型在不同决策点上的行为和结果，进而对模型的内部结构和决策逻辑进行深入分析。这有助于研究人员和开发者更好地理解模型是如何做出决策的，以及哪些因素影响了模型的表现，从而为模型的改进提供依据。\n",
    "\n",
    "总之，这个论文题目所蕴含的内容与大型语言模型的可解释性及理解其内部机制密切相关，它旨在通过一种自我评估的方法来优化模型性能，同时促进对模型内部工作原理的深入了解。\n",
    "\n",
    "- Split and Rephrase with Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：该论文标题“Split and Rephrase with Large Language Models”（大型语言模型的拆分与重述）表明研究可能涉及对大型语言模型（Large Language Models，LLMs）进行更深入的分析和操作，特别是如何通过拆分（splitting）和重述（rephrasing）文本来增强或改变LLMs生成的内容。这与大型语言模型的可解释性或理解其内在机制有关，因为研究可能探讨了LLMs在处理不同输入时的行为模式、决策过程以及它们是如何生成特定输出的。通过拆分和重述文本，研究人员可能试图揭示LLMs在理解和生成文本时的内部工作原理，以及它们如何捕捉和表达语义信息。这种研究有助于提高我们对LLMs内部结构的理解，并可能为提升模型性能、改进模型解释性和增强其应用提供见解。\n",
    "\n",
    "解读：该论文题目中的关键词“Split”和“Rephrase”暗示了研究可能关注于LLMs处理和生成文本的过程细节，包括它们如何分解输入、重组内容以及最终生成输出。这样的研究通常涉及到对LLMs的内部机制进行分析，比如它们如何使用注意力机制（attention mechanisms）、上下文依赖（contextual dependencies）以及语言模型的参数化方式等，来实现高效和准确的文本生成。因此，这一主题与LLMs的可解释性和理解其内在工作原理密切相关。\n",
    "\n",
    "- SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "理由：这篇论文的标题\"SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\"表明它探讨了一种名为SAPT的方法，该方法旨在通过共享注意力框架来提高大型语言模型在持续学习过程中的参数效率。这直接与大型语言模型（Large Language Models，LLMs）的可解释性和理解其内在机制相关联。具体来说，论文关注的是LLMs内部的注意力机制（attention structure），这是LLMs理解和生成文本的关键组成部分。通过提出一个共享注意力框架，研究者试图提供一种更深入的理解LLMs如何处理和学习新任务的方式，以及它们内部的工作原理。此外，参数效率也是LLMs设计和应用中的一个重要考虑因素，因为更高效的模型通常意味着更低的计算成本和更好的可扩展性，这对于解释和理解LLM的内部结构至关重要。因此，这个论文的主题与大型语言模型的可解释性、理解其内部机制以及优化其性能紧密相关。\n",
    "\n",
    "- Quantifying Generalizations: Exploring the Divide Between Human and LLMs’ Sensitivity to Quantification\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：该论文标题“量化泛化：探索人类与大型语言模型在量化敏感性方面的差异”表明它探讨了大型语言模型（Large Language Models, LLMs）在处理量化任务时的特性和与人类之间的比较。这涉及到对LLMs内部机制的理解，特别是它们如何处理数字信息、进行计算以及在不同上下文中应用量化知识的能力。通过研究这些差异，论文可能深入分析了LLMs的内部结构、block和attention机制，以及它们如何影响LLMs的决策过程和输出。这种研究有助于提升我们对LLMs工作原理的认识，特别是它们在理解和生成量化内容时的能力和局限性，从而促进更有效的模型设计和应用。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "\n",
    "- LANDeRMT: Dectecting and Routing Language-Aware Neurons for Selectively Finetuning LLMs to Machine Translation\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“LANDeRMT: Detecting and Routing Language-Aware Neurons for Selectively Fine-tuning LLMs to Machine Translation”（LANDeRMT：检测和路由语言感知神经元以选择性地微调大型语言模型用于机器翻译）表明它探讨了如何通过特定的方法（即，检测和路由语言感知神经元）来调整大型语言模型（LLMs）以更好地服务于机器翻译任务。这涉及到对LLM内部结构的理解，特别是如何在不同的语言上下文中识别和利用关键的神经元（即，神经网络中的基本单元），以及如何基于这些识别的结果对模型进行微调，以提高其在特定任务（如机器翻译）上的性能。\n",
    "\n",
    "因此，这个论文与大型语言模型的可解释性和理解其内在机制相关，因为它关注于LLM内部结构的特定部分（语言感知神经元）及其在任务特定场景下的作用，以及如何通过调整这些部分来优化模型表现。此外，它还涉及到对LLM内部工作机制的理解，因为要实现这一目标，需要对模型如何处理不同语言信息有深入的了解。\n",
    "\n",
    "- What Do Language Models Learn in Context? The Structured Task Hypothesis.\n",
    "  回复: 是。\n",
    "\n",
    "理由：这个论文标题表明它探讨的是语言模型在上下文中学习的内容。\"Structured Task Hypothesis\"（结构化任务假设）可能涉及分析语言模型如何通过特定任务或结构化的数据集进行训练，以及这些训练过程如何影响模型的理解和预测能力。在大型语言模型（Large Language Models, LLMs）的背景下，研究它们在不同上下文中的学习方式有助于理解模型的可解释性，特别是关于它们如何处理文本、提取意义以及生成输出的过程。此外，这样的研究可以提供对LLMs内部机制的洞察，比如它们如何利用注意力机制（attention mechanisms）来聚焦于文本的不同部分以做出更准确的预测。因此，这个论文摘要与大型语言模型的可解释性和理解其内在机制有关。\n",
    "\n",
    "- On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它探讨的是神经语言模型（Neural Language Models, NLMs）在链式思维推理（Chain-of-Thought Reasoning）方面的表征能力。链式思维推理是指通过一系列逻辑步骤来解决问题的能力，这种能力对于理解自然语言文本至关重要。大型语言模型（Large Language Models, LLMs）通常指的是参数量巨大、能够处理复杂语言任务的神经网络模型，如通义千问、GPT系列等。\n",
    "\n",
    "论文标题中提到的“表征容量”（Representational Capacity）意味着研究将关注模型如何编码和表示信息，以及这些表示如何支持链式思维推理。这涉及到模型内部的结构和机制，比如如何处理信息、执行推理过程以及最终生成合理的输出。因此，论文很可能会深入分析LLMs的内部组件（如block、attention结构），探讨它们如何协同工作以实现更复杂的语言理解和生成任务。\n",
    "\n",
    "总的来说，这篇论文与大型语言模型的可解释性或理解其内在机制直接相关，因为它旨在揭示模型如何处理和利用信息进行推理，从而提高对LLMs内部工作原理的理解。\n",
    "\n",
    "- Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与Large Language Models (LLMs)的可解释性和理解其内在机制相关。具体来说，摘要提到的是分析LLM在对话摘要化（Dialogue Summarization）任务中的行为，揭示情境幻觉趋势（Circumstantial Hallucination Trends）。情境幻觉是指模型在生成文本时可能出现的错误或不准确的描述，这些描述可能基于对输入数据的理解偏差或是模型自身学习过程中的一些局限性。通过研究这些幻觉趋势，可以更深入地理解LLM的工作原理，包括其内部的block和attention结构如何影响模型的输出。这种分析有助于提高LLM的解释性，使得人类能够更好地理解模型是如何做出决策和生成特定文本的，从而改进模型的设计和应用。\n",
    "\n",
    "- Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“在高压缩比下保留关键信息：基于查询的LLMs压缩器”表明它探讨了如何在不损失重要信息的情况下对大型语言模型（Large Language Models, LLMs）进行压缩。这涉及到理解LLMs的内部结构和机制，特别是如何通过查询来指导压缩过程，从而更好地保留模型的关键信息。这种研究有助于提升我们对LLMs的理解，以及它们处理和生成文本的方式。因此，该论文摘要与大型语言模型的可解释性或理解其内在机制有关，因为它关注于提高我们对这些模型工作原理的认识，特别是在压缩过程中如何保持模型性能和能力不下降。\n",
    "\n",
    "- MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题和摘要提到了对语言模型（Language Models）进行深入分析，特别是通过条件消除退化（conditioning away degeneracy）来揭示真正的语言模型模式。这涉及到对语言模型内部机制的理解，尤其是如何避免模型在预测时出现的退化问题，即在面对某些输入时产生过于泛化的输出。这与大型语言模型（Large Language Models）的可解释性和理解其内在机制紧密相关。\n",
    "\n",
    "大型语言模型通常包含复杂的结构，如多层神经网络、注意力机制（Attention）、块（Blocks）等，这些组件共同作用以生成文本。通过对这些模型的内部结构进行分析和优化，可以帮助我们更好地理解它们的工作原理，提高模型性能，并开发出更有效的训练和使用策略。因此，该论文探讨的主题与大型语言模型的内部结构、解释性和理解其内在机制具有直接关联。\n",
    "\n",
    "- Harder Task Needs More Experts: Dynamic Routing in MoE Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到多专家模型（MoE models），也称为门控专家（Dynamic Routing）模型，是一种在大型语言模型中引入可解释性和理解其内部机制的方法。在传统的并行模型中，输入数据会同时被所有模型（即“专家”）处理，而在MoE模型中，输入数据首先通过一组门控函数（通常是神经网络层）进行路由，决定数据应该由哪个专家处理。这使得模型能够根据任务的复杂度动态地分配资源，更高效地处理不同的任务。通过这种方式，MoE模型可以实现对不同任务的优化，同时也提供了一种方法来理解模型是如何做出决策的，因为我们可以观察到哪些专家在特定任务中被更多地使用。\n",
    "\n",
    "因此，这个论文主题与大型语言模型的可解释性直接相关，因为它探讨了如何通过动态路由机制来增强模型性能的同时，增加模型的透明度和理解其内部工作原理的可能性。这有助于帮助人们更好地理解大型语言模型的内部结构和决策过程。\n",
    "\n",
    "- Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题《Symbol-LLM：为大型语言模型建立基础的符号为中心的接口》似乎在探讨一种方法，即通过构建一个符号为中心的接口来更好地理解和与大型语言模型（LLMs）交互。虽然论文的具体内容未被提供，但题目中提到的“符号”和“大型语言模型”的结合，以及“基础的符号为中心的接口”，表明它可能涉及对LLM内部机制的解释或分析。\n",
    "\n",
    "大型语言模型通常基于复杂的架构，如Transformer，这些架构包含多层的编码器和解码器，以及自注意力机制等组件。通过创建一个符号为中心的接口，研究者可能试图提供一种更直观的方式来理解这些模型如何处理和生成文本。这种接口可能会帮助人们更好地理解LLM是如何根据输入符号进行上下文推理、语义理解以及生成输出的。\n",
    "\n",
    "因此，这个论文摘要与大型语言模型的可解释性和理解其内部机制相关，特别是从LLM的内部块（如Transformer块）和注意力结构的角度出发，以提高我们对这些模型工作原理的理解。\n",
    "\n",
    "- Visualization Recommendation with Prompt-based Reprogramming of Large Language Models\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题涉及到Large language models（大型语言模型），并且提到了可视化推荐和基于提示的大型语言模型重编程。这表明该论文可能探索了如何通过可视化工具来理解和解释大型语言模型的工作机制，特别是通过改变模型的输入（基于提示）来观察输出变化的方式。这种研究方法有助于人类更好地理解大型语言模型内部的决策过程和工作原理，特别是关注模型的可解释性和内在机制。因此，这个论文摘要与Large language models的可解释性或理解其内在机制相关，尤其是从模型的内部结构和工作原理上进行解释。\n",
    "\n",
    "- Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明了其主要关注点是在长上下文问题回答（Long-Context Question Answering）场景中使用位置无关的分解式训练方法。这种研究方向与大型语言模型（Large Language Models, LLMs）的可解释性和理解其内在机制紧密相关。具体而言，该论文可能探讨了如何通过分解式训练策略来改进LLMs在处理包含大量历史信息或长篇文本的问题时的表现。这种策略旨在让模型能够更好地理解和利用长上下文信息，这对于提高LLMs在复杂任务中的性能至关重要。\n",
    "\n",
    "位置无关的分解式训练方法可能涉及到将输入文本分解为更小的部分，然后在这些部分之间建立联系，而不仅仅是基于它们在原始文本中的位置。这种方法有助于模型学习到更深层次的语义关系，而不是仅仅依赖于文本的表面顺序。这样的研究对于深入理解LLMs是如何处理和整合不同长度的信息，以及如何通过训练增强模型的这一能力具有重要意义。\n",
    "\n",
    "总之，这个论文题目表明它可能探讨了提升LLMs处理长上下文问题能力的技术细节，特别是通过改进训练策略来增强模型的理解和预测能力。因此，它与大型语言模型的可解释性和理解其内在机制密切相关。\n",
    "\n",
    "- Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs\n",
    "  回复: 是。\n",
    "\n",
    "理由：这篇论文的标题“Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs”表明它探讨的是大型语言模型（Large Language Models, LLMs）在多利益相关者对齐下的决策制定过程。这涉及到对LLMs内部结构和工作机制的理解，尤其是如何在多方面的考量下做出决策。论文通过这种方式试图增强我们对LLMs内部运作机制的理解，包括它们如何处理和整合不同利益相关者的信息，以及它们在决策过程中可能使用的特定结构或组件（如内部block、attention机制等）。因此，该论文与大型语言模型的可解释性和理解其内在机制相关。\n",
    "\n",
    "解读：这篇论文可能关注的是如何使LLMs的决策过程更加透明和可解释，这对于提高人们对这些模型的信任度和在实际应用中的接受度至关重要。通过研究多利益相关者对齐的概念，作者可能试图探索如何设计或调整LLMs，使其决策更加符合社会伦理标准，同时保持技术效率和实用性。这种研究方向有助于推动LLMs领域的发展，特别是在确保模型决策的公正性、道德性和可解释性方面。\n",
    "\n",
    "- Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题表明它探讨的是如何通过增强大型语言模型（Large Language Models, LLMs）的可解释性来提升它们在上下文学习（In-context Learning）中的表现。具体来说，“Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning”表示研究者提出了一种方法，即“可解释性软集合（Explanation-aware Soft Ensemble）”，旨在使大型语言模型能够更好地理解和适应给定情境下的任务，从而提高其在特定任务上的性能。\n",
    "\n",
    "大型语言模型的内部结构通常包括多层神经网络，如Transformer架构，其中包含自注意力机制（Attention Mechanism）等组件。这些结构允许模型在处理文本时考虑上下文信息，从而生成连贯且相关的响应。然而，尽管大型语言模型在许多自然语言处理任务上表现出色，但它们的决策过程往往难以解释，这限制了我们对模型行为的理解和优化。\n",
    "\n",
    "因此，这个论文关注的点在于提高大型语言模型的可解释性，使其内部工作原理更加透明，这有助于人们更好地理解模型如何做出决策以及为什么做出这样的决策。通过提高可解释性，研究者可能能够发现模型在特定任务上的优势和弱点，进一步优化模型设计和训练过程，以及改进其在实际应用中的性能。\n",
    "\n",
    "总之，这个论文标题与大型语言模型的可解释性和理解其内部机制有关，因为它探讨了如何通过一种新的方法来提升模型在上下文学习任务上的表现，同时增强我们对模型决策过程的理解。\n",
    "\n",
    "- DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention\n",
    "  回复: 是。\n",
    "\n",
    "理由及解读：论文标题“DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention”（DeCoT：通过因果干预在大型语言模型中为知识密集型任务去偏见化思考链）表明该研究关注于大型语言模型（Large Language Models, LLMs）的特定方面，即它们在处理知识密集型任务时可能存在的偏见问题。论文标题中提到的“Chain-of-Thought”（思考链）指的是模型生成答案时的思维过程，通常包括对问题的理解、相关信息的检索以及最终答案的形成等步骤。\n",
    "\n",
    "虽然论文标题并未直接提及“可解释性”或“理解LLM内在机制”，但考虑到去偏见化（debiasing）通常涉及到对模型决策过程的深入理解以及对模型内部结构和行为的分析，因此可以推断，该论文可能涉及对LLM的内部结构、特定组件（如block或attention机制）及其如何影响模型性能和决策过程的研究。通过因果干预（causal intervention），研究者试图理解和调整这些内部机制，以减少模型在知识密集型任务中的偏见。\n",
    "\n",
    "综上所述，尽管论文标题没有明确指出与LLM的可解释性和内在机制理解直接关联，但基于题目描述的内容推测，它很可能探讨了这些主题，因此答案是“是”。\n",
    "\n",
    "- Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor\n",
    "  回复: 是。\n",
    "\n",
    "这个论文摘要与 Large Language Models (LLMs) 的可解释性和理解其内在机制相关。标题中提到的 \"Black-box Language Models\" 指的是那些内部结构复杂且难以直接理解和解释的大型语言模型。\"Score-based Progressive Editor\" 这个方法似乎旨在通过逐步编辑模型生成文本的评分（score）来控制文本生成过程，从而提供一种方式来理解或影响黑盒模型的行为。这种方法可能涉及到对 LLM 内部的 block 或 attention 结构进行分析和调整，以更好地理解模型是如何处理输入并生成输出的。因此，该论文尝试通过具体的技术手段来提高对 LLM 内部工作原理的理解，符合题目要求的“解释 LLM 的内部结构和解释的”目标。\n",
    "\n",
    "- RDRec: Rationale Distillation for LLM-based Recommendation\n",
    "  回复: 是。\n",
    "\n",
    "理由和解读：论文标题“RDRec: Rationale Distillation for LLM-based Recommendation”表明该研究关注的是如何通过“理据蒸馏”（Rationale Distillation）来提升基于大型语言模型（Large Language Models, LLMs）的推荐系统的性能。在深度学习领域，特别是自然语言处理中，大型语言模型通常被认为是“黑箱”模型，即它们的决策过程对于人类来说往往难以理解和解释。然而，这种不透明性限制了我们对模型如何做出决策的理解，以及可能存在的偏差或错误。\n",
    "\n",
    "“理据蒸馏”是一种技术，旨在提取和解释大型语言模型的决策依据，使其决策过程变得可解释。这有助于人们更好地理解模型的工作原理，识别其在特定任务上的优势和局限性，并可能发现模型在处理某些类型数据时可能出现的偏差或偏见。通过这种解释方法，研究者可以提供关于模型内部结构和决策机制的洞察，从而增强模型的透明度和可信度。\n",
    "\n",
    "因此，该论文摘要与大型语言模型的可解释性和理解其内在机制相关，因为它探讨了如何通过理据蒸馏来解释基于LLMs的推荐系统的工作原理，帮助人们更好地理解这些模型的内部结构和决策过程。\n",
    "\n",
    "\n",
    "- Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval\n",
    "  回复: 是。\n",
    "\n",
    "这个论文标题“从开始深入：语言模型如何为密集检索嵌入长文档”表明它探讨了大型语言模型（Large Language Models, LLMs）在处理长文档时的内部机制，特别是与密集检索相关的部分。密集检索（Dense Retrieval）通常涉及到在大型文本集合中找到与查询最相关的段落或句子，这需要模型能够有效地理解和记忆大量信息。\n",
    "\n",
    "论文的标题和主题都指向了对LLMs内部结构的理解和解释，尤其是它们如何处理和存储长文档的信息以供后续检索使用。这可能涉及对模型的内部块（如Transformer的编码器层）、注意力机制（attention structure）以及其他可能用于捕捉上下文、长距离依赖性和文本表示的特定设计和技术的分析。\n",
    "\n",
    "因此，该论文摘要与大型语言模型的可解释性或理解其内在机制有关，因为它旨在揭示模型是如何实现这种特定任务的，从而帮助人们更好地理解LLMs的工作原理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
